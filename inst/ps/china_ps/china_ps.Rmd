
```{r 'check_ps', include=FALSE}

user.name = 'ENTER A USER NAME HERE'
```

---
output:
  word_document: default
  html_document: default
  pdf_document: default
---



# Technology in China's War on Air Pollution

Welcome to this interactive RTutor problem set on China's *War on Pollution* based on the scientific paper "Can Technology Solve the Principal-Agent Problem? Evidence from China's War on Air Pollution" by Greenstone, He, Jia, and Liu, published in the *American Economic Review: Insights* journal in 2022. This problem set is part of my master's thesis at Ulm University, and I hope it will pique your interest and expand your knowledge of R programming and econometrics.

Environmental pollution is a significant global issue, with China being especially affected due to its rapid economic growth in recent decades. This growth has led to a significant increase in air, water, and soil pollution, adversely affecting the health and well-being of billions of people. Therefore, environmental pollution is a highly relevant topic in Chinese politics. This interactive analysis focuses specifically on air pollution, which was also the centerpiece of the *War on Pollution* declared by the Chinese government in 2014. The initiative included a countrywide, real-time air quality monitoring and reporting program aimed at improving the availability and quality of air pollution data. Prior to the implementation of this new technology, local city governments, which are de facto subordinate to the Chinese central government, collected the data before it was published. As this scenario created a potential principal-agent problem, we will use a Regression Discontinuity (RD) design to analyze if there is any evidence for structural data manipulation of air pollution data before the new technology was implemented.

You can find the original paper, the working paper, the online appendix, and the associated data at the following links:

`Paper`: <https://www.aeaweb.org/articles?id=10.1257/aeri.20200373>

`Working paper`: <https://www.nber.org/papers/w27502> (published in 2020)

`Online appendix`: <https://assets.aeaweb.org/asset-server/files/16219.pdf>

`Data`: <https://www.openicpsr.org/openicpsr/project/125321/version/V1/view>


This RTutor problem set will be published on github.com and on shinyapps.io.


## Exercise Content

1. Introduction

  1.1 Air Pollution
  
  1.2 China and the War on Pollution
  
2. Data Preparation

3. Regression Discontinuity Designs - Implementation of New Air Pollution Reporting and Monitoring Technology

  3.1 Methodological and Context-Specific Introduction 
  
  3.2 Analysis of Station 1491 and Xi'an
  
  3.3 Countrywide Analysis and Core Findings

4. Difference-in-Differences Event-Study Analysis 
  
5. Validity Tests

6. Discussion and Conclusion

   Bibliography

   Appendix
   

## Overview

We will embark on an interactive, econometric journey, beginning with *Exercise 1*. Here, we will learn about the fundamentals of air pollution and China’s political system, as well as its *War on Pollution*. In *Exercise 2*, we will become acquainted with our data sets and apply some basic R functions to gain a better understanding of the available data. *Exercise 3* is the main section where we start small with just one single station and step by step create more complex models for our RD design which we finally apply to the entire data set. In *Exercise 4*, we will employ a method known as Difference-in-Differences event-study analysis to estimate medium-term effects and to verify the results of the RD design. In the next step, we will move on to *Exercise 5* where we will conduct further validity tests by scrutinizing the assumptions on which the RD design is based. Finally, *Exercise 6* will include a discussion and a conclusion. Additional resources and analyses can be found in the appendix.

## RTutor 

If this is your first RTutor problem set, here are some tips to help you get started:

- Information on the R-Package `RTutor` (Kranz 2020) is available on Github (https://github.com/skranz/RTutor).

-	Chapters in RTutor are called exercises, which can be navigated using the numbers at the top. They include text, code chunks, quizzes, and info boxes. Each exercise can be solved independently.

- Code chunks can be of three types: given code with gaps (___), empty code chunks, or completely given code which does not require input from you.

- To navigate code chunks, use the following buttons:

  -	`check`: this button checks your solution and returns either the desired output or an error message.
  
  - `hint`: if you cannot solve the given task, try using this button to get a hint.  

  - `run chunk`:  by using this button you run the code in the code chunk without checking if it is correct. 

  - `data`: by clicking here you can view the data in the data explorer tab.

  - `solution`: this button gives you the sample solution for the code chunk.

  - `original code`: restores the original code. 
  
  - `edit`: solves all previous code chunks so you can directly access the code chunk in which you are interested.


Please note: 

–	To avoid losing your progress in RTutor, please remember to open any external links in a new browser tab or window.

–	The language in this RTutor problem set was optimized using ChatGPT (see Appendix A5 for further details).


## Exercise 1 -- Introduction

The first two exercises serve as a foundation and provide background information on air pollution and China. Their purpose is to offer a broad overview of these topics, but they are not mandatory for the rest of the problem set. If you prefer, you can skip directly to data preparation in *Exercise 2* or the econometric and analytical parts of subsequent exercises.

*Exercise 1.1* and *Exercise 1.2* already contain some R code, but no input is required. The purpose of these exercises is to provide you with an understanding of the underlying topics rather than to test your R programming skills. You will have many opportunities to write and modify R code yourself in later exercises.


## Exercise 1.1 -- Air Pollution

Given that Greenstone et al.’s (2022) original paper is centered around air pollution, it is unsurprising that their and subsequently our analysis is based on air pollution data. Therefore, we start this RTutor problem set with a general overview of air pollution. This overview will not only help you better understand the context in later exercises, but it will also highlight the relevance of the topic in general. According to the United Nations Statistics Division, pollution is the “presence of substances and heat in environmental media (air, water, land) whose nature, location, or quantity produces undesirable environmental effects” (United Nations Statistics Division, 2022).

In 2019, approximately 9 million deaths were attributed to pollution, as revealed by a recent study published in *The Lancet Planetary Health* journal (Fuller et al. 2022, 535). According to this study, the risk of death from pollution has increased by over 66% since 2000, with rapid urbanization and industrialization identified as the main reasons. Over 6.5 million deaths in 2019, primarily in low- and middle-income countries, were attributed to air pollution, which is the most relevant type of pollution in this RTutor problem set. Please note that this is only a thematic introduction intended to emphasize the importance of the topic. Therefore, we will not further explore the methodology used to estimate these numbers or make comparisons to similar studies.

**Task:** Click on `check` below to get an initial overview of global air pollution mortality rates (age-standardized mortality rates in deaths per 100,000 citizens in 2019).

```{r "3_1",output='htmlwidget', widget='plotly', optional=TRUE}

#load packages
library(ggplot2)
library(plotly) 
library(hrbrthemes)

#load data
mortality_data <- readRDS("mortality_data.RDS")

#create map
mortality_map <- ggplot(data = mortality_data)+
  geom_polygon(aes(x = long, y = lat, group = group, Country=Country, 
                   fill = mortality_rate_2019))+
  scale_fill_gradient(low='lightgrey', high='red4')+
  coord_fixed()+
  theme_ipsum(plot_title_size = 12)+
  theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank(),
      panel.background=element_blank(),
      panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank())+
  labs(fill = "Mortality Rate \n (per 100,000 citizen)", 
       title="Air Pollution Mortality Rates 2019")


#show interactive map
ggplotly(mortality_map)


#Title: Air Pollution Mortality Rates 2019
#adapted version of: https://ourworldindata.org/air-pollution
#Source data: 
#Global Burden of Disease Collaborative Network (2021) 
#Global Burden of Disease Study 2019 (GBD 2019) Results. Seattle, 
#United States: Institute for Health Metrics and Evaluation (IHME).


```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("ggplot2 package")
```


Quiz: Did China have the highest air pollution mortality rate in 2019 compared to other countries based on this map?

[1]: yes
[2]: no

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("China mortality rate 1")
```



Quiz: What was the mortality rate in China in 2019 (two decimal places)?

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("China mortality rate 2")
```

Although there were countries with higher mortality rates from air pollution in 2019, China had the highest number of air pollution-related deaths in 2012 (2.26 million, 95% confidence interval: 1.91 to 2.64 million) according to Geng et al. (2021). This helps explain the need for China’s *War on Pollution* which we will cover in *Exercise 1.2*. The map above shows estimates of air pollution-related mortality rates based on data that includes deaths attributed to indoor and outdoor particulate matter (PM, more specifically $PM_{2.5}$ in this case). However, it is important to note that these estimates do not take into account mortality caused by other air pollutants, such as ozone. The difference between indoor and outdoor air pollution is quite self-explanatory. Indoor air pollution includes, among others, emissions from indoor combustion (e.g., from stoves), mold, and chemicals used in construction materials, resulting in approximately 2 million deaths per year (Murray et al. 2020). More pertinent to this RTutor problem set is outdoor air pollution, which is related to 4.5 million deaths in 2019 according to another study from *The Lancet Planetary Health* (Murray et al. 2020). A World Bank (2022a, 2) study estimated that air pollution caused a global welfare loss of 6.1% of global GDP (Gross Domestic Product, based on purchasing power parity adjusted USD) in 2019, equivalent to around **USD 8.1 trillion** (purchasing power parity adjusted).

PM is the most important air pollutant in the context of this RTutor problem set. It is a “mixture of solid particles and liquid droplets found in the air” (US EPA 2016). According to Russel and Brunekreef (2009, 4620), these heterogeneous particles can be dust, soot, or smoke and vary in size. $PM_{2.5}$, the pollutant referred to in the map above, is fine particulate matter with a diameter smaller than 2.5 micrometers or microns (μm). There is also $PM_{0.1}$, which is considered ultra-fine particulate matter, with a diameter of less than 0.1 μm. In this problem set, we will primarily focus on $PM_{10}$, which is particulate matter with a diameter of 10 μm or less, therefore including the smaller $PM_{2.5}$ and $PM_{0.1}$ particles by definition. The illustration below may help you understand how small these particles are:

<center>
<img src="pm2.5_scale_graphic-color_2.png" style="max-width:100%"></img>
</center>

This figure was taken from: https://www.epa.gov/pm-pollution/particulate-matter-pm-basics#PM


The official WHO (World Health Organization) and WHO European Centre for Environment and Health 2021 guidelines for $PM_{10}$ are: 
- annual average $15\ \mu g/m^3$,
- daily average $45\ \mu g/m^3$. 

For $PM_{2.5}$, they are even lower: 
- annual average $5\ \mu g/m^3$,
- daily average $15\ \mu g/m^3$.


PM is a highly heterogeneous substance that can originate from a wide range of sources. According to Russel and Brunekreef (2009, 4621), each emitting source is unique due to varying conditions and chemical substances present. Generally, PM is classified as either primary or secondary. The United States CDC (Centers for Disease Control and Prevention; 2022) explains that primary sources directly emit PM, such as wood stoves (indoor air pollution) or forest fires (outdoor air pollution). In contrast, secondary sources emit substances that react with other substances to form PM, such as power plants or coal fires. Certain sources are both primary and secondary, such as specific industrial processes or combustion engine vehicles. While PM is often caused by human activity, it can also result from natural causes such as soil erosion or wildfires. PM can be composed of several chemical substances which are considered air pollutants themselves, such as nitrogen oxides $(NO_x)$, sulfur oxides $(SO_x)$, and carbon monoxide $(CO)$. These pollutants can result in various forms of smog when present in high concentrations and specific compositions (Ma et al. 2012, 1007; Li et al. 2021). Considering temperature, precipitation, wind, and humidity is crucial in determining the severity of air pollution and therefore also must be included in our analysis (Greenstone et al. 2022). While greenhouse gases such as $CO_2$ have global effects, air pollutants have a more localized impact on air quality and can lead to immediate consequences for both the environment and living organisms.

Briefly summarizing, $PM_{2.5}$ is a byproduct of burning fossil fuels and of industrial processes involving high temperatures or certain chemicals, e.g., $NO_x$, $SO_2$ (Srimuruganandam and Shiva Nagendra 2012). Larger particles, such as $PM_{10}$, come from a variety of sources, including soil erosion, ocean spray, pollen or mold, and non-natural sources, such as dust from construction, mining, or farming, as well as from combustion of coal and oil (Srimuruganandam and Shiva Nagendra 2012). Nowadays, governmental regulations are often based on $PM_{2.5}$ due to its association with more severe health risks compared to particles larger than 2.5 μm (Russell and Brunekreef 2009, 4623). However, Russell and Brunekreef (2009, 4623) also report a higher mortality risk caused by larger particles, such as $PM_{10}$. For the remainder of this problem set, we will mostly focus on $PM_{10}$ (which, as previously mentioned, also comprises finer particles). The human body is affected differently by PM depending on the size and composition of particles (Russell and Brunekreef 2009, 4620). To summarize some of the results of Kim et al. (2015, 138–140), the health impacts of PM range from irritation of the upper respiratory system caused by larger particles to (lung) cancer, severe cardiovascular diseases, and infertility caused by smaller particles. Ultimately, some of these diseases are fatal, which explains the high number of global deaths attributed to air pollution.

**Index and Measurement** 

Many government agencies and scientific institutes continuously publish air pollution data, which is also used to create indices informing the public on websites and mobile apps about the current air quality in a clear and understandable way. There are in general two ways of measuring air pollution: local monitoring stations on the ground, and dedicated satellites which measure the concentration of gases and particles in the atmosphere based on the amount of light that reaches the surface. The latter is what we refer to as Aerosol Optical Depth (*AOD*; Wei et al. 2020). High PM pollution is denser (more aerosol particles) and therefore reduces the amount of sunlight that passes through. In theory, we can use *AOD* data to infer air pollution levels on the ground. However, *AOD* data is less precise on a local level and requires conversion using statistical methods and spatial modeling to derive PM equivalents. Moreover, cloud cover can impact data collection. On the other hand, ground-level monitoring stations may deliver an incomplete or incorrect picture for larger areas due to their fragmented network. Although not directly relevant to this RTutor problem set, van Donkelaar et al. (2016) demonstrate how Geographically Weighted Regression (GWR), using data from local monitoring stations and *AOD* satellite data, can result in more accurate (global) $PM_{2.5}$ predictions. Furthermore, the employment of drone technology in this field could increase spatial coverage and measurement accuracy in the future (Hedworth et al. 2021).

The Chinese government publishes an air quality index (AQI) based on six pollutants, including $PM_{2.5}$, $PM_{10}$, $SO_2$, $NO_2$, $O_3$ and $CO$. However, the data sets accompanying the underlying paper only include data for $PM_{10}$, $SO_2$, and $NO_2$. The reason is rather simple since the AQI and many of the incorporated pollutants like $PM_{2.5}$ were implemented on a city-level as part of the *War on Pollution* yet we need comparable and consistent data for our analysis from before **and** after the reporting and monitoring automation. Greenstone et al. (2022, 57) state that only $PM_{10}$, $SO_2$, and $NO_2$ were available in the pre-automation period yet other studies on air pollution in China (e.g., Geng et al. 2021) based their analysis on $PM_{2.5}$ data from as far back as 2002. We may therefore assume that some $PM_{2.5}$ data was available earlier yet not on the detailed level Greenstone et al. (2022) needed for their analysis.


Quiz: Why is publishing transparent and reliable pollution data important?

[1]: To facilitate effective implementation of preventive policies.
[2]: To enable comparability with other regions and tracking of measures.
[3]: To help individuals take appropriate personal protection measures based on current pollution levels, such as using air filters or face masks.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Importance of reliable data")
```


In areas where air pollution poses a significant health risk, people rely on pollution indices and metrics to make informed decisions about outdoor activities and the appropriate level of personal protection, such as face masks and indoor air filters (Greenstone et al. 2022). This highlights the importance of publishing reliable pollution data. Another critical reason for such publication is the implementation and enforcement of policy measures and regulations based on the data. In the next exercise, we will explore the changes made by the Chinese central government to improve the availability and quality of local pollution data.


## Exercise 1.2 -- China and the War on Pollution

The goal of this exercise is to provide fundamental information about China and demonstrate the crucial link between air pollution and the political system in the country. As of 2021, China, officially known as the People’s Republic of China (PRC), remains the most populous country globally, with an estimated 1.41 billion citizens (World Bank 2022c). The following map illustrates the population density across provinces.

**Task:** Click on `check`. It takes a moment to generate this map.

```{r "4_1",output='htmlwidget', widget='plotly'}
#load data
china_polygons <- readRDS("china_polygons.RDS")
china_data <- readRDS("china_data.RDS")

#create map
china_population <- ggplot()+
    geom_map(data = china_data, aes(map_id = id, fill = Population_2020), 
             size = 0.05, color = "lightgrey", map = china_polygons)+
    expand_limits(x = china_polygons$long, y =  china_polygons$lat)+
    scale_fill_gradient(low='skyblue1', high='navy')+
    coord_map()+
    labs(title="Chinese Provincial Level - Population 2020", 
         fill = "Population \n in millions")+
    theme_ipsum(plot_title_size = 12)+  
    theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank())


#create interactive map
ggplotly(china_population)


#Title: Chinese Provincial Level - Population 2020
#Source shapefile: 
#https://datapleth.io/blog/2015/10/11/2015-10-11_china-base-map/
#Source population data: 
#National Bureau of Statistics of China (2021) 
#China Statistical Yearbook 2021. chapter 2.5.

```

The map depicted above was created utilizing the `ggplot()` function from the `ggplot2` package. Once the map was generated and saved in the `china_population` variable, the `ggplotly()` function from the `plotly` package was employed to transform it into an interactive map.



Quiz: Where did the majority of China's population live in 2020?

[1]: eastern and southern China
[2]: western and northern China

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("China population")
```


Quiz: In 2020, which Chinese provincial entity had the highest population?

[1]: Xizang (Tibet), autonomous region
[2]: Bejing, municipality
[3]: Guangdong, province

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Highest population")
```


The uneven distribution of China’s population can be attributed to several factors. One of the reasons is the country’s diverse geography, with less habitable areas in the west due to deserts, mountain ranges, and harsher climates compared to the east and south. Economic factors also played a role, particularly in the 1970s when the Chinese government partially opened up the economy to foreign investment, leading to an increase in migration to the economic zones in the east and south and thereby reversing the effects of previous relocation measures to western provinces (Dunrui 2021, 115-125).

China’s successive economic convergence and opening to capitalism and global trade in the following decades had significant long-term impacts on the country’s development. While China did not fully open its economy, it transformed from a poor agricultural nation to the world’s manufacturing center in recent decades. This unprecedented economic development is evident in the GDP (purchasing power parity adjusted), which rose from USD 0.36 trillion in 1990 to USD 17.73 trillion in 2021, making China the world’s second-largest economy (World Bank, 2022b). However, this economic rise, heavily based on fossil fuels and limited environmental regulations, came at a cost, as all kinds of pollution increased over the years, with China consuming the largest amount of energy and coal in global comparison in 2010 (Greenstone et al. 2021, 281).

**War on pollution** 

The health risks and increased mortality associated with pollution, as briefly outlined in *Exercise 1.1*, have become increasingly relevant to Chinese society and consequently to the Chinese government (Barwick et al. 2019, 1). According to Matus et al. (2012), the estimated marginal welfare loss due to PM (and ozone) was USD 112 billion in 2005, compared to USD 22 billion in 1997, highlighting the economic risks associated with air pollution. In addition, Barwick et al. (2019) suggest that in 2010 and subsequent years, the Chinese public was dissatisfied with the government’s handling of pollution-related issues and the availability and quality of information. To provide context for our analysis, will now roughly summarize the key characteristics of the Chinese political system based on Heilmann (2016). The Chinese Communist Party (CCP) is de facto the only political party and exercises its constitutional leadership claim. The distinction between the CCP and the Chinese government is considered nominal since the CCP directs the government on all levels. Within the CCP, the most important party organs are the Political Bureau and its Standing Committee, which are elected by the Central Committee. Moreover, the General Secretary, the highest-ranking party official, is part of the Politburo’s Standing Committee and is typically also the country’s president. The CCP has the power to control who is elected on each level of government. At the highest level of government is the National People’s Congress (NPC), which elects the State Council and the President. The State Council is, among other things, responsible for the Five-Year Plans, which outline the country’s desired development over the next five years. Below the central government follow provincial governments (the level used in the map above), city or prefecture governments, county governments, township or municipal governments, and village committees. While this ranking may seem straightforward, there are further distinctions at each level. For example, at the provincial level, there are provinces, autonomous regions, and municipalities, each of which is organized differently on the subordinate levels. For each of these levels of government, there is a (superior) counterpart committee in the CCP.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("China's political system.")
```


**Task:** Click on `check`.

```{r "4_2",output='htmlwidget', widget='plotly'}

#create map (based on the same data sets as the previous map)
china_pollution <- ggplot()+
    geom_map(data = china_data, aes(map_id = id, fill = pm_2012_mean), 
             color = "lightgrey", map = china_polygons, size = 0.05) +
    expand_limits(x = china_polygons$long, y =  china_polygons$lat) +
    coord_map() +
    scale_fill_gradient(low='lightsalmon', high='red4')+
    labs(title="Chinese Provincial Level - Particulate Matter 2012", 
         fill = "Yearly average \n PM2.5 (in μg/m³)") +
    theme_ipsum(plot_title_size = 12)+  
    theme(axis.line=element_blank(),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.title.x=element_blank(),
      axis.title.y=element_blank())

#create interactive map
ggplotly(china_pollution)


#Title: Chinese Provincial Level - Particulate Matter 2012
#Source shapefiles: 
#https://datapleth.io/blog/2015/10/11/2015-10-11_china-base-map/
#Source pollution data: 
#https://aqli.epic.uchicago.edu/country-spotlight/china/

```




Quiz: In 2012, which entity on the provincial level had the highest pollution?

[1]: Xizang (Tibet)
[2]: Bejing
[3]: Guangdong

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("China pollution")
```


Quiz: In 2012, how many provinces (including municipalities and autonomous regions) were below the current WHO guideline for annual average $PM_{2.5}$?

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("China pollution WHO guideline")
```


The red gradient in the map above was intentionally applied to illustrate that all provinces, municipalities, and autonomous regions had annual average $PM_{2.5}$ levels above the current WHO guideline of $5\ \mu g/m^3$ in 2012, with some areas, such as Beijing, exceeding the guideline by almost 15 times (Greenstone et al. 2021, 281). Pollution had become a serious issue, not only for the environment but also for the government, leading to a significant shift in environmental policies after 2010. During the 2014 NPC meeting, the so-called *War on Pollution* was announced, which was a far-reaching and radical regulatory effort to reduce pollution (Barwick et al. 2019).

The following measures were important elements of the *War on Pollution*:

- evaluating and incentivizing government officials based on emission targets,

- setting limit values for $PM_{2.5}$ emissions,

- implementing several environmental policies (such as regional $CO_{2}$ cap-and-trade programs and electrification of the mobility sector),

- introducing a real-time air quality monitoring system that provides measurements to the public, including a new AQI that incorporates $PM_{10}$, $PM_{2.5}$, $O_{3}$, and $CO$ measurements (Greenstone et al. 2021, 283). 

Most relevant for this RTutor problem set is the last of these measures since we will analyze the effect of the new monitoring technology on published air pollution data. According to Barwick et al. (2019), the Chinese government already began to roll out the technology before officially announcing the *War on Pollution*, which is also reflected in our data.

Next, we will shortly cover the principal-agent theory, which is mentioned in the title. It is also known as the principal-agent problem, agent theory, or agency problem, and applies to a variety of settings in which an entity (the principal) hires another entity (the agent) to perform a task or service and provides incentives to ensure the agent acts in the principal’s interest rather than their own (Jensen and Meckling 1976, 308; Gailmard 2014, 91). The principal and agent may be human, legal, or institutional entities. In this scenario, the principal is the Chinese central government, and the agents are the local governments at the city-level. However, the central government does not directly appoint local governments, as they are officially elected by the respective local congress. Despite this, the scenario is still considered a principal-agent problem because high-ranking members of the central government are also in leading positions within the CCP, and the CCP committees at each organizational level are de facto appointed by upper CCP levels (Gao et al. 2016). Thus, the city governments are indirectly controlled and appointed by higher levels of government which qualifies this scenario as a principal-agent problem.

Information asymmetries and hidden actions are the foundation of this principal-agent problem. The principal aims to minimize these asymmetries as the agent might act in their own interest. The central government is interested in reliable and accurate pollution data for credibility, effective resource allocation, setting realistic targets, and enabling citizens to take appropriate protective measures based on current air quality. Local governments and individual officials are evaluated and incentivized, among many other factors, based on local air pollution (Greenstone et al. 2022, 54). Hence, they are motivated to reduce air pollution, which can lead to actual improvements in air quality if suitable incentives and control mechanisms are in place. However, if these mechanisms are insufficient to align interests, local governments may have an interest in manipulating their air pollution data without actually improving air quality, benefiting only themselves but not the central government or the people. We will analyze whether there is any evidence supporting the hypothesis of widespread air pollution data manipulation before the new technology was implemented.


Quiz: Which of the following best describes the principal-agent theory?

[1]: It applies only to human entities.
[2]: It refers to the conflicting interests between a principal and their agent.
[3]: It is also known as the agency solution.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Principal agent theory")
```


## Exercise 2 -- Data Preparation

In this exercise, we will explore an exemplary data preparation process to better understand how to process collected data. For the remainder of this RTutor problem set, we will only use the accompanying data supplied by Greenstone et al. (2022). Furthermore, we will use the programming language R throughout this problem set, and as mentioned in the introduction, prior knowledge of R is not necessary. Although Appendix A2 contains the answers to some introductory questions regarding regression analysis, it is important to note that this problem set is not a substitute for an R course or tutorial. If you encounter difficulty with a specific task, you can try the `hint` button and ultimately the `solution` button.

To begin, we will provide an overview of the collected data. Greenstone et al. (2022) have supplied eight individual data sets (files that contain data), but only the first five are relevant for the main part of this RTutor problem set. More details on the data sources are available in their data preparation readme-file, which can be found at https://www.openicpsr.org/openicpsr/project/125321/version/V1/view.

These are the relevant data sets:

- `pollution.dta`: daily average pollution data for $PM_{10}$, $NO_2$, and $SO_2$ from 654 monitoring stations in 123 Chinese cities. The source for this data is the China's Real-Time Air Quality Reporting and Analysis Platform.

- `aod_month.dta`: Aerosol Optical Depth (*AOD*) data from NASA satellites, manually mapped by Greenstone et al. (2022) to the geographic locations of the monitoring stations on the ground.

- `city_info.dta`: mapping from unique city codes to city names, including a column for the official deadline for implementing the new reporting and monitoring technology (automation deadline).

- `station_list.dta`: geographic locations of the monitoring stations as well as the actual automation date for each station.

- `weather.dta`: daily weather data (wind speed, precipitation, temperature, and humidity) for each monitoring station from the National Oceanic and Atmospheric Administration (NOAA). Greenstone et al. (2022) geographically matched the weather stations with the closest pollution monitoring station.

In addition, three other files are available, the first two of which will be used in Appendix A1:

- `mask_filter_search.dta`: daily online search data for face masks and indoor air filters, scraped from the website Baidu.

- `search_sale.dta`: monthly search and sales data on a city-level, including several other variables from the paper "Self-protection investment exacerbates air pollution exposure inequality in urban China" (Sun et al. 2017).

- `pm10_corrected_reference.dta`: monthly $PM_{10}$ data corrected retrospectively using a neural network.


Greenstone et al. (2022) used the statistics program *Stata* for data preparation and analysis, which is why the data sets they provided are stored in *Stata’s* standard file format `.dta`. To learn more about loading these files in R, open the info box below.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Basic R functionality and how to load .dta files")
```

Before beginning data preparation, it is important to consider the purpose of the data sets. Our primary objective is to estimate the effect of the automation of air pollution reporting and monitoring on publicly available air pollution data. A potential increase could be interpreted as evidence for data manipulation prior to the implementation of the new automation technology. For such an analysis we require air pollution data and weather data, as weather can have an impact on air pollution as mentioned in *Exercise 1.1*. If alternative data sources for air pollution are available, such as local monitoring stations versus satellite data, we may analyze both and compare the results. Since the data is derived from various sources, we will have to merge data sets as necessary. For instance, when we need station-specific pollution data and also the station’s automation date or geographic location. Although it is not part of the primary analysis, we may also examine whether the automation had any impact on people’s protective behavior, such as online searches for and purchases of air filters and face masks. This secondary analysis is part of Appendix A1.

The first step in preparing the data is to load or *read* the files into R. If you are unfamiliar with this process, please refer to the information box above.

**Task:** Fill in the gaps (___). First, load the `haven` package using the `library()` function. Second, use `read_dta()` to read the `pollution.dta` file and assign the resulting data to a variable named `pollution_data`.

```{r "5_1"}
#load package
library(___)

#load data
pollution_data <- ___(___)
```


To display the data that was just loaded, there are several options available. One simple option is to click the `data` button within the RTutor problem set. When using *RStudio* (a well-known IDE or Integrated Development Environment for R), we have a similar option by clicking on the variable name on the right side in the *Environment* tab. Additionally, we can use R code to view the data. The most basic option is to type the variable name and click `check` (or `run` in *RStudio*).

**Task:** Display the data by calling the variable in which we saved the data set. 

```{r "5_2"}
#simply type the name of the variable you just defined
```


Well, we were not able to display the entire data set, only the first 15 rows. There is often a default limit on the number of rows displayed to conserve computing capacity, and manually reviewing all rows (in this case, 1,433,568) would not make sense anyway. If we intentionally wanted to view only the first few rows, we could have used the `head()` function. Another way to briefly verify that the data was loaded correctly is to draw a random sample using the `slice_sample()` function. Can you guess how it works?

**Task:** Apply the `slice_sample()` function to the `pollution_data` data set, set the `.data` argument to `pollution_data` and the `n` argument to `3`, separating them with a comma. If you are unsure how to do this, try using the `hint` button.

```{r "5_3"}
#apply the `slice_sample()` function
```


Quiz: What is the purpose of the `slice_sample()` function above?

[1]: It randomly selects 3 observations from the data set.
[2]: It selects 150 observations from the data set which meet specific criteria.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("slice_sample()")
```

The data set comprises five columns. The first column, labeled $station\_n$, corresponds to the monitoring station’s ID. This ID is directly linked to $code\_city$, an identifier representing the city. However, city names are not present in the data set, making it challenging to identify the cities. The second column is $date$, displaying a 5-digit number, followed by three columns containing daily average data for $PM_{10}$ (in $\mu g/m^3$), $NO_2$ (in parts per billion, ppb), and $SO_2$  (in ppb). Some cells in the data set may contain the letters “NA” (Not Available), which means there are some missing values. Next, we will determine the number of stations in the data set by utilizing the `length()` and `unique()` functions.

**Task:** Fill in the gaps (___). Use the `length()` and `unique()` functions on the $station\_n$ variable of the `pollution_data` data frame. 

```{r "5_4"}
length(___(___$station_n))
```


Quiz: Which operator can we use to access a single column of our data frame?

[1]: the ! operator
[2]: the | operator
[3]: the + operator
[4]: the $ operator

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Operators")
```

The data set contains 654 individual stations, as expected. If we had only used the `unique()` function, we would have obtained a list of each unique station ID. In the upcoming chapters, it would be more useful to explain results based on city names rather than numeric IDs. Fortunately, we have another file that includes a mapping from city codes to names, which we can merge with the `pollution_data` data frame. To do this, we need to load the other file (`city_info.dta`) and assign it to a variable, which we will call `city_names`. Then, we can use the `head()` function to display the first three rows of `city_names`, which requires an additional argument since the default is to show only the first six rows.

**Task:** Fill in the gaps (___). Read the `city_info.dta` file and save its contents in a variable called `city_names`. Then, use the `head()` function on `city_names` to display the first three rows. If you are unsure of how to proceed, you can use the `hint` button for assistance.

```{r "5_5"}
#load package
library(utils)

#read data
___ <- read_dta("___")

#apply head() function
___(city_names, ___)

```


As expected, the data set contains the city ID $code\_city$ as well as the English name and a variable called $phase$, which will be explained later. In the next step, we will merge the `pollution_data` and `city_names` data frames. As you may have noticed, the data sets are stored in a structured and two-dimensional tabular format, commonly referred to as a *data frame*. There are several ways to combine data frames, one of which is available through the `dplyr` package. We will be using this package frequently throughout this RTutor problem set and you will find an info box on it further down. However, we will now employ the base R function `merge()`, which serves the same purpose. We could save the merged data frames in the *old* variable `pollution_data` (more precisely we would replace the *old* variable with a *new* variable that has the same name and thereby replace all data stored in the *old* variable) or we could save the merged data in an entirely new variable.

**Task:** Use the `merge()` function to combine `pollution_data` and `city_names` based on their shared variable $code\_city$ (column name). Save the result in the *new* data frame `pollution_city`, and display a random sample of three rows.

```{r "5_6"}
#merge data frames
___ <- merge(pollution_data, ___, by = "___")

#draw random sample
___(pollution_city, ___)
```


It worked, we created a new data frame that contains data from two original data frames. Unfortunately, the order of the variables is not ideal, but we will get there soon. In the upcoming regression exercises, we require both weather and pollution data. Therefore, we will merge the newly created data frame `pollution_city` with weather data from the file `weather.dta`, yet this time we will use the `left_join()` function from the `dplyr` package. Previously, we merged two data frames based on one common variable, $code\_city$. However, when we compare the column names of `pollution_city` and `weather_data`, we find two columns with the same name since both data frames contain daily observations for each station. Thus, we need to merge the data based on both $date$ and $station\_n$. The `left_join()` function retains all observations from the data frame listed in the first position of the function (which corresponds to the `x` argument of the `left_join` function) and adds new columns (variables) only for observations already existing in that data frame.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("dplyr package")
```

**Task:** Fill in the gaps (___). To merge data from `pollution_city` and `weather_data` and store it in a new variable `pollution_weather`, we will use the `left_join()` function from the `dplyr` package and apply the pipe operator `%>%`. It is important to note that `pollution_city` is indirectly used as the `x` argument of `left_join()`. Lastly, to display three random observations from the resulting data frame, we can use the `slice_sample()` function.

```{r "5_7"}
#load weather data
weather_data <- ___("weather.dta")

#merge data frames
pollution_weather <- pollution_city %>%
  ___(weather_data, by=c("___", "date"))

#draw random sample
slice_sample(pollution_weather, n = ___)

```

The weather data as well as the pollution data are now part of the newly created data frame called `pollution_weather`. As shown above, we included the following weather variables: $wind\_speed$ (in m/s), $rain$ (precipitation in mm), $temp$ (in $^{\circ}C$), and $rh$ (relative humidity in %). Since the variables $city\_en$ and $phase$ are still somewhere in between the pollution and the weather variables, we will now reposition them using the `relocate()` function.

**Task:** Click on `check`.

```{r "5_8"}

pollution_weather <- pollution_weather %>%
  relocate(c(city_en, phase), .after=code_city)

slice_sample(pollution_weather, n = 3)
```




Quiz: What is the purpose of the `c()` function, which we used in the two previous code chunks?

[1]: The letter "c" stands for "combine", and the function is used to combine values into a vector.
[2]: The letter “c” stands for “cut”, and the function is used to cut elements from our data set (such as the columns “station_n” and “date” used above).
[3]: Neither of the above is correct.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("c function")
```


After rearranging the order of the columns, the data is now almost prepared for the upcoming exercises. To add the actual automation date $auto\_date$ to `pollution_weather`, we have to again merge it with another data set. Since this would only be a repetition, this step was already performed in advance. The final version of the data frame will be named `station_day` since it contains daily data for each station.

In the info box above, you may have noticed the numerous useful functions provided by the `dplyr` package. Have a look at the next code chunk and its output. Are you able to answer the questions?

**Task:** Click on `check`.

```{r "5_9"}

#load package
library(dplyr)

#apply dplyr() functions
pollution_weather %>%
  select(code_city, city_en, station_n, date, pm10, temp) %>%
  filter(city_en == "Shanghai" & !is.na(pm10))%>%
  arrange(-pm10)%>%
  head(3)
```

Please note that the output of the functions was displayed directly and not saved in a new variable this time since it is just an example and is not needed in the upcoming analysis.


Quiz: Choose the `dplyr` function we us to keep only the desired column(s).

[1]: select()
[2]: filter()
[3]: arrange()
[4]: mutate()

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dplyr 1")
```


Quiz: Choose the correct `dplyr` function that sorts observations based on one or more variables.

[1]: select()
[2]: filter()
[3]: arrange()
[4]: mutate()

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dplyr 2")
```


Quiz: Select the `dplyr` function that retains only the observations which meet specific criteria.

[1]: select()
[2]: filter()
[3]: arrange()
[4]: mutate()

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dplyr 3")
```

Next, we will examine the unconventional five-digit date format that we have encountered in our data sets. This format is the default in *Stata* and is based on the origin date 1960-01-01. We will utilize the `mutate()` function from the `dplyr` package to create a new column with a more familiar date format.

**Task:** Fill in the gaps (___) by using the `mutate()` function and the `as.Date()` function (from the `lubridate` package) to create a new date column with a different format in the `pollution_weather` data frame. Once completed, use `slice_sample()` again but this time we include the function directly in the `dplyr` block.

```{r "5_10"}
#load package
library(___)

#create new column and draw random observations
 ___ %>%
  ___(date_2 = ___(date, origin="___")) %>%
  slice_sample(___ = 3)

```



Quiz: Which function would we use if we wanted to place the new $date\_2$ column next to the $date$ column?

[1]: mutate()
[2]: arrange()
[3]: head()
[4]: relocate()
[5]: merge()

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dplyr 4")
```

To complete this exercise, we will apply two more helpful functions from the `dplyr` package. In *Exercise 5*, we require monthly instead of daily $PM_{10}$ data for certain analyses. In this example, we set a filter to keep only data from 2012. Then, using the functions `mutate()` and `as.Date()`, we create new variables for the year and the month. Afterward, we apply `group_by()` to virtually group the data based on the month. Next, we utilize the `summarize()` function in combination with the `mean()` function to calculate the monthly averages for $PM_{10}$ while also removing any NA observations. This procedure is also quite helpful because we will frequently use pre-automation yearly averages as references when interpreting results in subsequent exercises.

**Task:** Click on `check`.

```{r "5_11"}
#apply dplyr() functions
monthly_pm <- pollution_weather %>%
  mutate(year = year(as.Date(date, origin="1960-01-01")),
         month = month(as.Date(date, origin="1960-01-01")))%>%
  filter(year == 2012)%>%
  group_by(month)%>%
  summarize(mean_pm10= mean(pm10, na.rm = T))

monthly_pm

```


Quiz: What was the average $PM_{10}$ in December 2012 (in $\mu g/m^3$, rounded to three decimal places)?

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Average pm10 Dec 2012")
```

If you want to learn more about Greenstone et al.’s (2022) data preparation process, an R replication of all major (and intermediate) files, which were originally created in *Stata*, will be published together with this RTutor problem set.


## Exercise 3 -- Regression Discontinuity Designs - Implementation of New Air Pollution Reporting and Monitoring Technology

*Exercise 3* consists of three parts. In *Exercise 3.1*, we introduce the most important method in this RTutor problem set called RD design. We also explain why we use this method when analyzing the implementation of a new air pollution monitoring and reporting technology in China. In *Exercise 3.2*, we analyze *Station 1491*, which was chosen randomly, and gradually add new elements to the model in order to learn step by step how RD designs work. We then apply this model to the data of Xi’an, the city where *Station 1491* is located. Finally, in *Exercise 3.3*, we replicate the core findings from Greenstone et al. (2022) using the previously established models on the entire data set.

## Exercise 3.1 -- Methodological and Context-Specific Introduction

Regression Discontinuity (RD) design, sometimes also RDD, is the central tool throughout *Exercise 3*. Instead of the acronym RDD we will mostly use RD design since it seems to be more commonly used in the relevant literature (e.g., Calonico et al. (2019) or Lee and Lemieux (2010)) and it was also used by the authors of the original paper (Greenstone et al. 2022). RD design is considered a *quasi-experimental* research design (Lee and Lemieux 2010, 281) that was first used in 1960 by Thistlethwaite and Campbell (1960) to analyze how merit awards (scholarships based on performance) impact future academic results. Although the research design has been around for several decades, it only gained popularity in the late 1990s (Lee and Lemieux 2010, 339–342). Lee and Lemieux (2010, 339–342) provide an interesting overview of RD design applications in economics which might be interesting to you. Before discussing how and why this method is suitable in the underlying scenario, we will briefly review its characteristics and why it is considered quasi-experimental. If you are not familiar with basic regression analysis, please refer to Appendix A2 of this RTutor problem set. You might also find helpful information there in case you are struggling with some of the quizzes in the upcoming exercises.

In RD designs, the assignment to the treatment and control groups is based on a specific variable known as the assignment, running, or forcing variable (Calonico et al. 2014b, 2296; 2015, 38). This variable has a defined threshold or cutoff value where the probability to receive treatment jumps **discontinuously** (Calonico et al. 2014b, 2296). In the scholarship example mentioned above, the observed test score serves as the assignment variable and the cutoff value is the minimum test score required to receive the scholarship while the dependent variable is the future academic outcome. The researchers aimed to estimate the treatment effect, which is the difference between future academic outcomes of the control group (students who scored below the cutoff) and the treatment group (students who scored above the cutoff), with a focus on those scoring close to the cutoff. Assuming certain conditions are met, the random assignment around the cutoff can provide enough variation to consider the observations on the left side as the control group and those on the right side as the treatment group. Note that in RD designs the independent variable is only observed, it is not manipulated by the researchers in form of a defined and deliberate treatment. Therefore, we consider this a *natural* or *quasi-experiment* (Sekhon and Titiunik 2017). To estimate the size of the treatment effect, we use **regression** analysis, hence the term **Regression Discontinuity** design.


Quiz: How did the merit scholarships affect future academic outcomes? Have a guess.

[1]: Lower future academic outcomes in the treatment group compared to the control group (negative estimated treatment effect at the cutoff score).
[2]: Higher future academic outcomes in the treatment group compared to the control group (positive estimated treatment effect at the cutoff score).

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Merit scholarships")
```

We now turn our attention to China and the implementation of new air pollution reporting and monitoring technology, which we also refer to as the *automation of air pollution reporting and monitoring* or simply as the *automation*. The data set created in *Exercise 2* includes daily air pollution data spanning several years. In this scenario, we can use **time** (days and months) as the assignment variable and the city-specific automation dates as cutoff values. This approach is known as Regression Discontinuity in Time (RDiT) and differs slightly from regular RD designs, as Hausman and Rapson (2018) explain in detail. Throughout this problem set, we will use the acronym RD (design) for simplicity reasons, although RDiT would be technically correct. We will create several RD designs for different levels of our data (station-, city-, and country-level) and various subsets and specifications. The objective of a sharp RD design is to estimate the coefficient representing the estimated *average treatment effect* (**ATE**) at the cutoff (Imbens and Lemieux 2008, 618). According to Pellegrini et al. (2013), who cite Lee (2008), we can extent this rationale to any observation that has a positive probability of being in proximity to the cutoff point. For simplicity reasons, we also use the term *treatment effect* in this context. While RD graphs are easy to understand and an excellent way to grasp the concept, the details of RD designs can be more complex. Additional information boxes and links to further resources for the tricky parts will be provided throughout this RTutor problem set.

Before starting the practical analysis using RD designs, we will briefly summarize the topic of our investigation. As part of the *War on Pollution*, the Chinese central government initiated the automation of air pollution reporting and monitoring in major cities, starting with the most relevant ones. By the time the *War on Pollution* was announced, the central government had already started the rollout of new reporting and monitoring technology to upgrade the monitoring stations in many cities. Prior to this, local city governments received the data from their stations before publishing it on a central platform. However, due to the previously described principal-agent problem, local governments may have manipulated their air pollution data for their own benefit, e.g., to receive performance-based rewards or a promotion within the CCP. To investigate this issue, we aim to analyze whether the air pollution reporting and monitoring automation had any effect on the published pollution data, especially $PM_{10}$. Greenstone et al. (2022) consider $PM_{10}$ the most relevant air pollutant in the evaluation process of local governments during the time when the technology was implemented. After the automation, the process no longer required direct involvement from local governments since the data was uploaded directly from the stations. Our main hypothesis is based on the possibility of data manipulation by local governments before the automation. A positive treatment effect, indicating an increase in $PM_{10}$ after the automation, would support the hypothesis and demonstrate that this technology helped to ease the principal-agent problem. We will use data from before and after the automation date which is naturally splitting all observations into a control and a treatment group. All observations from before the automation date are in the control group and all observations from after the automation are in the treatment group, therefore qualifying this setting as quasi-experimental.


Quiz: How will we use the variable $PM_{10}$ in upcoming RD designs?

[1]: dependent variable
[2]: assignment variable
[3]: other variable

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("PM10 variable")
```


Quiz: We can distinguish between two types of RD designs, sharp and fuzzy. Sharp RD designs involve a cutoff that immediately divides the units into treatment and control groups, while fuzzy RD designs have a transitional range around the cutoff. Regarding the described scenario from above, which type of RD design should we apply?

[1]: sharp
[2]: fuzzy

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Sharp vs Fuzzy RD")
```

With the basics of RD designs established and the analysis objective defined, we will commence with the RD equation from Greenstone et al. (2022), which is the foundation for the RD design in this problem set.

$$
P_{ict} = \beta_1 I(t \geq Auto_{ict}) + \beta_2 f(t - Auto_{ict}) + \beta_3 I(t \geq Auto_{ict}) \times f(t - Auto_{ict}) +\beta_4W_{ict} + \alpha_i + month_t + u_{ict}
$$

The elements of the equation are defined in the following way: 

- $P_{ict}$: air pollution level, most of the time represented by $PM_{10}$.

- $t$: time variable (date or month) used to construct functions and indicator variables. In our data set, $t$ is a continuous, five-digit number representing the date (see *Exercise 2*).

- $Auto_{ict}$: actual automation date, again in the same five-digit format. 

- $I(t \geq Auto_{ict})$: indicator variable (which we also call dummy variable). It is binary and takes on the value 0 for all $t$ which are smaller than the automation date (all observations before the city-specific automation date). Consequently, all observations larger than or equal to the automation date receive a value of 1.

- $f(t - Auto_{ict})$: control function, where $t - Auto_{ict}$ is the running or assignment variable. It is aligned to the automation date (set to 0) which means it is negative for all observations before and positive for all observations after the automation date.

- $\beta$: coefficients to be estimated, with $\beta_1$ being of particular interest since its estimate, $\hat \beta_1$, is representing the (estimated) average treatment effect at the cutoff. As we will use ready-to-use functions from the `rdrobust` package, we will only receive that coefficient as an output. Refer to Appendix A2 if you want to learn more about the interpretation of coefficients in simple regression models.

- $W_{ict}$: summarizing several weather variables, including temperature, precipitation, wind speed, and humidity.

- $\alpha_i$ and $month_t$: fixed effects which will be explained later.

- $u_{ict}$: disturbance or error term. 

Indices:

- $i$: station-index

- $c$: city-index

- $t$: time-index (day or month)



Quiz: Why do we standardize the automation date and set it to 0?

[1]: To account for the city-specific automation dates (not all cities automated on the same day).
[2]: It automatically splits the observations in a treatment and a control group.
[3]: Its purpose is to get rid of the 5-digit date format.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD equation")
```

In case this equation is slightly overwhelming, we will break it down step by step and have a look at all components in detail in *Exercise 3.2*. 

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Parametric vs. non-parametric RD")
```

Although there is no statistical test to prove the validity of RD designs, we can evaluate their underlying assumptions. When these assumptions hold, RD designs are considered “the most credible and internally valid quantitative approach for the analysis and interpretation of non-experimental data” (Cattaneo and Vazquez-Bare 2017, 135). Lee and Lemieux (2010, 330) suggest testing the following two assumptions (*Exercise 5*):

- We assume that the automation of reporting and monitoring is the sole cause for the discontinuity in the assignment variable (cutoff date). Consequently, we assume that observable covariates (variables that may have an effect on the dependent variable but are not explicitly of interest) are continuous because they should not be affected by the treatment (Imbens and Lemieux 2008, 631–632). We will utilize weather variables as covariates, which have an impact on pollution levels but should not be influenced by the automation of air pollution reporting and monitoring. Note that we only make an assumption here on the continuity of the covariates, not their relationship with the assignment or dependent variable as in e.g., Instrumental Variable (IV) estimation (Lee and Lemieux 2010, 283). Hahn et al. (2001) consider this continuity assumption to be crucial for the validity of the RD design.

–	To conduct a (regular) RD design, it is assumed that subjects of interest cannot precisely manipulate the assignment variable. Referring back to the example from the beginning of this exercise, this is because if the students were able to control their score (assignment variable) precisely, it would remove the random variation and result in a higher density of observations after the cutoff, known as a bunching effect. However, it is important to note that manipulation itself is not the main issue; it is only a problem if the subjects can *precisely* control their results, which violates the assumption (Lee and Lemieux 2010, 295). In the case of an RDiT design, where the assignment variable is time, this type of manipulation is mostly irrelevant, as Hausman and Rapson (2018) point out. There is no precise manipulation around the cutoff because time is continuous and dates are evenly distributed by design. Therefore, we would not expect to see a bunching effect around the cutoff as long as data is reported continuously. However, if this is not the case and data is not reported continuously, we need to consider another type of manipulation that is specific to this RDiT. As we hypothesized, officials may have been able to change measurements or deliberately not report any measurements at all before the automation, potentially to cover up tampering. This could have resulted in far more observations available after the cutoff than before the cutoff. For now, we assume that observations are equally distributed before and after the cutoff.

Moreover, we can formulate additional assumptions, some of which are specific to the given scenario and only possible due to the broad range of available data (e.g., $NO_2$, $SO_2$, and *AOD* data). We will test the following additional assumptions to gain further insight into the validity of the RD design.

–	While RD designs are one approach we can use in this scenario, we will also employ a variation of the Difference-in-Differences (DiD) method known as the DiD event study estimation and compare the results. We assume that using another method should not yield a considerably different outcome (*Exercise 4*).

–	RD designs offer several options for specifying the model, such as the choice of kernel, order of the polynomial, or bandwidth. To ensure the validity and robustness of our results, we will conduct sensitivity analyses by repeating the analysis with varying model specifications. Specifically, we will test the impact of different kernel functions, orders of the polynomial, and bandwidths on our results. By doing so, we can assess the sensitivity of our findings and determine the extent to which the estimated treatment effect is affected by the choice of model specification. We assume that different model specifications do not lead to significantly different estimates.

- Greenstone et al. (2022, 64) have indicated that other pollutants, such as $NO_2$ or $SO_2$, are unlikely to have significant impacts on the promotion of local government officials. As a result, we expect that a treatment effect at the cutoff for these pollutants is smaller or nonexistent when compared to $PM_{10}$. To test this assumption, we will modify the regression models using $NO_2$ and $SO_2$ as the dependent variable. This will enable us to compare the magnitude of the estimated treatment effects across different pollutants and confirm the specificity of our results to $PM_{10}$.

- If there was an additional and objective data source that also measures $PM_{10}$ or a proxy for $PM_{10}$, we would not expect any effect at the cutoff there. Luckily, data for *AOD*, a potential proxy for $PM_{10}$, is available in one of our data sets. Even though this kind of measurement is less accurate, we would not expect a treatment effect for *AOD* at the automation date (cutoff).

- As each station in our data set has only one actual automation date, we will introduce several placebo cutoffs in our analysis, following the recommendation of Imbens and Lemieux (2008, 632). These placebo cutoffs will be placed at dates other than the actual automation date, where no treatment effect is expected. If our RD design is valid, we should not find similar treatment effects at placebo cutoffs.

Greenstone et al. (2022) tested several of these assumptions as well, however, the test on the bunching effect, the bandwidth specifications, and the placebo test were added in this RTutor problem set.

As mentioned earlier, we will use time as the assignment variable and the actual automation date as the cutoff value. However, it is worth noting that there are three official deadline dates, as the Chinese government phased the implementation of air pollution reporting and monitoring in three waves (Greenstone et al. 2022, 58):

–	*Wave 1:* 74 cities (496 stations) in major economic areas and the Yangtze and Pearl River Deltas as well as the provincial capitals. The automation deadline for this wave the January 1st, 2013.

-	*Wave 2:* 116 cities (449 stations) in key environmental cities and model cites for environmental protection. The automation deadline for this wave was January 1st, 2014.

-	*Wave 3:* 177 cities (552 new stations) that had no prior monitoring. The government planned to build these stations by November 2014.




Quiz: Although the three waves include many cities, our data set only consists of 123 cities (654 stations). Can you speculate on the reason for this discrepancy?

[1]: The data was restricted by the Chinese central government and is therefore not available.
[2]: There is no pre-automation data for all cities as there was previously no monitoring in some cities.
[3]: The scientists were only interested in specific cities.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("123 cities")
```


The following code chunk provides a histogram for the *actual* automation dates, which can differ from the deadlines set by the Chinese central government. The $phase$ variable, which we already encountered in *Exercise 2*, contains the values `1` and `2`, indicating if the city was assigned to *Wave 1* or *Wave 2*. As announced, we will now use the `station_day` data set, which also includes the $auto\_date$ variable containing the actual automation date of each station. To generate visualizations, we will use the `ggplot2` and `dplyr` packages. Please note that all required data sets from now on are saved in `.RDS` (R Data Serialization) files which allows for a faster import into R compared to e.g., `.csv` files.

**Task:** Click on `check`. 

```{r "7_1",fig.width=8, fig.height=5, fig.align='center'}
#load data
station_day <-readRDS("station_day.RDS")

#create graph
histo_auto <- ggplot(data = station_day %>%
                    group_by(station_n)%>%
                    slice(1))+
  geom_histogram(aes(x=auto_date, y = ..density.., 
                     fill = as.factor(phase)), bins = 24)+
  scale_fill_manual(values = c("slategrey", "slategray2"))+
  scale_x_continuous(breaks = c(18993, 18993+180, 19359, 19359+180, 19724), 
                     labels = c("Jan 2012", "Jul 2012", "Jan 2013", 
                                "Jul 2013","Jan 2014"))+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)+
  labs(title = "Automation Date Histogram", fill = "Wave")+
  xlab("Automation Date")+
  ylab("Density")

#display graph
histo_auto

#Greenstone et al. (2022): Figure A5 (Appendix)

```



Quiz: Which waves are included in our data set?

[1]: Wave 1
[2]: Wave 2
[3]: Wave 3

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Waves in data set")
```



Quiz: There are some cities which already automated their monitoring and reporting before the deadline. Should we include those cities in our RD designs (have a guess)?

[1]: yes
[2]: no
[3]: it depends

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Automation before deadline")
```

The histogram shows that the majority of cities in both waves automated exactly on the respective deadline. However, it is also visible now that a considerable number of cities automated their monitoring stations earlier. In our core analysis in *Exercise 3.3*, we will include all 123 cities from the `station_day` data set, comprising 60 cities in *Wave 1* and 63 cities in *Wave 2*. However, to investigate the possibility of local governments using an earlier automation date to mask underreporting, we will conduct a separate analysis for cities that automated before the deadline, as suggested by Greenstone et al. (2022, 59). You can find the analysis of pre-deadline cities in Appendix A4. Yet prior to examining the entire data set, we start in *Exercise 3.2* with a random station and systematically develop and explain the RD design used in the core analysis.


## Exercise 3.2 -- Analysis of Station 1491 and Xi'an

We begin this exercise by presenting the pollution data collected at *Station 1491*, which was chosen arbitrarily. Our objective for this exercise is to step by step develop a model, which we can then apply in the next exercise on the entire data set. It is important to note that $PM_{10}$ is the most relevant air pollutant for this RTutor problem set, as it was monitored before and after the automation date. Additionally, it was, according to Greenstone et al. (2022), the most relevant air pollutant in the evaluation of local city governments during the time when the technology was implemented. Our first step is now to create a graph showing the $PM_{10}$ concentration over time for *Station 1491*, which is in the city of *Xi’an*. But before we can create the graph, we have to perform some modifications to our data set.

**Task:** Fill in the gaps (___) with the correct functions and variable. We only use data from *Station 1491* and only observations where $PM_{10}$ and the weather variables are available. Therefore, we employ the functions `filter()`, to filter the observations, and `is.na()`, which returns only rows where **no** data is available. However, if we combine it with the `!` operator, it only returns rows where data **is** available. The `&` operator ensures that all conditions are satisfied, otherwise the observation is filtered out.

```{r "8_1"}
#load data
station_day <- ___("station_day.RDS")

#prepare data
data_1491 <- station_day %>% 
  ___(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & 
           !is.na(temp) & !is.na(rh) & ___ == 1491)

#display sample
slice_sample(data_1491, n = 3)

```

<br/>

Next, we use `ggplot2` functions to create the desired graph using the data frame `data_1491` we just prepared.

**Task:** Click on `check`.

```{r "8_2",fig.width=8, fig.height=4, fig.align='center'}

#create and display graph
ggplot(data = data_1491)+
  geom_point(aes(x=date, y= pm10), col = "lightgrey")+
  geom_vline(xintercept =  19359, col = "Red")+
  scale_x_continuous(breaks = c(18628, 18993, 19359, 19724, 
                                20089, 20454, 20820), 
                     labels = c(2011:2017))+
  xlab("Year")+
  ylab(expression("PM"[10]))+
  labs(title= expression("Station 1491 (Daily Average PM"[10]*")"))+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10,)+
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

```


The data points represent the daily average $PM_{10}$ at *Station 1491*, which means each point is equivalent to one observation. To plot the data points, we used the `geom_point()` function. The y-axis shows the $PM_{10}$ values, while the x-axis shows the corresponding dates from 2011 to 2017. The automation date for this station (January 1st, 2013) is marked by a red line using the `geom_vline()` function. Overall, we observe a cyclical pattern and a potential increase in $PM_{10}$ after the vertical red line.


Quiz: Which automation wave did *Station 1491* belong to?

[1]: Wave 1
[2]: Wave 2

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Station 1491 Wave")
```



Quiz: The city of *Xi'an*, where *Station 1491* is located, has a four-season climate. Based solely on the graph, do you think pollution levels in *Xi'an* are higher in summer or winter?

[1]: summer
[2]: winter

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Pollution Xi'an summer vs winter")
```

To analyze the data further, we can employ `dplyr` functions again to calculate the annual means of $PM_{10}$.  

**Task:** Fill in the gaps (___). Group the data and apply the `mean()` function on the $PM_{10}$ $(pm10)$ variable. 

```{r "8_3"}

#prepare and display data
data_1491%>%
  ___(year)%>%
  summarise(mean_pm10 = ___(pm10))


```

<br/>

You may have already noticed that we randomly selected a station located in an area with heavy pollution. As we learned in *Exercise 1.1*, the WHO recommends an annual average of $15\ \mu g/m^3$ for $PM_{10}$, a standard that was exceeded every year in our data set for this location. Of particular significance here is the annual average for 2012 $(82.8\ \mu g/m^3)$ since it is the year prior to the automation. We will often use the average $PM_{10}$ from the year before the automation as a reference for the estimated treatment effects.

Our standard for the upcoming RD **graphs** is to limit observations to one year before and after automation which is consistent with Greenstone et al. (2022).


Quiz: Which `dplyr` function can we use if we want to limit the time interval to one year before and one year after the automation?

[1]: select()
[2]: filter()
[3]: mutate()

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dplyr question")
```



Quiz: Based on your observations so far, do you anticipate a positive estimated treatment effect at the cutoff in the upcoming analysis?

[1]: yes
[2]: no

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Goal of RD designs")
```

We return to the RD equation from the previous exercise, but this time it is a shorter version and we only use the time index $t$ since we are only utilizing data from one station, which is logically situated in a single city.

$$
P_{t} = \beta_1 I(t \geq Auto_{t}) + \beta_2 f(t - Auto_{t}) + \beta_3 I(t \geq Auto_{t}) \times f(t - Auto_{t}) + u_{t}
$$

All elements in the equation are defined as in *Exercise 3.1*. It includes $P_t$, representing daily air pollution, and the dummy or indicator variable $I(t \geq Auto_{t})$. For *Station 1491*, this dummy variable is set to 0 for all observations before January 1st, 2013 (the automation date), and to 1 for all observations after this date. As previously noted, $t - Auto_{t}$ is the assignment variable. We still include an interaction term (if you are not sure why, have a look again at the info box regarding parametric and non-parametric models). In this simple model, we do not include any other variables or fixed effects.

We will now use the function `rdrobust()` from Calonico et al.’s (2015) `rdrobust` package, which is crucial for this RTutor problem set, to estimate the model outlined above. Note that this function was also employed by Greenstone et al. (2022) since it is also available in *Stata*. We have to specify the following arguments to use the `rdrobust()` function (specifications based on Greenstone et al.(2022):

- `y` is the vector that includes the $y$-values or in other words the values of the dependent variable $PM_{10}$.

- `x` is the vector of the $x$-values, which are the values of the assignment variable `diff_days`. Instead of using the $date$ variable with the `c` argument of `rdrobust()`, we created a new variable called `diff_days` which contains the differences between the automation date and the date of each observation. For example, if an observation is from 50 days before the automation, the `diff_days` value would be -50, and if it is from 50 days after the automation, the value would be +50. In the equation, $t - Auto_{t}$ is equivalent to `diff_days`.

–	`c` represents the RD cutoff value which we do not explicitly specify here since the default value is 0 and we created the `diff_days` variable which standardizes the automation date for all stations as 0. We therefore have negative values before the automation date and positive values after the automation date.

–	`p` specifics the order of the local polynomial of the regression function (we will use `p` = 1 which is equivalent to a local linear regression. Since `p` = 1 is the default, we would actually not have needed to specify it explicitly, but it also does no harm to do so for transparency reasons).

- `q` specifies the local-polynomial which is used for the construction of the bias-correction (we will keep the default of `q` = 2 which is a local quadratic regression).

–	`vce` defines the computation of the variance-covariance matrix estimator (`vce` = “hc0” is what we use for our regressions; hc0 is a “heteroskedasticity-robust plug-in residuals variance estimator without weights” (Calonico et al. 2022, 4).

-	`kernel` specifies the kernel function (we use the triangular kernel).

–	`masspoints` checks if there are repeated observations in our assignment variable. It will be turned off for all our RD designs since we use time or a proxy for time (`diff_days`) as the assignment variable. When looking at only one station, there is no repetition possible since there is only one observation per date.


Quiz: Why does it make sense to create the additional variable `diff_days` instead of simply specifying the `c` argument of `rdrobust()`?

[1]: The performance of the function is better when we create a separate variable.
[2]: It increases the chance that the RD estimate is biased when using the c argument.
[3]: The c argument only allows one specific automation date but we want to include multiple stations with different automation dates in one model.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("c argument rdrobust")
```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Kernel functions")
```


**Task:** Fill in the gaps (___). First, we have to load the `rdrobust` package and create the described `diff_days` variable. We then use $PM_{10}$ from the `data_1491` data set as the $y$ argument in the `rdrobust()` function. Afterward, we will display the estimated coefficients of `rd_table_1491`.

```{r "8_5"}

#load package
library(___)

#prepare data
data_1491 <- data_1491 %>% 
  mutate(diff_days = date - auto_date)

#run rdrobust() function
rd_raw_1491 <- ___(y = data_1491$___, x = data_1491$diff_days,
               p = 1, q = 2, kernel = "tri", vce = "hc0", 
               masspoints = "off")

#display estimated coefficients
___$coef

```

<br/>

We can access the output of the `rdrobust()` function, which we saved in the variable `rd_raw_1491` by using the `$` operator followed by the so-called `Value` we are interested in. Besides the estimated coefficient (`$coef`) of the assignment variable, which represents the average treatment effect, we can also retrieve other *Values* such as bandwidth (`$bws`), standard errors (`$se`), and many others. See https://rdpackages.github.io/rdrobust/ for further details. Note that we also had to use the `$` operator within the `rdrobust()` function to extract the variables from out data set since it is not possible to use `dplyr` code within the `rdrobust()` function.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Bandwidth")
```

Another way to display the results is by using the `modelsummary()` function. Please note that `modelsummary()` can only be used in conjunction with `rdrobust()` due to additional lines of code running in the background, which can be found on GitHub. This package is a great way to customize the output of regressions in tabular form. There are many design and customization options to choose from (also in combination with the `kableExtra` package) yet we will not cover them in this RTutor problem set. If you want to learn more about these packages, refer to their documentation, tutorials on the internet, or Maier’s (2022) RTutor problem set.

**Task:** Click on `check`.

```{r "8_6"}
#load packages
library(modelsummary)
library(kableExtra)
library(tibble)

#create tabular RD output
modelsummary(list(rd_raw_1491), coef_omit = 2:3, coef_rename = "ATE", 
             stars = TRUE, add_rows = 
               tribble(~A, ~B,
                       "Sample", "Station 1491")%>%
               structure(position = 3),
             title = "RD Design - Station 1491")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

```

<br/>


Quiz: What is the estimated treatment effect for this station (in $\mu g/m^3$, three decimal places)?

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD estimation station 1491")
```

In case you are wondering why we omitted the other estimates in the specification of the `modelsummary()` function, have a look at the next info box.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Conventional vs bias-corrected vs robust bias-corrected RD estimates")
```


Quiz: Which additional factors (variables) did we already include in the previous model?

[1]: weather (rain, temperature, etc.) only
[2]: seasonality only
[3]: daily weather and seasonality
[4]: no other factors

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Rdrobust with or without covariates")
```

What can we infer from these `rdrobust()` results? First of all, we should keep in mind that we are currently analyzing a single station. The estimated average treatment effect, represented by the $\hat \beta_1$ coefficient, is $120.822\ \mu g/m^3$ and significant at the 1% level (refer to Appendix A2 for details on significance levels). Compared to the yearly average $PM_{10}$ of *Station 1491* in 2012, which was $82.764\ \mu g/m^3$, the estimate is almost 46% larger. The standard error is rather unremarkable at $40.443$, which is around one third of the estimate. It is also interesting to note that the default optimal bandwidth selection based on Calonico et al. (2014b) results in the same bandwidth on both sides of the cutoff (128 days each).


Quiz: The variables in a simple linear regression models might be endogenous as one regresses only one variable on another variable and therefore risking omitted variable bias (Appendix A2). In the model of the RD design, we also included only two variables, $diff\_days$ and $PM_{10}$ (pm10). Is endogeneity a potential problem here?

[1]: Yes, there is most likely bias and we have to include further variables.
[2]: No, as long as the continuity assumption for the covariates holds.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Result validity")
```


It might seem that in order to obtain decent a model in an RD design, we need to include all possible covariates to minimize the residuals $\hat u$ and account for potential confounding factors. However, this is not necessarily the case, as the key assumption for RD designs is that all covariates are continuous around the cutoff. This means that an increase in the dependent variable can only be attributed to the treatment assignment when passing the cutoff value. In addition, the dates of the automation are also considered to be exogenous by design (Greenstone et al. 2022, 60; Lee and Lemieux 2010, 297). Nevertheless, it makes sense to include covariates and fixed effects to increase accuracy and validity by controlling for potential confounding factors.

To graphically display RD designs, the `rdrobust` package includes a built-in function called `rdplot()`. However, before applying the function, we need to specify some arguments which are not available or defined differently in `rdrobust()`:

–	`p` is now defined as the order of the *global* polynomial instead of the *local* polynomial for the approximation of the “population conditional mean function” (Calonico et al. 2022). For an illustration of different polynomials, see the info box below.

–	`nbins` directly specifies the number of bins into which the data is split. If `nbins` is not specified, one can choose from several procedures in `binselect` to find the optimal number of bins. We include each day as a separate bin by specifying the `nbins` argument, the `binselect` argument is therefore not needed.

–	The other specified arguments are only relevant for graphical aspects. There are many other arguments in the `rdplot()` function which can be looked up in the `rdrobust` documentation (https://cran.r-project.org/web/packages/rdrobust/rdrobust.pdf).

**Task:** Click on `check`.

```{r "8_8",fig.width=8, fig.height=4, fig.align='center'}

#prepare data
rd_graph_1491 <- data_1491 %>% 
  filter(diff_days >=-364 & diff_days<=364)

#run rdplot2()
rd_raw_graph_1491 <- 
  rdplot2(rd_graph_1491$pm10, rd_graph_1491$diff_days, p= 3, 
          nbins = c(364, 364), kernel = "tri", col.lines = "black", 
          col.dots = "lightgrey", x.label = "Days before/after Automation", 
          y.label = expression("Raw PM"[10]), 
          title = expression("RD Design - Station 1491 (Raw PM"[10]*")"), 
          scale_x_breaks = seq(-360,360,120), scale_y_breaks = seq(0,800,200), 
          blank_vert_grid_lines = T, shade = F)
#display graph
rd_raw_graph_1491$rdplot 
```

<br/>

The main differences between the new graph generated by the `rdplot2()` function and the graph from the beginning of this exercise are the time interval and the line fitted to the data. With these kinds of graphs, we can now easily visualize the average treatment effect at the cutoff. However, we must be careful because the line fitted by the `rdplot2()` function is only an approximation. The treatment effect calculated with the accompanying `rdrobust()` function may be methodologically different due to differences in argument definitions (see documentation of functions for details). As cautioned by Lee and Lemieux (2010, 283-284), RD graphs can be helpful but should not be solely relied upon to determine the presence or absence of any effect.


Quiz: In which period is the variation of $PM_{10}$ higher?

[1]: before the automation
[2]: after the automation

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD results graph")
```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("rdplot vs rdplot2")
```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Order of Polynomials")
```

As mentioned before, it is good practice to use a second model which is based on the first model but now includes additional factors. Using `rdrobust()`, there are two main options for the implementation: using the function’s `covs` argument or utilizing a two-step method that residualizes the dependent variable (Greenstone et al. 2022, 60). In this RTutor problem set, we will use the latter option by utilizing the `felm()` function from the `lfe` package. This method, which is explained in further detail in Lee and Lemieux (2010, 331-334), requires an intermediate step of running a separate multiple linear regression using `felm()` before the actual RD estimation. If you are not familiar with linear regression or the `felm()` function, please refer to Appendix A2. The advantage of using `felm()` is that it allows for easy control of fixed effects and the inclusion of clustered standard errors. Furthermore, there is another benefit that we will discover later. Since we are using the `felm()` function in conjunction with `dplyr` functions, there is no need to specify the data argument. Instead, we only need to specify the argument for the linear regression model. However, we do not know yet what the required linear regression model looks like.

Starting with the previous equation, we have to add the weather covariates, which were aggregated in the variable $W_{t}$ in the last exercise, as well as the month fixed effects $month_t$ to the model.

$$
P_{t} = \beta_1 I(t \geq Auto_{t}) + \beta_2 f(t - Auto_{t}) + \beta_3 I(t \geq Auto_{t}) \times f(t - Auto_{t}) + \beta_4 rain_t + \beta_5 wind\_{speed}_t + \beta_6 temp_t + \beta_7 rh_t + month_t + u_{t}
$$

Before we can actually estimate this model, we have to perform the first step of the two-step process. Therefore, we basically remove the complete section which comprises the RD design, which leaves us with the weather variables and the month fixed effects. Since this is a different model, we use different coefficients $(\omega$ instead of $\beta)$. 


$$
P_{t} = \omega_0 +  \omega_1 rain_t + \omega_2 wind\_speed_t + \omega_3 temp_t + \omega_4 rh_t  +  month_t + \varepsilon_t
$$

With this model, we can now estimate the effect of precipitation, wind speed, temperature, and relative humidity on $PM_{10}$ while including month fixed effects to control for seasonality. Fixed effects are in general used to account for unobserved factors, that are assumed to be constant, in a regression model. Time fixed effects are used when we assume that there are unobservable factors, which vary over time but are constant across entities. In this case, the fixed effect of $month_t$ is used to capture the seasonal variation over several years (we use all observations of *Station 1491* here, not only from one year before and after the automation). As soon as we include stations from different cities in our regression model (e.g., in the next exercise), entity fixed effects can be used to account for unobservable factors that are assumed to be constant over time but vary across entities. Possible examples are station-specific locations (e.g., varying altitudes or proximity to a forest compared to a busy road) and potential inconsistencies or variations in the quality of the measuring equipment at each station. As we will later see, it is also possible to add both time and entity fixed effects to the same model. When adding fixed effects to a model, it is also called a fixed effects model.

**Task:** Fill in the gaps (___). We will use the `felm()` function from the `lfe` package to estimate the model above. We add the $month$ variable after the `|` operator to include the fixed effects. Afterward, we will display the estimated coefficients using `modelsummary()`.

```{r "8_10"}
#load package
library(___)

#perform linear regression
reg_1491 <- ___(pm10 ~ rain + wind_speed + temp + rh | ___, 
                  data = data_1491)


#create tabular regression output
___(reg_1491, stars = TRUE, gof_omit = "BIC|AIC", 
             add_rows = tribble(~A, ~B,
                                "Sample", "Station 1491")%>%
               structure(position = 9))%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

```

<br/>

The estimated coefficients $\hat \omega$ represent the change in $PM_{10}$ $(P_{t})$ when a variable increases by one unit, while assuming that all other variables remain constant (ceteris paribus). Due to the fixed effects included in the model, the y-intercept $\omega_0$ is not shown here as it now represents one of the fixed effects (dummy) coefficients, which are generally not displayed. Below the estimates, we find the respective standard errors. In the lower part, we find summary statistics referring to the entire model.


Quiz: When temperature increases by $1^{\circ} C$, how does $PM_{10}$ (in $\mu g/m^3$) change (refer to appendix A2 in case you need help)?

[1]: It decreases by 7.055.
[2]: It increases by 7.055.
[3]: There is no effect.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("felm interpreation 1 ")
```




Quiz: What is the standard error of the wind speed variable (in m/s, three decimal places)?

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("felm interpreation 2 ")
```


Briefly summarized, we found that when $rain$ (precipitation) or $wind\_speed$ increase, $PM_{10}$ decreases, while when $temp$ or $rh$ (relative humidity) increase, $PM_{10}$ increases. The estimates for $wind\_speed$, $temp$, and $rh$ are significant at the 0.1% level, the estimate for $rain$ at the 1% level. The weather variables and month fixed effect explain 26.1% (`R²`) of the variation in $PM_{10}$ at this station.

However, our main interest lies in understanding how the automation of air pollution reporting and monitoring has affected published $PM_{10}$, not just how weather affected $PM_{10}$. Hence instead of using the variation we just *explained* by the multiple linear regression we will now extract the variation which was **not** explained by it and use it as the dependent variable in our model. These remaining, unexplained variations in our data are the residuals $\hat \varepsilon$. Luckily, the `felm()` function can be combined with the `resid()` function (from the `stats` package) to extract these residuals. Using only these residuals is the equivalent of including them directly as covariates in the RD model.

**Task:** Click on `check`. We will estimate a new model that utilizes **residual** $PM_{10}$ instead of **raw** (reported) $PM_{10}$ as the dependent variable, with the goal of improving our model’s accuracy. With `modelsummary()`, we can again display and compare the results.

```{r "8_11"}
#prepare data (extract residuals)
data_1491 <- data_1491 %>%
  mutate(resids = resid(felm(pm10 ~ wind_speed + rain + temp + rh | month)))

#run rdrobust() function
rd_resid_1491 <- rdrobust(data_1491$resids, data_1491$diff_days, c = 0, 
                          p = 1, q = 2, kernel = "tri", vce = "hc0", 
                          masspoints = "off")

#display and compare RD output
modelsummary(list(rd_raw_1491, rd_resid_1491), coef_omit = 2:3, 
             coef_rename = "ATE", stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C,
                                "Sample", "Station 1491", "Station 1491",
                                "Month fixed effects","","X",
                                "Weather covariates","","X")%>%
                        structure(position = c(3:5)),
              title = "RD Design - Station 1491")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

```

<br/>

We now virtually included the weather factors as covariates in our RD design using this two-step approach. 


Quiz: Did we anticipate a similarity between this outcome and the one without covariates?

[1]: yes
[2]: no

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Covariates included")
```

The estimated treatment effect using residual $PM_{10}$ is $124.992\ \mu g/m^3$, which is now significant at the 0.1% level. This effect is slightly larger than in the first model where we did not include covariates or time fixed effects. The standard error decreased by 26.0%. An improvement in significance combined with a smaller standard error while receiving a similar RD estimate for the treatment effect when adding covariates and fixed effects to the model indicates not only accuracy but also demonstrates the robustness of the model. However, it is important to note that this result is based on only one station. In *Exercise 5*, we will further analyze the impact and relevance of weather covariates. Please refer to the info box below if you are interested in a graphical comparison between the two models.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Graphical comparison of the two RD designs")
```

We consider the data published after the automation date to be less susceptible to manipulation, hence the positive and significant estimated treatment effects we found in both models can be considered as a first indication for data manipulation prior to the automation. Regarding the principal-agent problem, the city government of *Xi'an*, were *Station 1491* is located, potentially underreported $PM_{10}$. To further investigate this suspicion, we will analyze next whether similar results are found for the other six stations in *Xi'an*.

Before we proceed with the analysis, there is one more aspect regarding *Station 1491* that warrants attention. When looking closely at the graphs of this station, we find that $PM_{10}$ on the days right after the cutoff is similar to $PM_{10}$ on the days before the cutoff. The automation *treatment* appears to be some days later, after a short period without any reported data. We could therefore suggest that the automation date for this station is not correct in our data. The analysis of this issue, which is included in Appendix A3, indicates that the date is potentially incorrect for all seven stations of *Xi’an* as well as for stations in one other city. While a manual correction of the automation date on the station- or city-level leads to a noticeable increase in the estimated treatment effect for those stations and cities, the overall effect when using the entire sample seems to be minor. This potential issue is not discussed by Greenstone et al. (2022). Therefore, we will continue our analysis with the original data.

For the sake of completeness, we will briefly analyze the results for *Xi’an* which comprises seven stations, including *Station 1491*. Therefore, we have to slightly modify our code: instead of setting a filter for the station, we now need to include a filter for the city ID variable $code\_city$, which is `610100` for *Xi'an*.

**Task:** Click on `check`.

```{r "8_13"}

#prepare data
data_610100 <- station_day %>% 
  filter(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & !is.na(temp) & 
           !is.na(rh) & code_city == 610100)%>% 
  mutate(diff_days = date - auto_date,
         resids = resid(felm(pm10 ~ wind_speed + rain + temp + rh | 
                               station_n + month)))

#run rdrobust() functions
rd_raw_610100 <- rdrobust(data_610100$pm10, data_610100$diff_days, c = 0, 
                          p = 1, q = 2, kernel = "tri", vce = "hc0", 
                          masspoints = "off")

rd_resid_610100 <- rdrobust(data_610100$resids, data_610100$diff_days, c = 0, 
                            p = 1, q = 2, kernel = "tri", vce = "hc0", 
                            masspoints = "off")

#display and compare RD estimates
modelsummary(list(rd_raw_610100, rd_resid_610100), 
             coef_omit = 2:3, coef_rename = "ATE", stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C,
                                "Sample", "Xi'an", "Xi'an",
                                "Month fixed effects","","X",
                                "Weather covariates","","X")%>%
                        structure(position = c(3:5)),
            title = "RD Design - Xi'an")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

```



Quiz: What is the purpose of the newly added $station\_n$ variable within the `felm()` function?

[1]: Entity fixed effects to account for multiple stations in Xi'an.
[2]: Adding stations from other cities to the regression model.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Additional felm input")
```

We will not analyze the RD estimates for *Xi’an* in detail since the results and their interpretation are similar to those of Station *1491* for both models. However, including data from seven stations led to standard errors of less than half the size compared to the previous models. In the next exercise, we will continue our analysis and apply the models explained in this exercise on the entire data set.



## Exercise 3.3 -- Countrywide Analysis and Core Findings

We have now arrived at the exercise where we will apply our previously established RD design on the entire data set. Once again, we revisit the equations from *Exercises 3.1* as they are central to this analysis. By now, we have explained all the necessary elements of these models to analyze the data of 123 cities. In the first model, we use **raw** or reported $PM_{10}$ as the dependent variable. The second model was established by applying a two-step approach which allows us to use **residual** $PM_{10}$ as an independent variable hence indirectly including covariates and fixed effects in the model of the RD design.

**RD design - Raw $PM_{10}$:**

$$
P_{ict} = \beta_1 I(t \geq Auto_{ict}) + \beta_2 f(t - Auto_{ict}) + \beta_3 I(t \geq Auto_{ict}) \times f(t - Auto_{ict}) + u_{ict}
$$

**RD design - Residual $PM_{10}$:** 

$$
P_{ict} = \beta_1 I(t \geq Auto_{ict}) + \beta_2 f(t - Auto_{ict}) + \beta_3 I(t \geq Auto_{ict}) \times f(t - Auto_{ict}) +\beta_4W_{ict} + \alpha_i + month_t + u_{ict}
$$

Again, we will use the `rdrobust()` function introduced in the last exercise. Before we start, we have to briefly prepare the data (remember, we performed these steps in the last exercise only on a subset of the data). These data preparation steps are similar to before yet without including any station or city filters.

**Task:** Fill in the gaps (___) with the appropriate functions and variables.

```{r "9_1",fig.width=8, fig.height=4}

#load data
station_day <- ___("station_day.RDS")

#prepare data
rd_data <- station_day %>%
  ___(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & 
        !is.na(___) & !is.na(rh))%>%
  mutate(diff_days = date - auto_date,
         resids = ___(felm(pm10 ~ wind_speed + rain + temp + rh | 
                             station_n + month)))
```


As on the city-level, we again include both time and entity fixed effects. We continue with the estimation of the RD treatment effects for both models using the data set prepared above. The next task is a repetition for the specification of the `rdrobust()` function.

**Task:** Fill in the gaps (___).

```{r "9_2"}

#run rdrobust() functions
rd_raw <- ___(rd_data$pm10, rd_data$diff_days, c = 0, p = 1, 
              q = 2, kernel = "___", vce = "hc0", 
              cluster = rd_data$code_city, 
              masspoints = "off")


rd_resid <- rdrobust(rd_data$___, rd_data$diff_days, c = 0, p = 1, 
                     q = 2, kernel = "tri", vce = "hc0", 
                     cluster = rd_data$code_city, 
                     masspoints = "off")


#display and compare RD output
modelsummary(list(___, rd_resid), coef_omit = 2:3, 
             coef_rename = "ATE", stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C, 
                                "Sample", "All", "All",
                                "Station fixed effects","","X",
                                "Month fixed effects","","X",
                                "Weather covariates","","X")%>%
                        structure(position = c(3:6)),
             title = "RD Design - All 123 Cities (Complete Sample)")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

#Greenstone et al. (2022): Table 1 - Panel A, column 1 and 2

```



Quiz: What is the average treatment effect when we use **residual** $PM_{10}$ as the dependent variable (in $\mu g/m^3$, three decimal places)?

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Result resdiual PM10")
```



Quiz: Comparing the results obtained from the entire sample to those obtained from *Station 1491*, what observation can be made?

[1]: The treatment effect is smaller in the entire sample, but the standard error increased.
[2]: While the rectangular kernel was used for station 1491, the triangular kernel was used for the entire sample.
[3]: The estimates are almost identical compared to those of the previous exercise.
[4]: The treatment effect and the standard error are considerably smaller for the entire sample.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Comparison RD results")
```

When using reported $PM_{10}$ as the dependent variable, we find an estimated treatment effect of $34.717\ \mu g/m^3$, which is significant at the 1% level, for the entire sample of 123 cities. For the second model, which includes weather covariates and fixed effects, the estimate is almost identical at $34.893\ \mu g/m^3$. While the estimate from the first model is significant at the 1% level, the second estimate is significant at the 0.1% level. More importantly, the standard error of the second estimate, which is $5.786$, is 45.8% smaller than the first model’s standard error of $10.682$. Moreover, the second model’s calculated optimal bandwidth is larger than that of the first model.

What do these results imply concerning the main hypothesis of this RTutor problem set (and the underlying paper)? The estimated treatment effect for the entire sample of around $35\ \mu g/m^3$ is equivalent to around 78% of the yearly average WHO guideline for $PM_{10}$ and corresponds to an increase of roughly 40% relative to the yearly average pre-automation $PM_{10}$ of $86.1\ \mu g/m^3$. If the assumptions from *Exercise 3.1* hold, the results suggest that on the exact day when air pollution reporting and monitoring was automated in each city, $PM_{10}$ concentrations increased significantly. This provides plausible support for our initial hypothesis that air pollution data was manipulated by local governments before the reporting and monitoring were automated. However, before we can make any conclusions, we should analyze if there are any relevant subgroups in our data that contributed overproportionally to the overall result. Further, we have to test the outlined assumptions (*Exercise 5*).

You might have noticed one difference in the specification of the `rdroubst()` functions above in comparison to those of the last exercise. We added the `cluster` argument to account for the city-clusters in our data which is relevant for the calculation of standard errors. We analyze our data on the station level while the automation date is the same for all stations in one city. To account for the resulting autocorrelation, we use clustered standard errors. There is an info box on this topic in the next exercise.

Have a look at the graphs below, which show the corresponding RD plots created with `rdplot2()`.

**Task:** Click on `check`.

```{r "9_3",fig.width=10, fig.height=4}

#prepare data
rd_graph <- rd_data %>% 
  filter(diff_days >= -364 & diff_days <= 364)

#run rdplot2() using raw PM10 - without covariates
rd_raw_graph <- 
  rdplot2(rd_graph$pm10, rd_graph$diff_days, p= 3, nbins = c(364, 364),
          kernel = "tri", masspoints = "off", ci = 95, col.lines = "black",
          col.dots = "lightgrey", x.label = "Days before/after Automation", 
          y.label = expression("Raw PM"[10]),
          title = expression("RD Design - Complete Sample (Raw PM"[10]*")"), 
          scale_x_breaks = seq(-360,360,120),scale_y_breaks = seq(50,200,50),
          blank_vert_grid_lines = T, shade = F) 

#run rdplot2() using residual PM10 - including covariates
rd_resid_graph <- 
  rdplot2(rd_graph$resids, rd_graph$diff_days, p= 3, nbins = c(364, 364), 
          kernel = "tri", masspoints = "off", ci = 95, col.lines = "black", 
          col.dots = "lightgrey", x.label = "Days before/after Automation", 
          y.label = expression("Residual PM"[10]),
          title = 
            expression("RD Design - Complete Sample (Residual PM"[10]*")"), 
          scale_x_breaks = seq(-360,360,120),scale_y_breaks = seq(-50,50,50),
          blank_vert_grid_lines = T, shade = F)

#display RD graphs
plot_grid(rd_raw_graph$rdplot, rd_resid_graph$rdplot)

#Greenstone et al. (2022): Figure 1 - Panel A and B

```


Quiz: Compare the y-axis range of both graphs. Why is it smaller when using residual $PM_{10}$?

[1]: The felm() function used to construct the residuals smooths the data automatically, reducing the overall range.
[2]: The residuals represent the remaining variation that was not explained by the intermediate regression. As a result, we anticipate a narrower range since some of the initial variation has already been explained by the inclusion of covariates and fixed effects.
[3]: The difference in y-axis range could also be due to the use of different bin sizes, with the graph on the right having more observations in each bin.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD graph - entire sample")
```


We will analyze next if these results are indicative of pre-automation data manipulation for the entire sample or only for certain subsamples. To investigate further, we will start with the *Wave 1* and *Wave 2* subsamples described in *Exercise 3.1* which have different official automation deadlines. For this purpose, we have to filter our data set again before we can estimate our established models on these subsamples.

**Task:** Click on `check`. The $phase$ variable in the data frame indicates the automation deadline and thereby divides the data into the subsamples *Wave 1* and *Wave 2*.

```{r "9_4"}

#prepare data
rd_data_w1 <- rd_data %>%
  filter(___ == 1)

#run rdrobust() function
rd_resid_w1 = rdrobust(rd_data_w1$resids, rd_data_w1$diff_days, c = 0, 
                       p = 1, q = 2, kernel = ___, vce = "hc0", 
                       cluster = rd_data_w1$code_city, 
                       masspoints = "off")

#prepare data 
rd_data_w2 <- rd_data %>%
  filter(phase == 2)

#run rdrobust() function
rd_resid_w2 <- rdrobust(rd_data_w2$resids, rd_data_w2$___, c = 0,
                        p = 1, q = 2, kernel = "tri", vce = "hc0", 
                        cluster = rd_data_w2$code_city,
                        masspoints = "off")

#display and compare RD output
___(list(rd_raw, rd_resid, rd_resid_w1, rd_resid_w2), 
             coef_omit = 2:3, coef_rename = "ATE", stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C, ~D, ~E,
                                "Sample", "All", "All","Wave 1", "Wave 2",
                                "Station fixed effects","","X","X","X",
                                "Month fixed effects","","X","X","X",
                                "Weather covariates","","X","X","X")%>%
                        structure(position = c(3:6)))

#Greenstone et al. (2022): Table 1 - Panel A, column 1, 2, 3, and 4

```


Quiz: For which subsample do we find a larger estimated treatment effect?

[1]: Wave 1
[2]: Wave 2

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD results - waves")
```

We find significant estimated treatment effects for both waves. The estimated treatment effect for *Wave 1* $(27.545\ \mu g/m^3)$ is similar compared to that of the entire sample, while the effect for *Wave 2* cities was much larger $(64.653\ \mu g/m^3)$. Unfortunately, 47 out of the 123 cities in our data set did not report sufficient $PM_{10}$ data for more than two months (60 days) in the four months (120 days) before the automation date, with no specific reason given. Interestingly, 45 out of the 47 cities below the defined threshold for missing data in those four months were from *Wave 2*, compared to only two from *Wave 1*. Moreover, 43 of those 45 *Wave 2* cities did not report **any data** at all during that period. Greenstone et al. (2022) conducted further RD designs using data from the 76 cities with less than two months of missing data. However, they did not mention that 68.3% of *Wave 2* cities did not report any data in the four months before the automation, which means that the treatment effect for *Wave 2* cities relied heavily on the data from those cities which did report data during that period. Furthermore, the subsample including the 76 cities we will use below contains only 23.7% *Wave 2* cities.

**Task:** Click on `check`.

```{r "9_5",fig.width=10, fig.height=5}
#load data
rd_graph_76 <- readRDS("rd_graph_76.RDS")


#create graph and display
ggplot(data = rd_graph_76[[1]], aes(x= city_N, y= rd_estimate))+ 
  geom_linerange(data = rd_graph_76[[1]], 
                 aes(ymin =l_95_1116, ymax = h_95_1116))+
  geom_point(size = 0.9, aes(col = l_95_1116 >0, 
                             shape = l_95_1116 >0))+
  scale_color_manual(values = c("black", "red"), 
           labels = c("ATE",
                      "ATE positive and significant at 5%"))+
  scale_shape_manual(values = c(19, 15), 
           labels = c("ATE",
                      "ATE positive and significant at 5%"))+
  geom_hline(aes(yintercept= mean(rd_estimate)), linetype = 2, 
             col = "red")+
  geom_hline(aes(yintercept= -100), linetype = 3)+
  geom_hline(aes(yintercept= 0), linetype = 3)+
  geom_hline(aes(yintercept= 100), linetype = 3)+
  scale_y_continuous(breaks = c(-100,0, 28.2, 100, 200), 
                     labels = c("-100", "0", "Mean", "100", "200")) +
  scale_x_continuous(breaks =  rd_graph_76[[3]]$x1, 
                     labels = rd_graph_76[[2]])+
  ylab("ATE")+
  xlab("City")+
  labs(title="City-specific RD Estimates - Subsample of 76 Cities")+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)+
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),
        legend.position = c(0.22, 0.85),
        legend.title = element_blank(),
        )+
  geom_segment(data = rd_graph_76[[3]], aes(x = x1, y = y1, 
                                            xend = x2, yend = y2), 
               arrow = arrow(length= unit(0.15, "cm")))

#Greenstone et al. (2022): Figure 2

```


Quiz: What does the result indicate when using the subsample of 76 cities? Is it consistent with previous estimates?

[1]: The results confirm previous findings indicating large scale underreporting, but not all cities show a significant and positive effect.
[2]: The results confirm previous findings indicating large scale underreporting in all cities in this sample.
[3]: The results contradict all previous findings.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("76 cities sample")
```

In this graph, which shows individual RD estimates for each city of the outlined subsample, we observe positive and significant estimates (shown in red) for 33 out of the 76 cities. The vertical lines represent the 95% confidence intervals and the horizontal dotted line represents the average of the individual RD estimates $(28.2\ \mu g/m^3)$. These findings suggest that these cities manipulated their air pollution data before the automation of reporting and monitoring. Additionally, we observe negative treatment effects in some cities, which are not significant and not plausibly explainable in connection with the automation event. Overall, these results strengthen our hypothesis that data manipulation was an extensive issue rather than limited to a few cities.

We can split this subsample of 76 cities further using the deadline dates (*Wave 1* and *Wave 2*). We will use this data set in the info box below and repeat the previous analysis.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Analysis of subsample")
```

There is another option to split the data set into subsamples: as we know, the deadline for the automation was determined by the central government for each city. However, city governments were given the option to implement the technology prior to the designated deadline. Since it is possible that city governments strategically chose their automation dates to hide underreporting, we split the data into a *deadline* and a *pre-deadline* subsample. The analysis of these subsamples is included in Appendix A4 of this RTutor problem set. Summarized, we find a substantially larger estimate for the deadline sample $(57\ \mu g/m^3)$ compared to the entire sample, while the effect in the pre-deadline sample was close to 0 and insignificant. Greenstone et al. (2022) only show the results for the deadline sample and suggest that it is plausible that local governments strategically choose their automation dates to hide underreporting. However, there is also the possibility that pre-deadline cities were simply less engaged in data manipulation, which could have had a potentially similar outcome. Although the effect of the deadline sample was almost 64% larger compared to the effect of the entire sample, they argue that the influence of pre-deadline cities on the overall estimate “is at most modest” (Greenstone et al. 2022, 63).

Briefly summarizing this exercise, the estimated treatment effects of our various models for the entire sample and subsamples suggest that the relationship between the Chinese central and local governments was burdened by a principal-agent problem, leading to widespread data manipulation before the technology was implemented. While we found positive and significant treatment effects for the entire sample of 123 cities (raw and residual $PM_{10}$), 47 of these cities had missing data issues before the automation date (mainly *Wave 2* cities), making it impossible to analyze these cities individually. Overall, out of the 76 cities with less than 60 days of missing observations before the automation, 33 showed a positive and significant treatment effect. For these cities, we could therefore argue that the reporting automation not only improved data quality, but it also helped to mitigate the principal-agent problem. In the next exercise, we will conduct another analysis using a different method to compare the results. Additionally, we will test our assumptions from *Exercise 3.1* in *Exercise 5* to find out if our RD design is valid.


Quiz: What is the main conclusion of this exercise?

[1]: The results support the hypothesis of widespread pre-automation data manipulation.
[2]: The reporting and monitoring automation had no effect on data quality.
[3]: We found positive and significant treatment effects in all 123 cities analyzed.
[4]: The principal-agent problem was likely not related to data manipulation.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD conclusion")
```




## Exercise 4 -- Difference-in-Differences Event-Study Analysis

We used the RD design in the last exercise to estimate the short-term effects of the automation. In this exercise, we will employ another method called Difference-in-Differences (DiD) event-study analysis to compare the results from both methods and also to estimate the medium-term effects of the automation. This method is also known as dynamic DiD or staggered DiD and is applicable in scenarios with several periods and some form of treatment received by only one group of the sample (e.g., Marcus & Sant’Anna (2021)). Referring to Wooldridge (2013, 458-459), the **basic** DiD approach can be expressed as follows:

$$
y = \beta_0 + \delta_0After +  \beta_1Treat +  \delta_1After\times Treat + \varepsilon
$$

In this equation, we use two dummy variables: $After$, which takes on the value 1 for all observations in the post-treatment period and 0 for all pre-treatment observations, and $Treat$, which is 1 for all subjects who received the treatment and 0 for those in the control group. If we assume the error term $\varepsilon$ to be 0, the estimate $\hat{\delta}_1$ of $\delta_1$, which is in front of the interaction term of the two dummy variables, is equal to the result of taking the actual differences as explained below.

$$
\hat \delta_1 = (\bar y _{2,T} - \bar y _{2,C}) - (\bar y _{1,T} - \bar y _{1,C})
$$

Here, $T$ and $C$ refer to the treatment and control group, respectively, and the numbers $1$ and $2$ denote the pre- and post-treatment periods. The $\hat{\delta}_1$ represents the DiD estimator, which is the difference of the differences between the average outcomes $(\bar{y})$ of the treatment and control group in the pre- and post-treatment periods. Since the DiD method is not as relevant for this problem set compared to RD designs, we will not go further into theoretical details yet if you want to find out how to interpret the coefficients in such a (simple) DiD study, have a look at the next info box. Further down, you will also find an info box that contains an application of this simple DiD approach.


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Interpretation of DiD coefficients")
```

In the upcoming DiD estimation, we will include covariates and fixed effects in our DiD regression model. When we add covariates and fixed effects, the result of the regression will differ from manually subtracting the differences yet the interpretation of the coefficient $\delta_1$ remains similar (Wooldridge 2013, 458).

DiD event studies are useful when multiple treatment events occur and not all subjects receive treatment at the same time. Although our case satisfies this condition, we do not use the DiD event study in that way. Instead, our scenario implies that the treatment occurs once, and subjects remain treated indefinitely (see Athey and Imbens (2022) for details). We use the following setting: for *Wave 1*, the automation date (treatment event) is January 1st, 2013, and for *Wave 2* it is January 1st, 2014. We only include cities in this analysis that automated exactly on the deadline date during 2012 and 2013. In this time period, the technology was only implemented in *Wave 1* cities. Since the deadline for *Wave 2* cities was neither in 2012 nor 2013, none of these cities was automated in that period. We therefore consider *Wave 1* cities as the treatment group and *Wave 2* cities as the control group, assuming that both waves have parallel trends in the periods before the automation. In the equation above, we used only two periods: the pre-treatment and the post-treatment periods. However, it is possible to include more periods by splitting the two years of data into further sub-periods. The subperiods we will use are described below the following equation, which represents the model for our DiD event study:

$$
P_{ict} =  \gamma_{\tau} \displaystyle\sum_{\tau = -4}^{+3} Auto_{ic\tau} +\beta W_{ict} + \alpha_i + month_t + \epsilon_{ict}
$$

In this DiD event study model, we define the following elements: 

- $P_{ict}$ and $W_{ict}$ are used in the same way as in the previous RD setting.

-	$\alpha_i$ are the station fixed effects, $month_t$ are the month **or** year-month fixed effects. 

- $\epsilon_{ict}$ is the error term.

-	$Auto_{ic\tau}$ represents dummy variables for different time periods:
 
  o	$\tau = -4$: Months 7-12 before the automation date.

  o	$\tau = -3$: Months 5-6 before the automation date.

  o	$\tau = -2$: Months 3-4 before the automation date.

  o	$\tau = -1$: Months 1-2 before the automation date.

  o	$\tau = 0$: Months 1-2 after the automation date.

  o	$\tau = 1$: Months 3-4 after the automation date.

  o	$\tau = 2$: Months 5-6 after the automation date.

  o	$\tau = 3$: Months 7-12 after the automation date.


We use $\tau = -1$ as the reference period, which means that the coefficients $\gamma_{-4}$ to $\gamma_3$ are always relative to this reference. To prevent collinearity, we also exclude $\tau = -1$ from the regression analysis (e.g., Wooldridge 2010). The coefficients $\gamma_{-4}$ to $\gamma_{-2}$ are expected to be constant relative to $\gamma_{-1}$ and non-significant. The coefficients $\gamma_{-0}$ to $\gamma_{3}$ represent the periods after automation and are expected to be significantly higher than the base coefficient if our hypothesis is correct.

We begin with the `station_day` data set, which we already used in the last exercises. First, we load the file again (since it is a new exercise) and add the new variable $year\_month$, which we will apply as fixed effects. Then, we limit the $year$ variable to 2012 and 2013 and only include data from cities that automated as of the official deadline, hence excluding all cities that voluntarily automated earlier. We also drop some of the columns we do not need in this exercise.

**Task:** Fill in the gaps.

```{r "10_1"}

#load data
station_day <- ___("station_day.RDS")

#prepare data
did_data <- ___%>%
  mutate(year_month = paste(year,month, sep = ""))%>%
  filter(year___2012 & year <=2013 & 
           (auto_date == 19724 | auto_date == 19359))%>%
  select(-c(no2, so2))

#draw random sample
slice_sample(did_data, n = 3)

```


The next step involves creating (binary) dummy variables. As in the previous exercises, these variables are assigned a value of 1 if a specific criterion is met, and 0 otherwise. For instance, we have determined that the *Wave 1* cities will serve as the treatment group. As a result, we need a variable that is set to 1 for cities in *Wave 1*, and 0 for those that are in *Wave 2*. Additionally, we require dummy variables that indicate the different time periods we have defined earlier.


Quiz: Which dummy variable has a value of 1 for a random observation that corresponds to the date April 12th, 2013, represented by the number 19460 in the data set?

[1]: m712_before
[2]: m56_before
[3]: m34_before
[4]: m12_before
[5]: m12_after
[6]: m34_after
[7]: m56_after
[8]: m712_after

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Dummy variable DiD")
```


**Task:** Just click on `check`.

```{r "10_2"}

#prepare data
did_data <- did_data %>%
  mutate(treat = ifelse(auto_date == 19359,1,0),
         after = ifelse(year >= 2013,1,0),
         m12_before = ifelse(year==2012 & month>=11 & month<=12,1,0),
         m34_before = ifelse(year==2012 & month>=9 & month<=10,1,0),
         m56_before = ifelse(year==2012 & month>=7 & month<=8,1,0),
         m712_before = ifelse(year==2012 & month>=1 & month<=6,1,0),
         m12_after = ifelse(year==2013 & month>=1 & month<=2,1,0),
         m34_after = ifelse(year==2013 & month>=3 & month<=4,1,0),
         m56_after = ifelse(year==2013 & month>=5 & month<=6,1,0),
         m712_after = ifelse(year==2013 & month>=7 & month<=12,1,0))

```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Simple DiD model")
```

Since we now have dummy variables for both the treatment and the defined time periods, our next step involves generating interaction terms. We do this by multiplying the treatment dummy with each time period dummy. As previously demonstrated in the exercise’s initial example, these interaction terms are necessary for estimating DiD treatment effects.

**Task:** Click on `check`. 

```{r "10_5"}

#prepare data
did_data <- did_data %>%
  mutate(treat_m12_before=treat*m12_before,
         treat_m34_before=treat*m34_before,
         treat_m56_before=treat*m56_before,
         treat_m712_before=treat*m712_before,
         treat_m12_after=treat*m12_after,
         treat_m34_after=treat*m34_after,
         treat_m56_after=treat*m56_after,
         treat_m712_after=treat*m712_after)%>%
  filter(!is.na(wind_speed) & !is.na(rain) & !is.na(temp) & !is.na(rh))

```


Quiz: The same observation as before (from April 12th, 2013) is taken from a station located in a *Wave 2* city. Which interaction dummy variable has a value of 1 for this observation?

[1]: treat_m12_before
[2]: treat_m34_before
[3]: treat_m56_before
[4]: treat_m712_before
[5]: treat_m12_after
[6]: treat_m34_after
[7]: treat_m56_after
[8]: treat_m712_after
[9]: None of them

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Dummy variable DiD interaction")
```

We have now modified our data set by adding all necessary variables. We can now use it for our first DiD event study regressions using the `felm()` function once again. We will estimate two different models.

**Task:** Fill in the gaps (___) using the previously created data frame `did_data`. Add the missing dummy variable and use the function previously used to display the regression results.

```{r "10_6"}

#run "DiD" regression
did_1 <- felm(data= ___, pm10 ~ treat_m712_before + treat_m56_before + 
                treat_m34_before + ___ + treat_m34_after + 
                treat_m56_after + treat_m712_after + wind_speed + rain +
                temp + rh | station_n + month | 0 | code_city,          
              na.action = "na.omit")

did_2 <- felm(data=did_data, pm10 ~ treat_m712_before + treat_m56_before +
                treat_m34_before + treat_m12_after + treat_m34_after + 
                treat_m56_after + treat_m712_after + wind_speed + rain +  
                temp + rh | station_n + year_month | 0 | code_city,  
              na.action = "na.omit")

#display and compare DiD estimates
___(list(did_1, did_2), coef_omit = (8:11), gof_omit = "IC", stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C, 
                                "Sample", "Deadline", "Deadline",
                                "Weather covariates","X","X",
                                "Station fixed effects","X","X",
                                "Month fixed effects","X","",
                                "Year-month fixed effects","","X")%>%
                        structure(position = c(15:19)))

#Greenstone et al. (2022): Table 1 - Panel B, column 1 and 2

```


Quiz: Which variable(s) did we add as fixed effects in first DiD model?

[1]: month
[2]: year_month
[3]: station_n
[4]: code city

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("DiD fixed effects")
```


Quiz: What is the difference between the two DiD models?

[1]: We used different fixed effects.
[2]: We used different data sets.
[3]: We included additional variables in the second model.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("DiD difference")
```

The `felm()` function has been utilized multiple times in this problem set, but there are some differences in its application in this case. Previously, we regressed the dependent variable $PM_{10}$ on the weather variables while including fixed effects and then extracted the residuals thereof. In contrast, in the current application we not only include the weather variables and fixed effects but also the previously created interaction terms (except for the reference period). Therefore, we use the `felm()` function this time not just in an intermediate step but to directly estimate the final regression model. In the `modelsummary()` output, we will follow Greenstone et al. (2022) and omit the estimates for the weather variables as they are not the primary focus. Additionally, we cluster standard errors on the city-level $(code\_city)$ in both models already within the `felm()` function (further details can be found in the info box below).

Regarding the two DiD models we estimated above, they only differ in the choice of fixed effects. In the first model, we utilize fixed effects for the station and month, while in the second model, we use year-month instead of month fixed effects. The first model produces higher estimates for all post-automation periods, especially in the 1-2 months after the automation, where the gap between the first model’s estimate of $60.3\ \mu g/m^3$ and the second model’s estimate of $31.4\ \mu g/m^3$ is considerable, despite similar standard errors. In the 3-4 and 5-6 months post-automation, the estimates become smaller, as does the difference between the two models. Notably, all *Wave 2* cities that automated precisely on the official deadline did not report any data in the four months before their automation date, which was January 1st, 2014. Therefore, there are only two months of data in the 7-12 months post-automation period for the control group.

When we use month fixed effects, we assume that the month-specific variation in the data remains constant over the years (two years in this specific DiD data set), which is why we hold it fixed across both years. However, in the year-month model, we assume that the month-specific variation does not repeat itself in the following year. Both models result in significant and positive estimates for the post-automation periods, but in the first model (month fixed effects), the estimates are not only higher, but they are also declining over the three post-automation periods (ignoring the fourth as explained earlier), while they remain relatively constant in the second model (year-month fixed effects). It is possible that the fixed effects in the year-month fixed effects model absorb more variation in the period right after the automation since the relevant fixed effects for the period after the automation are only based on post-treatment months. In the month fixed effects model, on the other hand, the fixed effects remove less month-specific variation since they are calculated based on data from pre- and post-treatment months.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Clustered standard errors")
```



Quiz: In this exercise so far, we included all *Wave 2* cities in the control group if they were automated precisely on the deadline. What was our underlying assumption when using *Wave 2* cities as a control group for *Wave 1* cities?

[1]: We assumed parallel trends before the treatment (automation).
[2]: We assumed parallel trends after the treatment (automation).

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Treatment and control group")
```

The parallel trend assumption plays an important role in DiD approach (for further details, see Marcus & Sant’Anna 2021). According to Greenstone et al. (2022), the pre-treatment trends for both model specifications above are not entirely satisfying. Therefore, we will try to improve our setting: so far, we included all *Wave 2* cities in our control group without considering their actual geographic location relative to the location of the *Wave 1* cities in the treatment group. As the geographic location might be one reason for the differences in pre-trends, we will only keep those *Wave 2* cities that are geographically closest to the respective *Wave 1* cities and re-estimate the two models.

**Task:** Click on `check`. 

```{r "10_8"}

#load data
did_match_data <- readRDS("did_match_data.RDS")


#run "DiD" regressions
did_match_1 <- felm(data=did_match_data, pm10 ~ treat_m712_before + 
                      treat_m56_before + treat_m34_before +
                      treat_m12_after + treat_m34_after + 
                      treat_m56_after + treat_m712_after + wind_speed + 
                      rain +  temp + rh | station_n + month | 0 | code_city,
                    na.action = "na.omit")

did_match_2 <- felm(data=did_match_data, pm10 ~ treat_m712_before + 
                      treat_m56_before + treat_m34_before +
                      treat_m12_after + treat_m34_after + 
                      treat_m56_after + treat_m712_after + wind_speed + 
                      rain +  temp + rh | station_n + year_month | 0 | 
                      code_city, na.action = "na.omit")

#display and compare DiD estimates
modelsummary(list(did_1, did_2, did_match_1, did_match_2), 
             coef_rename = c("7–12 months before","5–6 months before","3–4 months before","1–2 months after","3–4 months after","5–6 months after","7–12 months after"),
             gof_omit = "AIC|BIC|Std.Errors|R2 Adj.|RMSE",
             coef_omit = (8:11), stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C, ~D, ~E,
                             "Sample","Deadline","Deadline","Match","Match",
                             "Weather covariates","X","X","X","X",
                             "Station fixed effects","X","X","X","X",
                             "Month fixed effects","X","","X","",
                             "Year-month fixed effects","","X","","X")%>%
                        structure(position = c(15:19)),
             title = "DiD event study")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)


#Greenstone et al. (2022): Table 1 - Panel B, column 1, 2, 3 and 4

```

In models $(1)$ and $(3)$, where month fixed effects were used, there is no real improvement in the alignment of pre-automation trends. Further, comparing models $(2)$ and $(4)$ where year-month fixed effects were applied, the estimates become insignificant for the pre-automation periods (which is favorable) however, there is also no noticeable improvement when it comes to the parallelism of pre-trends. The estimates in those periods are now closer to zero but they vary more compared to the specification without geographical matching (model $(2)$). Yet according to Greenstone et al. “the coefficients for all the “lead” variables become small in magnitude and statistically insignificant” (Greenstone et al. 2022), which we cannot entirely confirm at this point. The estimates in model $(3)$ compared to model $(1)$ remain similar while those in model $(4)$ compared to model $(2)$ differ more (especially in the first period after the automation) and some become less significant. In addition, the standard errors increase for both specifications. The following info box contains a graphical representation of model $(4)$.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Graphical results form DiD estimation")
```



Quiz: What is the main conclusion from this exercise?

[1]: DiD is not as effective as RD designs.
[2]: DiD event studies estimates strengthen the findings obtained from prior RD designs and indicate medium-term effects on air pollution levels.
[3]: RD designs are not effective in identifying treatment effects.
[4]: DiD and RD designs have completely different results.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Results of Did event study")
```



## Exercise 5 -- Validity Tests

The results obtained from the RD design in *Exercise 3* were based on certain assumptions, as outlined in *Exercise 3.1*. Although there is no statistical test to definitively prove the validity of RD designs, testing those assumptions can at least gives us an indication to support or dismiss its validity. In the following exercise, we will review and analyze each assumption in detail. Several of them were also tested by Greenstone at al. (2022). For an explanation of these tests, see chapter 4.4 of Lee and Lemieux (2010). Note that we will not explain most of the R code in this exercise since the functions are either known already or used only in this specific exercise and therefore overall, not as relevant. Furthermore, many of the tables and graphs in this exercise are already given to avoid unnecessarily long loading times.

**1. Assumption: Continuity in covariates**

In all of the models in *Exercise 3* that utilized residual $PM_{10}$, we indirectly included weather covariates (temperature, precipitation, humidity, and wind speed) in the estimation of the regression model using `rdrobust()`. Since weather can significantly impact $PM_{10}$, any sudden and random change in weather conditions on the automation date would be equivalent to a discontinuity in the assignment variable and therefore potentially affecting the estimated treatment effect of $PM_{10}$. Therefore, we will examine each weather variable individually to determine if this assumption holds.

<center>
<img src="weather_rd.png" style="max-width:100%"></img>
</center>

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for graph above")
```

In this application, we use each weather variable as a dependent variable rather than an independent variable. By extracting the residuals, we aim to retain the variation that is not explained by the month fixed effects (which are assumed to be constant across all stations but vary over time) or station fixed effects (which are constant over time but vary across stations).


Quiz: Based on the plots, which weather variable appears to have a (small) treatment effect at the cutoff?

[1]: rain
[2]: temperature
[3]: wind speed
[4]: humidity

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Weather covariates")
```

Although the estimated treatment effects in precipitation, humidity, and wind speed appear to be rather small, the residual temperature exhibits a treatment effect of slightly below $1^{\circ} C$ according to the graph. However, as we have learned previously, we should not base our argument solely on an RD graph. Consequently, we will utilize `rdrobust()` function to further analyze the data.

**Task:** Click on `check`. 

```{r "11_2"}

#load data 
station_day <-readRDS("station_day.RDS")

#prepare data
temp_data <- station_day %>% 
  mutate(diff_days = date - auto_date,
         month = month(as.Date(date, origin="1960-01-01")))%>%
  filter(!is.na(wind_speed) & !is.na(rain) & !is.na(temp) & !is.na(rh))%>% 
  mutate(resids_temp = resid(felm(temp ~ 1 |station_n+ month)))
  
#run rdrobust() function
rd_temp <- rdrobust(temp_data$resids_temp, temp_data$diff_days, c = 0, 
                    p = 1, q = 2, kernel = "tri", vce = "hc0", 
                    cluster = temp_data$code_city, masspoints = "off")

#display RD output
modelsummary(list(rd_temp), coef_omit = (2:3), stars = TRUE, 
             coef_rename = "ATE",
             add_rows = tribble(~A, ~B,
                                "Station fixed effect","X",
                                "Month fixed effect","X")%>%
                        structure(position = c(3:4)),
             title = "RD Design - Temperature")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

#Greenstone et al. (2022): Table B2 (Appendix)

```

The pre-automation average temperature was $14.96^{\circ} C$, the treatment effect we found at the cutoff for the residual temperature (although not significant) is $0.919^{\circ}C$. Is this effect even worth discussing? According to Greenstone et al.’s (2022) Appendix B2, all weather variables, including temperature, are distributed continuously although finding the same estimates as we did above. Since the treatment effect it not massive and not even significant, we should not worry too much. Furthermore, the result remains almost unchanged when we use different kernels and is only around $0.5^{\circ} C$ when we use the subsample of 76 cities with better data availability. Still, further references indicating when and how changes in weather factors affect air pollution might have been interesting. Regarding changes in temperature, the quantitative impact on PM concentrations appears to be highly dependent on specific study designs and circumstances. For example, Tai et al. (2010) discovered a positive correlation between temperature and PM $(PM_{2.5})$ emissions in the United States. Conversely, other studies report a negative correlation between temperature and PM, such as Hernandez et al. (2017) or Dawson et al. (2007). Briefly summarized, the insignificant estimate we discovered is most likely irrelevant, however, additional context would have been interesting.


Quiz: Did we actually need to examine all weather variables in detail? Consider the RD designs from previous exercises, particularly the comparison between raw and residual $PM_{10}$ data. Did we indirectly demonstrate that there these weather variables are most likely continuously distributed?

[1]: Yes, we indirectly demonstrated that there is likely no meaningful effect at the cutoff for the weather variables.
[2]: No, we did not indirectly demonstrate that there is likely no effect at the cutoff for the weather variables.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Weather covariates in RD")
```

In fact, when we added weather variables as covariates to our models, the results suggested that there is most likely no significant treatment effect in any covariate since the results remained rather similar. This may also be a reason why Greenstone et al. (2022) did not delve further into the small and insignificant treatment effect when using temperature as a dependent variable. Overall, it appears that the assumption of continuous covariates holds and therefore delivers support for the validity of the RD design.

**2. Assumption: No bunching effect**

As mentioned in *Exercise 3.1*, RDiT does not exhibit a bunching effect similar to a standard RD, as there is no direct manipulation of the assignment variable (time). However, we have suggested the possibility of a different type of bunching effect when cities cease reporting data (prior to the automation) and then resume reporting immediately after the cutoff. In fact, we observed this behavior at the end of *Exercise 3.2* when analyzing the subsample of 76 cities based on data availability. The following two histograms illustrate this effect well.

```{r "11_3",fig.width=12, fig.height=4, fig.align='center'}

#prepare data
phase_1_data <- station_day %>%
  mutate(year = year(as.Date(date, origin="1960-01-01")),
         diff_days = date - auto_date,
         month = as.numeric(month(as.Date(date, origin="1960-01-01"))))%>%
  filter(phase == 1 & diff_days >=-364 & diff_days<=364 & !is.na(pm10))

#create histogram
histo_phase_1 <- ggplot(data= phase_1_data)+
  geom_histogram(aes(x=diff_days), binwidth = 1)+
  scale_x_continuous(breaks= seq(-360, 360, 120))+
  labs(title="Wave 1 Histogram")+
  xlab("Days before/after automation")+
  ylab("Density")+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)


#prepare data
phase_2_data <- station_day %>%
  mutate(year = year(as.Date(date, origin="1960-01-01")),
         diff_days = date - auto_date,
         month = as.numeric(month(as.Date(date, origin="1960-01-01"))))%>%
  filter(phase == 2 & diff_days >=-364 & diff_days<=364 & !is.na(pm10))

#create histogram
histo_phase_2 <- ggplot(data= phase_2_data)+
  geom_histogram(aes(x=diff_days), binwidth = 1)+
  scale_x_continuous(breaks= seq(-360, 360, 120))+
  ggtitle("Wave 2 Histogram")+
  xlab("Days before/after automation")+
  ylab("Density")+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)

#display both histograms
plot_grid(histo_phase_1, histo_phase_2)
```

Particularly for the *Wave 2* cities, we found a higher number of observations after the cutoff compared to before, which could be characterized as a bunching effect. However, we were still able to obtain significant estimates for the *Wave 2* sample. Moreover, by applying the models on a sample with little to no missing data, we accounted for this potential issue. The results were comparable to those obtained from the entire sample, suggesting that the missing data for *Wave 2* is not a big concern. Yet the estimates of the *Wave 1* cities and the sample of 76 cities with better data available might be more credible.

Furthermore, Greenstone et al. (2022) conducted an additional test in their appendix to investigate potential manipulation in the post-automation period. They utilized an AQI (Air Quality Index) to determine if there were any bunching effects of reported $PM_{10}$ around specific AQI thresholds. Their tests, which utilized local polynomial density estimation, did not reveal any evidence of bunching effects around particular AQI values. This suggests that the possibility of data manipulation (such as underreporting) was reduced through automation.

**3. Assumption: Results not dependent on model specifications**

A simple yet effective approach to test the sensitivity of the RD results is to try different model specifications. Throughout *Exercise 3*, we already provided in most cases one model with and one without covariates which did not have a strong impact on the estimates. Additionally, we will now compare different kernel functions, global polynomial models with varying orders of polynomials as well as different bandwidths.


Quiz: We will re-run the model from *Exercise 3* on the complete data set using the triangular, rectangular, and epanechnikov kernels. What do we expect the results to be?

[1]: Distinct from each other.
[2]: Rather similar.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Kernel functions")
```

<center>
<img src="kernels_rd.png" style="max-width:65%"></img>
</center>

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for graph above")
```

Our results indicate that the choice of kernel function does not significantly affect the outcome of our analysis, suggesting that our estimate is robust.

So far, we have utilized the `rdrobust()` function to estimate the models of the RD design, which is both simple and widely used. However, in *Exercise 3.1*, we learned that we can also use global polynomial regression for RD designs which we can implement using the `felm()` function. This requires us to generate new variables, such as a dummy variable indicating pre- or post-treatment observations, various polynomial versions of the assignment variable, and additional dummy variables that are the products of these variables. In addition, we incorporate fixed effects (station and month) and clustered standard errors at the city-level.

<center>
<img src="parametric_rd.png" style="max-width:65%"></img>
</center>

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for graph above")
```


Quiz: Did we limit or select the bandwidth for the global polynomial RD design?

[1]: yes
[2]: no

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Bandwidth global polynomial RD")
```



Quiz: Do the results from the global polynomial approach differ substantially from the results of the local linear approach in *Exercise 3*?

[1]: yes
[2]: no

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Global polynomial RD vs local linear RD")
```

The results show that there are no considerable differences between the results obtained using local linear regression and those above using global polynomial regression. Therefore, both approaches seem to be appropriate for this scenario, which further highlights the robustness of our models and the validity of our RD design. Please note that the results presented in the table above may vary slightly from those in Table B3 of the appendix of Greenstone et al. (2022). This is because the `felm()` function used here apparently has minor functional differences from the `reghdfe` function in *Stata*. The number of observations (1,049,325) is identical in both cases.

As suggested by Lee and Lemieux (2010, 335), we use a graph to show the sensitivity of our (residual) $PM_{10}$ model to a range of bandwidths. We therefore plot the estimated average treatment effects for bandwidths from 4 to 750 days.

**Task:** Click on `check`. 

```{r "11_6",fig.width=10, fig.height=5}

#load data
rd_bws <- readRDS("rd_bws.RDS")

#show graph for different bandwidths
ggplot(data= rd_bws%>%
                  filter(bws<=750))+
  geom_linerange(aes(x = bws, ymin =ci_lower, ymax =ci_upper), 
                 col = "lightgrey")+
  geom_point(aes(y=coef, x= bws, col = pv < 0.05), size = 0.1)+
  scale_color_manual(labels = c("above or equal 5%", "below 5%"), 
                     values = c("black", "red"))+
  geom_vline(aes(xintercept = 264), col = "black", linetype = 2, size = 0.5)+
  scale_y_continuous(breaks = c(-75,-50,-25,0,25,50))+
  xlab("Bandwidth")+ ylab("Estimated Treatment Effect")+
  labs(title = "RD Design - Bandwidth Comparison",
       col = "p-value:")+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 11)+
  ylim(-25,50)+
  theme(legend.position = "bottom")

```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for bandwidth RD design above")
```

Ignoring two outliers for the bandwidths of 4 and 5 days, the estimates are significant from a bandwidth of 33 days. The optimal bandwidth calculated for this model is represented by the black dotted line. Overall, these results suggest that the estimated treatment effect we found is not highly sensitive to changes in bandwidth after passing a certain point.

**4. Assumption: Smaller treatment effect in other pollutants**

According to Greenstone et al. (2022, 64), $NO_2$ and $SO_2$ played only a negligible role in the assessment of local governments during the time of the rollout of new monitoring and reporting technology. Since we have access to pre- and post-automation data for both of these pollutants, we can examine whether a reduced incentive to manipulate these measurements still results in a discontinuity at the automation date and subsequently a relevant treatment effect.


<center>
<img src="so2_no2_rd.png" style="max-width:65%"></img>
</center>

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for graph above")
```

The mean values for $SO_2$ and $NO_2$ in the pre-automation year were 14.4 ppb and 20.4 ppb, respectively. The size of both treatment effects above is noteworthy, but only the treatment effect for $NO_2$ is significant, with a relatively small standard error. However, both effects are smaller than the effect for $PM_{10}$, which is consistent with our assumption.

To investigate whether the effects are valid for the entire sample or only for specific groups, we could use the same models as in *Exercise 3*, which would involve raw and residual data, comparing results for *Wave 1* and *Wave 2* cities, and using data from the subsample of 76 cities. We will keep it short at this point, the effects are comparable for almost all subgroups, detailed results can be found in the appendix of Greenstone et al. (2022).

Although not highly relevant for this assumption, it would be interesting to know how exactly $SO_2$, $NO_2$, and $PM_{10}$ were embedded in the evaluation process of local governments during the time when the technology was implemented. Greenstone et al. (2022, 64) indicate that these pollutants are less important than $PM_{10}$, but they do not provide any further references. Nevertheless, the results above indicate that $PM_{10}$ was more relevant due to a larger and more significant estimated treatment effect.

**5. Assumption: Continuity in AOD**

As we learned in *Exercise 1.1*, we can measure air pollution not only on the ground using monitoring stations, but we can also use satellite data, e.g., *AOD*, as a proxy for $PM_{10}$. Fortunately, we have a data set that includes *AOD* data which were manually mapped to each ground station in the `station_day` data set. As mentioned before, the procedure of measuring *AOD* is not as precise since it covers an area (in this case 10 x 10 km) and its accuracy can be affected by weather conditions like clouds. Furthermore, it has to be converted to be used as a proxy for ground level $PM_{10}$. Since daily *AOD* data might be too noisy (due to missing values), we will now use monthly residual $PM_{10}$ and *AOD* in the following regression models.

<center>
<img src="aod_graph_rd.png" style="max-width:70%"></img>
</center>

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for graph above")
```


Quiz: We know that we should not solely rely on the graphical results of an RD design to make a statement about the existence of a treatment effect. However, it is good enough for preliminary indication. The residual $PM_{10}$ and *AOD* graphs suggest that...

[1]: ...both PM10 and AOD data were potentially manipulated.
[2]: ...only PM10 data was potentially manipulated.
[3]: ...only AOD data was potentially manipulated.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("PM10 vs AOD graph")
```

Next, we will examine the RD estimates of *AOD* using the `rdrobust()` function. We will follow the same procedure as in *Exercise 3*.

<center>
<img src="aod_rd.png" style="max-width:65%"></img>
</center>


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for graph above")
```


Quiz: Do these results verify the *AOD* RD plots?

[1]: Yes, the AOD treatment effect is rather small, especially when including covariates and fixed effects.
[2]: No, the AOD treatment effect is significant for both models.
[3]: No, the AOD treatment effect is bigger than expected.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD AOD")
```



Quiz: What is the reason for using monthly data instead of daily data in this case?

[1]: AOD is only measured on a monthly basis due to technical limitations.
[2]: AOD is not available for all days of the month (e.g., due to clouds) and is therefore too noisy on a daily level.
[3]: It makes more sense to collect monthly data since daily measurements would be too expensive.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD AOD daily vs monthly data")
```

In both models, we obtain non-significant RD estimates that are relatively small, especially when including covariates and fixed effects. The results therefore suggest that the automation had, as assumed, no effect on published *AOD* data. Assuming that *AOD* is a valid proxy for $PM_{10}$, this result can be used as evidence supporting the main hypothesis since the *AOD* data used in this case is not relevant for and not manipulable by Chinese (city) governments. Thus far, we can conclude that the assumption seems to hold. However, we need to confirm that *AOD* is indeed a valid proxy for $PM_{10}$.


Quiz: How can we check if *AOD* is a reasonable proxy for $PM_{10}$?

[1]: Calculate the partial correlation for these variables.
[2]: Compare the RD coefficients of both models and calculate the difference.
[3]: Apply a statistical method that was not introduced yet.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("AOD proxy PM10")
```

**Task:** Check the following code chunk to see how we can implement the solution. 

```{r "11_11",eval=FALSE}
#load packages
library(ppcor)
library(scales)
library(fastDummies)

#load data
station_month <- readRDS("station_month.RDS")

#prepare data
station_month_1 <- station_month %>%
  filter(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & !is.na(temp) 
         & !is.na(rh) & !is.na(aod))%>%
  filter(after_auto_dummy == 0)%>%
  select(aod,pm10,wind_speed, rain, temp, rh, month_date, station_n, 
         after_auto_dummy)%>%
  dummy_cols(select_columns = c("month_date", "station_n"))%>%
  select(-c(station_n, after_auto_dummy, month_date))

#calculate correlation coefficient
A1 <- cor.test(station_month_1$aod, station_month_1$pm10)


#prepare data
station_month_2 <- station_month %>%
  filter(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & !is.na(temp) & 
           !is.na(rh) & !is.na(aod))%>%
  filter(after_auto_dummy == 1)%>%
  select(aod,pm10,wind_speed, rain, temp, rh, month_date, 
         station_n, after_auto_dummy)%>%
  dummy_cols(select_columns = c("month_date", "station_n"))%>%
  select(-c(station_n, after_auto_dummy, month_date))

#calculate correlation coefficient
B1 <- cor.test(station_month_2$aod, station_month_2$pm10)
 
#create data frame
options(scipen=999)
corr_aod <- data.frame(
  row_name = c("Reported PM10", "Num.Obs.","Reported PM10", "Observations",
               "Increase in explanatory power", "Weather controls",
               "Year-month fixed effects", "Station fixed effects"), 
  col1 = c(format(round(A1$estimate,3),nsmall=3), 
           format(A1$parameter, big.mark=","),
           format(round(B1$estimate,3), nsmall =3), 
           format(B1$parameter, big.mark=","), 
           percent(B1$estimate/A1$estimate-1, accuracy = 0.1), "","",""))



kable(corr_aod, "html", col.names = c(" ", "(1)"), align = "lc",  
      caption = "(Partial) Correlation AOD and PM10")%>%
    add_header_above(c(" " = 1, "AOD" = 1))%>%
    pack_rows("Pre-Automation", 1, 2) %>%
    pack_rows("Post-Automation", 3, 4) %>%
    pack_rows("", 5, 8)%>%
    kable_classic(full_width = F, position = "center")%>%
    kable_styling(font_size = 15)


#Greenstone et al. (2022): Table B10 (Appendix), column 1

```



Quiz: How did the automation affect the correlation between $PM_{10}$ and *AOD*?

[1]: The correlation decreased after the automation.
[2]: The correlation increased after the automation.
[3]: The correlation was not affected.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Correlation AOD")
```


Quiz: Did we expect the correlation to increase?

[1]: Yes, since post-automation PM10 is supposed to be less manipulable (and therefore closer aligned to AOD which was not affected by the automation).
[2]: No, since pre-automation PM10 should be less manipulable (and therefore further away from AOD which was more affected by the automation).

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Change in correlation")
```

The correlation increased by 61.7% after the automation, as expected given that the available *AOD* data was not manipulated. However, the absolute correlation is not particularly high, raising questions about whether *AOD* is a suitable proxy for $PM_{10}$ in this case. To investigate further, we can control for weather and include fixed effects.

**Task:** Click on `check`.

```{r "11_12"}

#calculate partial correlation
A2 <- pcor.test(station_month_1$aod, station_month_1$pm10, 
                list(station_month_1$temp, station_month_1$rh, 
                     station_month_1$rain, station_month_1$wind_speed))

A3 <- pcor.test(station_month_1$aod, station_month_1$pm10, 
                station_month_1[,c(3:40)])

A4 <- pcor.test(station_month_1$aod, station_month_1$pm10, 
                station_month_1[,-c(1:2)])

B2 <- pcor.test(station_month_2$aod, station_month_2$pm10, 
                list(station_month_2$temp, station_month_2$rh, 
                     station_month_2$rain, station_month_2$wind_speed))

B3 <- pcor.test(station_month_2$aod, station_month_2$pm10, 
                station_month_2[,c(3:64)])

B4 <- pcor.test(station_month_2$aod, station_month_2$pm10, 
                station_month_2[,-c(1:2)])


#create data frame
options(scipen=999)
corr_aod  <- data.frame(
  row_name = c("Reported PM10", "Num.Obs.","Reported PM10", "Observations",
               "Increase in explanatory power", "Weather controls",
               "Year-month fixed effects", "Station fixed effects"), 
  col1 = c(format(round(A1$estimate,3),nsmall=3), 
           format(A1$parameter, big.mark=","), 
           format(round(B1$estimate,3), nsmall =3), 
           format(B1$parameter, big.mark=","), 
           percent(B1$estimate/A1$estimate-1,
                   accuracy = 0.1),"","",""),
  col2 = c(format(round(A2$estimate,3),snmall=3), 
           format(A2$n, big.mark=","),
           format(round(B2$estimate,3),nsmall=3), 
           format(B2$n, big.mark=","),
           percent(B2$estimate/A2$estimate-1, 
                   accuracy = 0.1),"X","",""),
  col3 = c(format(round(A3$estimate,3),nsmall=3), 
           format(A3$n, big.mark=","),
           format(round(B3$estimate,3),nsmall=3), 
           format(B3$n, big.mark=","), 
           percent(B3$estimate/A3$estimate-1,
                   accuracy = 0.1), "X","X",""), 
  col4 = c(format(round(A4$estimate,3),nsmall=3), 
           format(A4$n, big.mark=","), 
           format(round(B4$estimate,3),nsmall=3), 
           format(B4$n, big.mark=","), 
           percent(B4$estimate/A4$estimate-1, 
                   accuracy = 0.1), "X","X","X"))



kable(corr_aod, "html", col.names = c(" ", "(1)","(2)","(3)","(4)"), 
      align = "lcccc", 
      caption = "(Partial) Correlation AOD and PM10")%>%
    add_header_above(c(" " = 1, "AOD" = 4))%>%
    pack_rows("Pre-Automation", 1, 2) %>%
    pack_rows("Post-Automation", 3, 4) %>%
    pack_rows("", 5, 8)%>%
    kable_classic(full_width = F, position = "center")%>%
    kable_styling(font_size = 15)

#Greenstone et al. (2022): Table B10 (Appendix), column 1, 2, 3, and 4

```

As we can see, adding weather controls and year-month fixed effects not only increases the absolute (partial) correlation but also improves the explanatory power (by +84.6% when using weather controls and by +73.1% when using weather controls and year-month fixed effects). Again, this suggests that the use of *AOD* as a proxy supports the hypothesis of pre-automation manipulation. However, incorporating station-specific fixed effects leads to a decrease in the absolute pre- and post-automation correlations (to $0.120$ and $0.121$, respectively) and only a +1.2% improvement in explanatory power. This shift from cross-sectional to panel variation weakens the correlation between *AOD* and $PM_{10}$, as noted by Greenstone et al. (2022) and Fowlie et al. (2019).

Although this test only provides minor support for the validity of the RD designs, it does not invalidate the previous findings. In summary, using *AOD* as a proxy seems to be a reasonable approach which however requires further research since the inclusion of station-specific fixed effects in this particular case raises doubts about its validity. Despite the last model, Greenstone et al. (2022) still consider the results of this analysis to confirm the validity of the RD design. Please note that the number of observations used in this calculation is the same as in Greenstone et al.’s (2022) Appendix B10, indicating that they are likely based on the same data. The deviation between the partial correlation we calculated here and the one in Appendix B10 is minor, and it can be attributed to differences in calculation methods between the R functions `pcor()` (`pcor.test()`) and the *Stata* function `pcorr`.


**6. Assumption: No placebo treatment effects**

The cities in our data set automated their air pollution monitoring and reporting on several dates, which we referred to as *automation dates* and used as the cutoff value in the previous RD designs. Therefore, there should be only one **true** automation date per city. We assume that the discontinuity in treatment assignment occurs only at the actual automation dates and not at any other arbitrary placebo automation dates. To further analyze our data, we will employ multiple placebo cutoffs and apply the established model which uses residual $PM_{10}$ as the dependent variable.


Quiz: Do we expect to find similar (significant) treatment effects for the placebo cutoff values?

[1]: yes
[2]: no

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Placebo automation dates")
```

**Task:** Click on check. The plot is generated from the code in the info box below, which runs the `rdrobust()` function for each date in our data input. Running this code can be time-consuming, so the results have already been calculated and saved in the file `rd_placebo_730.rds`.

```{r "11_13",fig.width=10, fig.height=5}

#load data
rd_placebo <- readRDS("rd_placebo_730.rds")

#create and display graph
ggplot(data= rd_placebo)+
  geom_linerange(aes(x = index, ymin =ci_lower, ymax =ci_upper), 
                 col = "lightgrey")+
  geom_point(aes(y=coef, x= index, col = pv < 0.05), size = 0.5)+
  scale_color_manual(labels = c("above or equal 5%", "below 5%"), 
                     values = c("black", "red"))+
  geom_vline(aes(xintercept =0), col = "red")+
  geom_vline(aes(xintercept =-439), col = "black", linetype = 2)+
  geom_vline(aes(xintercept =666), col = "black", linetype = 2)+
  scale_x_continuous(breaks = c(-730,-365,0, 365, 730,1095))+
  labs(title="Summary of 1,461 Individual Placebo RD Designs",
       col = "p-value:")+
  xlab("Days before/after actual automation")+
  ylab(expression("Estimated Coefficient Residual PM"[10]))+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)+
  #ylim(-25,50)+
  theme(legend.position = "bottom")

```

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Code for placebo RD design above")
```

The graph displays the estimated treatment effects of 1,461 individually estimated RD models, spanning 730 days before and after the true automation date, based on the same specifications as in *Exercise 3.3*. The entire sample was used for all models. The red line represents the actual automation date, which has been standardized and set to 0 as it varies between cities. Additionally, the black lines represent the estimates at the medians of all observations in the pre-automation and post-automation periods, as recommended by Imbens and Lemieux (2008, 632). The estimates are color-coded, with green indicating significance at the 5% level.

Overall, we find the highest RD estimate close to the true automation date, which can be considered evidence for the validity of our RD design. However, there is also some noise, as several other significant results are found within the $PM_{10}$ range of $-25$ to $+25$. The estimated placebo treatment effects at the two medians (-439 and 666) of the pre- and post-automation periods are unremarkable, at $-1.820$ and $-8.413$, respectively. When we perform this kind of placebo analysis on subgroups (not included here), we always observe the largest estimated treatment effect around the actual automation date. Although the placebo treatment effects are higher than expected, we can easily identify the actual treatment effect and therefore provide support for the validity of the RD design.


Quiz: What is the main finding of this placebo check?

[1]: The highest RD estimate is found within a range of -25 to +25.
[2]: The estimated placebo treatment effects at the two medians are significant.
[3]: The actual treatment effect can be easily identified using the placebo RD designs.
[4]: There is too much noise in the data to draw any conclusions.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Placebo findings")
```


Summarizing this exercise, none of the six tests we performed give us a reason to dismiss the overall findings from our RD design. On the contrary, the continuity tests on covariates and other pollutants as well as the placebo and the sensitivity test increase confidence in the validity of our RD design. Using *AOD* as a proxy for $PM_{10}$ delivered only support for the validity as long as we excluded entity specific fixed effects from the model. We also discovered a potential bunching effect for the *Wave 2* subsample that reduces confidence in the respective estimates, yet the overall results are still likely to be valid.



## Exercise 6 -- Discussion and Conlusion


Before we conclude this RTutor problem set with a summary of the results, we will briefly discuss three topics referring to the original paper.

**Discussion**

First, the title “Can Technology Solve the Principal-Agent Problem?” includes two major elements: *Technology* and *Principal-Agent-Problem*. Greenstone et al. (2022) use the term *Technology* only in the rather specific and narrow sense of “information and monitoring technology” (Greenstone et al 2022, 56) and they demonstrate in their paper how it can help alleviate a government-related principal-agent problem. However, they do not discuss the potential drawbacks of using technology in this context. They only mention that it may have “other important implications” (Greenstone et al. 2022, 69). While the paper focuses on the implications of monitoring technology in the given scenario, it is important to at least acknowledge at this point the potential risks associated with excessive use of such technology, which can be interpreted as a form of surveillance. See Lyon (2005) or Ball et al. (2012) for an overview and discussion of technology in the context of surveillance. Furthermore, Greenstone et al. (2022) found supporting evidence for data manipulation in several cities before the implementation of the new technology. Hence this raises another (research) question, which goes beyond the scope of this RTutor problem set: why did the pre-automation incentive and control system for local governments fail to achieve the desired data quality for air pollution data? It is crucial to note that technology is just one of several possible solutions to address hidden actions and mitigate the principal-agent problem and it would therefore be interesting to study why the principal-agent problem occurred in the first place.

Second, Greenstone et al. (2022) do not discuss any potential effects of the **announcement date** of the reporting and monitoring automation on published air pollution data. Although the *War on Pollution* was declared in March 2014 (Greenstone et al. 2021), it is unclear when the Chinese central government notified city governments about the impending automation of air pollution monitoring and reporting. While our data set indicates that the first cities automated around March 2012, the exact date of notification remains unknown. If we had this information, we could have conducted an additional RD design using the announcement date as a cutoff to determine whether cities, which had potentially manipulated their data, aligned their falsified data with the actual data prior to the implementation of the technology. However, if those cities had gradually increased their underreported pollution levels to the actual pollution levels between the announcement and implementation dates, we would not have observed a treatment effect at either the announcement or automation date. In both cases, Greenstone et al.’s (2022) estimated treatment effect for the automation date would have been systematically too small.

Finally, there is one more aspect to discuss which is the level of detail in Greenstone et al.’s (2022) original paper and its appendix. It is not the purpose of scientific papers to explain every topic and every result in detail. Hence this paragraph is not intended to criticize the authors but to describe examples where some readers may benefit from additional information. For instance, when discussing the functions from the `rdrobust` package, Greenstone et al. (2022) give technical insights into how they prepared their data sets in their published *Stata* code but do not explain why they used specific functions or arguments. Moreover, although they created additional models using different kernels in the appendix, they did not do the same for bandwidth, which is supposedly the most important argument in an RD design. Regarding bandwidth, Greenstone et al. (2022) used the integrated method by Calonico et al. (2014b) for optimal bandwidth selection. At this point, it would have been interesting to learn more about this method and its suitability in the given scenario. Another point to consider is the evaluation and interpretation of results. Again, it would have fallen outside the scope of the paper to discuss and explain all results in detail, but additional context might be helpful in some cases. For example, in the DiD event study, they found considerably varying estimates (especially for the first post-treatment period) due to different fixed effects in the models yet without explaining the origin of the difference.

**Summary & Conclusion**


Our objective was to analyze whether the introduction of new air pollution monitoring and reporting technology had an effect on published pollution data in China, addressing the hypothesis that there was a systematic underreporting of air pollution on a local government level prior to the automation. We consider this a principal-agent problem as potential underreporting or data manipulation can be regarded as hidden actions. In the first exercises of this RTutor problem set, we familiarized ourselves with air pollution, China’s government structure, and its *War on Pollution*. We then carried out the data preparation process and proceeded with our analysis by introducing RD designs and explaining a small-scale example before analyzing the main hypothesis with the complete data set. We found a substantial and significant treatment effect on the day of the implementation of new monitoring and reporting technology, providing support for the central hypothesis of widespread data manipulation within Chinese local governments. These results replicate those of Greenstone et al. (2022). Furthermore, we applied the established models of the RD design on various subgroups and conducted a DiD event-study. To test the validity of the RD design, we also tested the assumptions (e.g., on covariates) and did not discover any significant issues which would dismiss the results of the RD design. As mentioned, several of the validity tests we conducted were also performed by Greenstone et al. (2022). In addition, we performed 1,460 placebo regressions using all dates within two years before and after the actual automation date as placebo cutoffs. The largest treatment effect was found in proximity to the actual automation date. While conducting placebo tests is suggested by researchers as mentioned in this RTutor problem set, this specific approach was rather unconventional and was likely not suggested in any research paper. Nevertheless, it worked in this instance, as we were able to identify the actual treatment effect and therefore supported the validity of the RD design. Overall, the scenario analyzed by Greenstone et al. (2022) and within this RTutor problem set presents an example for technology as a possible solution to principal-agent problems in government organizations. In this specific case, the technology enabled better data quality for air pollution data and therefore likely contributed to more effective air pollution reduction policies. As presented in Appendix A1, the improvement in data quality also facilitates better individual protective behavior and therefore potentially reduces welfare losses from air pollution. The relationship between technology and welfare losses in this context represents another interesting area of research.

Congratulations on completing this RTutor problem set! Take a moment to acknowledge all your achievements:

```{r "12_1",optional=TRUE}
awards()
```




## Exercise 7 -- References


### Original paper 

`Paper`: https://www.aeaweb.org/articles?id=10.1257/aeri.20200373

`Working paper`: https://www.nber.org/papers/w27502 (published in 2020)

`Online-Appendix`: https://assets.aeaweb.org/asset-server/files/16219.pdf

`Data`: https://www.openicpsr.org/openicpsr/project/125321/version/V1/view


### Bibliograph

Reference List

Abadie, A., Athey, S., Imbens, G.W. & Wooldridge, J.M. (2022) When Should You Adjust Standard Errors for Clustering? The Quarterly Journal of Economics, 138(1), 1–35. Available from: https://doi.org/10.1093/qje/qjac038.

Athey, S. & Imbens, G.W. (2022) Design-based analysis in Difference-In-Differences settings with staggered adoption. Journal of Econometrics, 226(1), 62–79. Available from: https://doi.org/10.1016/j.jeconom.2020.10.012.

Ball, K., Haggerty, K. & Lyon, D. (2012) Routledge Handbook of Surveillance Studies. Routledge.

Barwick, P.J., Li, S., Liguo, L. & Zou, E. (2019) From Fog to Smog: The Value of Pollution Information. SSRN Electronic Journal. Available from: https://doi.org/10.2139/ssrn.3441954.

Calonico, S., Cattaneo, M.D., Farrell, M.H. & Titiunik, R. (2019) Regression Discontinuity Designs Using Covariates. The Review of Economics and Statistics, 101(3), 442–451. Available from: https://doi.org/10.1162/rest_a_00760.

Calonico, S., Cattaneo, M.D., Farrell, M.H. & Titiunik, R. (2022) Package ‘rdrobust’: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs. Available from: https://cran.r-project.org/web/packages/rdrobust/rdrobust.pdf [Accessed 18 March 2023].

Calonico, S., Cattaneo, M.D. & Titiunik, R. (2014a) Robust Data-Driven Inference in the Regression-Discontinuity Design. The Stata Journal: Promoting communications on statistics and Stata, 14(4), 909–946. Available from: https://doi.org/10.1177/1536867X1401400413.

Calonico, S., Cattaneo, M.D. & Titiunik, R. (2014b) Robust Nonparametric Confidence Intervals for Regression-Discontinuity Designs. Econometrica, 82(6), 2295–2326. Available from: https://doi.org/10.3982/ECTA11757.

Calonico, S., Cattaneo, M.D. & Titiunik, R. (2015) rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs. The R Journal, 7(1), 38. Available from: https://doi.org/10.32614/RJ-2015-004.

Cattaneo, M.D. & Vazquez-Bare, G. (2017) The Choice of Neighborhood in Regression Discontinuity Designs. Observational Studies, 3(2), 134–146. Available from: https://doi.org/10.1353/obs.2017.0002.

Centers of Disease Control and Prevention [CDC] (2022) Particle Pollution. Available from: https://www.cdc.gov/air/particulate_matter.html [Accessed 16 November 2022].

Chen, B.H., Hong, C.J., Pandey, M.R. & Smith, K.R. (1990) Indoor air pollution in developing countries. World health statistics quarterly, (43), 127–138.

Cheung, K., Daher, N., Kam, W., Shafer, M.M., Ning, Z. & Schauer, J.J. et al. (2011) Spatial and temporal variation of chemical composition and mass closure of ambient coarse particulate matter (PM10–2.5) in the Los Angeles area. Atmospheric Environment, 45(16), 2651–2662. Available from: https://doi.org/10.1016/j.atmosenv.2011.02.066.

Dawson, J.P., Adams, P.J. & Pandis, S.N. (2007) Sensitivity of PM2.5 to climate in the Eastern US: a modeling case study. Atmospheric Chemistry and Physics, 7(16), 4295–4309. Available from: https://doi.org/10.5194/acp-7-4295-2007.

Dunrui, L. (2021) The Geographical Characteristics of China’s Opening-up in the Past 40 Years and the New Realm in the Future. In: Loitsch, T. (Ed.) China im Blickpunkt des 21. Jahrhunderts. Springer Gabler: Berlin, Heidelberg, 115–125.

Fowlie, M., Rubin, E. & Walker, R. (2019) Bringing Satellite-Based Air Quality Estimates Down to Earth. AEA Papers and Proceedings, 109, 283–288. Available from: https://doi.org/10.1257/pandp.20191064.

Fuller, R., Landrigan, P.J., Balakrishnan, K., Bathan, G., Bose-O'Reilly, S. & Brauer, M. et al. (2022) Pollution and health: a progress update. The Lancet Planetary Health, 6(6), e535-e547. Available from: https://doi.org/10.1016/S2542-5196(22)00090-0.

Gailmard, S. (2014) Accountability and Principal–Agent Theory. In: Bovens, M., Goodin, R.E. & Schillemans, T. (Eds.) The Oxford Handbook of Public Accountability. Oxford University Press, 90–105.

Gajewicz-Skretna, A., Kar, S., Piotrowska, M. & Leszczynski, J. (2021) The kernel-weighted local polynomial regression (KwLPR) approach: an efficient, novel tool for development of QSAR/QSAAR toxicity extrapolation models. Journal of Cheminformatics, 13(1). Available from: https://doi.org/10.1186/s13321-021-00484-5.

Gao, N., Long, C.X. & Xu, L.C. (2016) Collective Leadership, Career Concern, and the Housing Market in China: The Role of Standing Committees. Review of Development Economics, 20(1), 1–13. Available from: https://doi.org/10.1111/rode.12202.

Geng, G., Zheng, Y., Zhang, Q., Xue, T., Zhao, H. & Tong, D. et al. (2021) Drivers of PM2.5 air pollution deaths in China 2002–2017. Nature Geoscience, 14(9), 645–650. Available from: https://doi.org/10.1038/s41561-021-00792-3.

Greenstone, M., He, G., Jia, R. & Liu, T. (2022) Can Technology Solve the Principal-Agent Problem? Evidence from China’s War on Air Pollution. American Economic Review: Insights, 4(1), 54–70. Available from: https://doi.org/10.1257/aeri.20200373.

Greenstone, M., He, G., Li, S. & Zou, E.Y. (2021) China’s War on Pollution: Evidence from the First 5 Years. Review of Environmental Economics and Policy, 15(2), 281–299. Available from: https://doi.org/10.1086/715550.

Hahn, J., Todd, P. & Klaauw, W. (2001) Identification and Estimation of Treatment Effects with a Regression-Discontinuity Design. Econometrica, 69(1), 201–209. Available from: https://doi.org/10.1111/1468-0262.00183.

Hausman, C. & Rapson, D.S. (2018) Regression Discontinuity in Time: Considerations for Empirical Applications. Annual Review of Resource Economics, 10(1), 533–552. Available from: https://doi.org/10.1146/annurev-resource-121517-033306.

Hedworth, H.A., Sayahi, T., Kelly, K.E. & Saad, T. (2021) The effectiveness of drones in measuring particulate matter. Journal of Aerosol Science, 152, 105702. Available from: https://doi.org/10.1016/j.jaerosci.2020.105702.

Heilmann, S. (Ed.) (2016) Das politische System der Volksrepublik China, 3rd edition. Springer VS: Wiesbaden.

Hernandez, G., Berry, T.-A., Wallis, S. & Poyner, D. (2017) Temperature and humidity effects on particulate matter concentrations in a sub-tropical climate during winter. International Proceedings of Chemical, Biological and Environmental Engineering, 102, 41–49. Available from: https://doi.org/10.7763/IPCBEE.

Imbens, G. & Kalyanaraman, K. (2012) Optimal Bandwidth Choice for the Regression Discontinuity Estimator. The Review of Economic Studies, 79(3), 933–959. Available from: https://doi.org/10.1093/restud/rdr043.

Imbens, G.W. & Lemieux, T. (2008) Regression discontinuity designs: A guide to practice. Journal of Econometrics, 142(2), 615–635. Available from: https://doi.org/10.1016/j.jeconom.2007.05.001.

Jensen, M.C. & Meckling, W.H. (1976) Theory of the firm: Managerial behavior, agency costs and ownership structure. Journal of Financial Economics, 3(4), 305–360. Available from: https://doi.org/10.1016/0304-405X(76)90026-X.

Kennedy, P. (2008) A guide to econometrics, 6th edition. Blackwell Publishing: Malden,MA, Oxford, Carlton,Victoria.

Kim, K.-H., Kabir, E. & Kabir, S. (2015) A review on the human health impact of airborne particulate matter. Environment International, 74, 136–143. Available from: https://doi.org/10.1016/j.envint.2014.10.005.

Lee, D.S. (2008) Randomized experiments from non-random selection in U.S. House elections. Journal of Econometrics, 142(2), 675–697. Available from: https://doi.org/10.1016/j.jeconom.2007.05.004.

Lee, D.S. & Lemieux, T. (2010) Regression Discontinuity Designs in Economics. Journal of Economic Literature, 48(2), 281–355. Available from: https://doi.org/10.1257/jel.48.2.281.

Li, J., Li, H., Wang, X., Wang, W., Ge, M. & Zhang, H. et al. (2021) A large-scale outdoor atmospheric simulation smog chamber for studying atmospheric photochemical processes: Characterization and preliminary application. Journal of Environmental Sciences (China), 102, 185–197. Available from: https://doi.org/10.1016/j.jes.2020.09.015.

Lyon, D. (2005) Surveillance society: Monitoring everyday life. Open University Press: Buckingham, Philadelphia, Pennsylvania.

Ma, J., Xu, X., Zhao, C. & Yan, P. (2012) A review of atmospheric chemistry research in China: Photochemical smog, haze pollution, and gas-aerosol interactions. Advances in Atmospheric Sciences, 29(5), 1006–1026. Available from: https://doi.org/10.1007/s00376-012-1188-7.

Marcus, M. & Sant’Anna, P.H.C. (2021) The Role of Parallel Trends in Event Study Settings: An Application to Environmental Economics. Journal of the Association of Environmental and Resource Economists, 8(2), 235–275. Available from: https://doi.org/10.1086/711509.

Maier, S. (2022) China’s Political Promotion Incentives to Control River Border Pollution. Available from: https://simonmaier.shinyapps.io/Chinas-Political-Promotion-Incentives/ [Accessed 11 April 2023].

Matus, K., Nam, K.-M., Selin, N.E., Lamsal, L.N., Reilly, J.M. & Paltsev, S. (2012) Health damages from air pollution in China. Global Environmental Change, 22(1), 55–66. Available from: https://doi.org/10.1016/j.gloenvcha.2011.08.006.

Murray, C.J.L., Aravkin, A.Y., Zheng, P., Abbafati, C., Abbas, K.M. & Abbasi-Kangevari, M. et al. (2020) Global burden of 87 risk factors in 204 countries and territories, 1990–2019: a systematic analysis for the Global Burden of Disease Study 2019. The Lancet, 396(10258), 1223–1249. Available from: https://doi.org/10.1016/S0140-6736(20)30752-2.

Oksenberg, M. (2001) China's Political System: Challenges of the Twenty-First Century. The China Journal, 45, 21–35. Available from: https://doi.org/10.2307/3182364.

Panik, M.J. (2012) Statistical inference: A short course. Wiley: Hoboken, N.J.

Pellegrini, G., Terribile, F., Tarola, O., Muccigrosso, T. & Busillo, F. (2013) Measuring the effects of European Regional Policy on economic growth: A regression discontinuity approach. Papers in Regional Science, 92(1), 217–233. Available from: https://doi.org/10.1111/j.1435-5957.2012.00459.x.

Perraillon, M.C. (2020) Regression discontinuity designs. Week 10. Health Services Research Methods I. Available from: https://clas.ucdenver.edu/marcelo-perraillon/sites/default/files/attached-files/week_10_rdd_perraillon.pdf [Accessed 10 March 2023].

Peter, B. (2017) Water Pollution and Cancer in China. Available from: https://brigittepeter.shinyapps.io/RTutorWaterPollutionChina/ [Accessed 11 April 2023].

Russell, A.G. & Brunekreef, B. (2009) A focus on particulate matter and health. Environmental Science & Technology, 43(13), 4620–4625. Available from: https://doi.org/10.1021/es9005459.

Sekhon, J.S. & Titiunik, R. (2017) On Interpreting the Regression Discontinuity Design as a Local Experiment. In: Cattaneo, M.D. & Escanciano, J.C. (Eds.) Regression Discontinuity Designs. Emerald Publishing Limited, 1–28.

Srimuruganandam, B. & Shiva Nagendra, S.M. (2012) Source characterization of PM10 and PM2.5 mass using a chemical mass balance model at urban roadside. The Science of the Total Environment, 433, 8–19. Available from: https://doi.org/10.1016/j.scitotenv.2012.05.082.

Sun, C., Kahn, M.E. & Zheng, S. (2017) Self-protection investment exacerbates air pollution exposure inequality in urban China. Ecological Economics, 131, 468–474. Available from: https://doi.org/10.1016/j.ecolecon.2016.06.030.

Tai, A.P., Mickley, L.J. & Jacob, D.J. (2010) Correlations between fine particulate matter (PM2.5) and meteorological variables in the United States: Implications for the sensitivity of PM2.5 to climate change. Atmospheric Environment, 44(32), 3976–3984. Available from: https://doi.org/10.1016/j.atmosenv.2010.06.060.

Thistlethwaite, D.L. & Campbell, D.T. (1960) Regression-discontinuity analysis: An alternative to the ex post facto experiment. Journal of Educational Psychology, 51(6), 309–317. Available from: https://doi.org/10.1037/H0044319.

U.S. Environmental Protection Agency [US EPA] (2016) Particulate Matter (PM) Basics. Available from: https://www.epa.gov/pm-pollution/particulate-matter-pm-basics [Accessed 10 November 2022].

United Nations Statistics Division (2022) Pollution. Available from: http://data.un.org/Glossary.aspx?q=pollution [Accessed 9 November 2022].

van Donkelaar, A., Martin, R.V., Brauer, M., Hsu, N.C., Kahn, R.A. & Levy, R.C. et al. (2016) Global Estimates of Fine Particulate Matter using a Combined Geophysical-Statistical Method with Information from Satellites, Models, and Monitors. Environmental Science & Technology, 50(7), 3762–3772. Available from: https://doi.org/10.1021/acs.est.5b05833.

Verbeek, M. (2017) A guide to modern econometrics, 5th edition. Wiley: Hoboken, NJ.

Węglarczyk, S. (2018) Kernel density estimation and its application. ITM Web of Conferences, 23, 37. Available from: https://doi.org/10.1051/itmconf/20182300037.

Wei, X., Chang, N.-B., Bai, K. & Gao, W. (2020) Satellite remote sensing of aerosol optical depth: advances, challenges, and perspectives. Critical Reviews in Environmental Science and Technology, 50(16), 1640–1725. Available from: https://doi.org/10.1080/10643389.2019.1665944.

Wickham, H. (2016) ggplot2: Elegant graphics for data analysis, 2nd edition. Springer: Cham.

Wooldridge, J.M. (2010) Econometric analysis of cross section and panel data, 2nd edition. MIT Press: Cambridge, Mass.

Wooldridge, J.M. (2013) Introductory econometrics: A modern approach, 5th edition. South-Western Cengage Learning: Mason, Ohio.

World Bank (2022a) The Global Health Cost of PM2.5 Air Pollution: A Case for Action Beyond 2021, Washington, DC. International Development in Focus. Available from: https://doi.org/10.1596/978-1-4648-1816-5.

World Bank (2022b) World Development Indicators database: GDP (current US$); (NY.GDP.MKTP.CD). Primary data source: World Bank national accounts data, and OECD National Accounts data files. Available from: https://data.worldbank.org/indicator/NY.GDP.MKTP.CD [Accessed 16 March 2023].

World Bank (2022c) World Development Indicators database: Population, total; (SP.POP.TOTL). Primary data from (1) United Nations Population Division. World Population Prospects: 2022 Revision. (2) Census reports and other statistical publications from national statistical offices, (3) Eurostat: Demographic Statistics, (4) United Nations Statistical Division. Population and Vital Statistics Report (various years), (5) U.S. Census Bureau: International Database, and (6) Secretariat of the Pacific Community: Statistics and Demography Programme. Available from: https://data.worldbank.org/indicator/SP.POP.TOTL [Accessed 16 March 2023].

World Health Organization [WHO] & WHO European Centre for Environment and Health (2021) WHO global air quality guidelines: Particulate matter (PM2.5 and PM10), ozone, nitrogen dioxide, sulfur dioxide and carbon monoxide. World Health Organization: Geneva.


### R

R Core Team (2022). _R: A language and environment for statistical computing_. R version 4.2.2. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.

<br/>

### R Packages

**broom**

Robinson, D., Hayes, A. & Couch, S. (2023). _broom: Convert Statistical Objects into Tidy Tibbles_. R package version 1.0.3. <https://CRAN.R-project.org/package=broom>.

<br/>

**cowplot**

Wilke, C. (2020). _cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'_. R package version 1.1.1. <https://CRAN.R-project.org/package=cowplot>.
  
<br/>

**data.table**

Dowle, M. & Srinivasan, A. (2022). _data.table: Extension of "data.frame"_. R package version 1.14.6. <https://CRAN.R-project.org/package=data.table>.

<br/>

**dplyr** 

Wickham, H., François, R., Henry, L., Müller, K. & Vaughan, D. (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.0. <https://CRAN.R-project.org/package=dplyr>.

<br/>

**extrafont**

Chang, W. (2023). _extrafont: Tools for Using Fonts_. R package version 0.19. <https://CRAN.R-project.org/package=extrafont>.

<br/>

**fastDummies**

Kaplan, J. (2020). _fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables_. R package version 1.6.3. <https://CRAN.R-project.org/package=fastDummies>.

<br/>

**geosphere**

Hijmans, R. (2022). _geosphere: Spherical Trigonometry_. R package version 1.5-18. <https://CRAN.R-project.org/package=geosphere>.

<br/>

**ggplot2**

Wickham, H .(2016). _ggplot2: Elegant Graphics for Data Analysis_. R package version 3.4.0. Springer-Verlag New York. <https://CRAN.R-project.org/package=ggplot2>.

<br/>

**haven**

Wickham, H., Miller, E. & Smith, D. (2022). _haven: Import and Export 'SPSS', 'Stata' and 'SAS' Files_. R package version 2.5.1. <https://CRAN.R-project.org/package=haven>.

<br/>

**hrbrthemes**

Rudis, B. (2020). _hrbrthemes: Additional Themes, Theme Components and Utilities for 'ggplot2'_. R package version 0.8.0. <https://CRAN.R-project.org/package=hrbrthemes>.

<br/>

**kableExtra**

Zhu, H. (2021). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.3.4. <https://CRAN.R-project.org/package=kableExtra>.

<br/>

**lfe**

Gaure, S. (2013). _lfe: Linear group fixed effects_. R package version 2.8.8. The R Journal, 5(2):104-117. <https://CRAN.R-project.org/package=lfe>.

<br/>

**lubridate**

Grolemund, G. & Wickham, H. (2011). _Dates and Times Made Easy with lubridate_. R package version 1.9.1. Journal of Statistical Software, 40(3), 1-25. <https://www.jstatsoft.org/v40/i03/>.

<br/>

**maps**

Becker, R.A., Wilks, A.R., Brownrigg, R., Minkam T.P. & Deckmyn, A. (2022). _maps: Draw Geographical Maps_. R package version 3.4.1. <https://CRAN.R-project.org/package=maps>.

<br/>

**modelsummary**

Arel-Bundock, V. (2022). _modelsummary: Data and Model Summaries in R_. R package version 1.3.0. Journal of Statistical Software, 103(1), 1-23. <https://doi.org/10.18637/jss.v103.i01>.

<br/>

**plotly**

Sievert, C. (2020). _Interactive Web-Based Data Visualization with R, plotly, and shiny_. R package version 4.10.1. Chapman and Hall/CRC Florida. <https://CRAN.R-project.org/package=plotly>.

<br/>

**ppcor**

Kim, S. (2015). _ppcor: Partial and Semi-Partial (Part) Correlation_. R package version 1.1. <https://CRAN.R-project.org/package=ppcor>.

<br/>

**reshape2**

Wickham, H. (2007). _Reshaping Data with the reshape Package_. R package version 1.4.4. Journal of Statistical Software, 21(12), 1-20. <http://www.jstatsoft.org/v21/i12/>.

<br/>

**rdrobust**

Calonico, S., Cattaneo, M.D., Farrell, M.H. & Titiunik, R. (2022). _rdrobust: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs_. R package version 2.1.1. <https://CRAN.R-project.org/package=rdrobust>.

<br/>

**RTutor**

Kranz, S. (2020). _RTutor: Interactive R problem sets with automatic testing of solutions and automatic hints_. R package version 2020.11.25. <https://github.com/skranz/RTutor>.

<br/>

**scales**

Wickham, H. & Seidel, D. (2022). _scales: Scale Functions for Visualization_. R package version 1.2.1. <https://CRAN.R-project.org/package=scales>.

<br/>

**tibble**

Müller, K. & Wickham, H. (2022). _tibble: Simple Data Frames_. R package version 3.1.8. <https://CRAN.R-project.org/package=tibble>.

<br/>

**tidyr**

Wickham, H., Vaughan, D. & Girlich, M. (2023). _tidyr: Tidy Messy Data_. R package version 1.3.0. <https://CRAN.R-project.org/package=tidyr>.

<br/>

**zoo**

Zeileis, A. & Grothendieck, G. (2005). _zoo: S3 Infrastructure for Regular and Irregular Time Series_. R package version 1.8.11. Journal of Statistical Software, 14(6), 1-27. <https://CRAN.R-project.org/package=zoo>

<br/>




## Exercise Appendix -- Appendix

### Overview: 

A1: Behavioral Response

A2: Regression Analysis Basics

A3: Inaccurate Automation Date

A4: Deadline vs Pre-Deadline Automation

A5: ChatGPT Optimization

The file `background_code.R` which includes `rdplot2()` and `tidy.rdrobust()`, which is needed to apply `modelsummary()` on a `rdrobust()` output, will be published on GitHub




## Exercise A1 -- Behavioral Response

There is one aspect of Greenstone et al.’s (2022) paper that we did not include in the main section of this RTutor problem set. However, we will cover it briefly here in the appendix.

As we have seen, there are more data sets available than we have used so far. After finding support for the data manipulation hypothesis, Greenstone et al. (2022) continued with the behavioral implications. In particular, they analyzed the following data sets:

- `mask_filter_search.dta`: daily online search data for face masks and indoor air filters, scraped from the website Baidu.

- `search_sale.dta`: monthly search and sales data on a city-level, including several other variables from the paper "Self-protection investment exacerbates air pollution exposure inequality in urban China" (Sun et al. 2017).

Their hypothesis: as soon as people realized the increased published pollution levels, basically overnight, their search and purchase behavior changed in regard to face masks and indoor air filters. Hence Greenstone et al. (2022) applied the same methods, as we have used throughout this RTutor problem set, yet instead of $PM_{10}$ as the dependent variable, they used online search indices for face masks and air filters (sales data was not available however Greenstone et al. (2022) show evidence of a strong relationship between purchases and searches).

The following code exemplarily shows how Greenstone et al. (2022) conducted the RD design and DiD estimations. For more details and further models, please refer to Greenstone et al. (2022, 66-69).

**Replication of RD designs using face mask and air filter data** 

(procedure comparable to *Exercise 3*)

```{r "15_1",eval=FALSE}

#load data
search_city_month <- readRDS("search_city_month.RDS")

#prepare "filter" data
rd_filter_data <- search_city_month %>%
  filter(!is.na(filter) & !is.na(wind_speed) & !is.na(rain) & 
           !is.na(temp) & !is.na(rh))%>%
  mutate(resids = resid(felm(filter ~ wind_speed + rain + 
                               temp + rh | code_city + month)))

#run rdrobust() using "filter" data
rd_filter_raw <- rdrobust(rd_filter_data$filter,rd_filter_data$diff_months, 
                          c = 0, p = 1, q = 2, kernel = "tri", vce = "hc0", 
                          cluster = rd_filter_data$code_city, 
                          masspoints = "off")

rd_filter_resid <- rdrobust(rd_filter_data$resids, 
                            rd_filter_data$diff_months, c = 0, p = 1,q = 2, 
                            kernel = "tri", vce = "hc0", 
                            cluster = rd_filter_data$code_city, 
                            masspoints = "off")

#prepare "mask" data
rd_mask_data <- search_city_month %>%
  filter(!is.na(pmmask) & !is.na(wind_speed) & !is.na(rain) & 
           !is.na(temp) & !is.na(rh))%>%
  mutate(resids = resid(felm(pmmask ~ wind_speed + rain + 
                               temp + rh | code_city + month)))

#run rdrobust() using "mask" data
rd_mask_raw <- rdrobust(rd_mask_data$pmmask, rd_mask_data$diff_months, 
                        c = 0, p = 1, q = 2, kernel = "tri", 
                        vce = "hc0", cluster = rd_mask_data$code_city, 
                        masspoints = "off")

rd_mask_resid <- rdrobust(rd_mask_data$resids, rd_mask_data$diff_months, 
                          c = 0, p = 1, q = 2, kernel = "tri", vce = "hc0", 
                          cluster = rd_mask_data$code_city, 
                          masspoints = "off")


#display and compare RD estimates
modelsummary(list(rd_filter_raw, rd_filter_resid, rd_mask_raw, 
                  rd_mask_resid), coef_omit = 2:3, 
             coef_rename = "ATE", stars = TRUE, 
             add_rows = 
               tribble(~A, ~B, ~C, ~D, ~E,
                       "Type", "Filter","Filter","Mask", "Mask",
                       "Sample", "All", "All", "All","All",
                       "Pre-auto mean","35.5","35.5","0.62","0.62",
                       "City fixed effects","","X","","X",
                       "Month fixed effects","","X","","X",
                       "Weather covariates","","X","","X")%>%
                        structure(position = c(3:8)))
```


**Replication of DiD event study using face mask and air filter data** 

(procedure comparable to *Exercise 4*)

```{r "15_2",eval=FALSE}
#load data
did_ddl_search <- readRDS("did_ddl_search.RDS")

#prepare data
did_data <- did_ddl_search%>%
  mutate(year = year(as.Date(date, origin="1960-01-01")),
         month = as.numeric(month(as.Date(date, origin="1960-01-01"))),
         year_month = paste(year,month, sep = ""),
         treat = ifelse(auto_date == 19359,1,0),
         after2 = ifelse(year >= 2013,1,0),
         m12_before2=ifelse(year==2012 & month>=11 & month<=12,1,0),
         m34_before2=ifelse(year==2012 & month>=9 & month<=10,1,0),
         m56_before2=ifelse(year==2012 & month>=7 & month<=8,1,0),
         m712_before2=ifelse(year==2012 & month>=1 & month<=6,1,0),
         m12_after2=ifelse(year==2013 & month>=1 & month<=2,1,0),
         m34_after2=ifelse(year==2013 & month>=3 & month<=4,1,0),
         m56_after2=ifelse(year==2013 & month>=5 & month<=6,1,0),
         m712_after2=ifelse(year==2013 & month>=7 & month<=12,1,0),
         treat_m12_before2=treat*m12_before2,
         treat_m34_before2=treat*m34_before2,
         treat_m56_before2=treat*m56_before2,
         treat_m712_before2=treat*m712_before2,
         treat_m12_after2=treat*m12_after2,
         treat_m34_after2=treat*m34_after2,
         treat_m56_after2=treat*m56_after2,
         treat_m712_after2=treat*m712_after2)%>%
  filter(!is.na(wind_speed) & !is.na(rain) & !is.na(temp) & !is.na(rh))

#conduct DiD estimations using felm()
did_filter_1 <- felm(data = did_data, filter ~ treat_m712_before2 + 
                      treat_m56_before2 + treat_m34_before2 +
                      treat_m12_after2 + treat_m34_after2 + 
                      treat_m56_after2 + treat_m712_after2 +
                      year_month | code_city | 0 | code_city, 
                    na.action = "na.omit")

did_filter_2 <- felm(data = did_data, filter ~ treat_m712_before2 + 
                      treat_m56_before2 + treat_m34_before2 +
                      treat_m12_after2 + treat_m34_after2 + 
                      treat_m56_after2 + treat_m712_after2 +
                      wind_speed + rain +  temp + rh + 
                      year_month | code_city | 0 | code_city,  
                    na.action = "na.omit")

did_mask_1 <- felm(data = did_data, pmmask ~ treat_m712_before2 + 
                      treat_m56_before2 + treat_m34_before2 + 
                      treat_m12_after2 + treat_m34_after2 + 
                      treat_m56_after2 + treat_m712_after2 + 
                      year_month | code_city | 0 | code_city,  
                    na.action = "na.omit")

did_mask_2 <- felm(data = did_data, pmmask ~ treat_m712_before2 + 
                      treat_m56_before2 + treat_m34_before2 +
                      treat_m12_after2 + treat_m34_after2 + 
                      treat_m56_after2 + treat_m712_after2 +
                      wind_speed + rain +  temp + rh + 
                      year_month | code_city | 0 | code_city,  
                    na.action = "na.omit")

#display results
modelsummary(list(did_filter_1, did_filter_2, did_mask_1, did_mask_2),
             coef_rename = c("7–12 months before","5–6 months before","3–4 months before","1–2 months after","3–4 months after","5–6 months after","7–12 months after"),
             gof_omit = "AIC|BIC|Std.Errors|R2 Adj.|RMSE", 
             coef_omit = (8:34), stars = TRUE, 
             add_rows = tribble(~A, ~B, ~C, ~D, ~E,
                             "Type", "Filter","Filter","Mask", "Mask",
                             "Sample","Deadline","Deadline","Deadline","Deadline",
                             "City fixed effects","X","X","X","X",
                             "Year-month fixed effects","X","X","X","X",
                             "Weather covariates","","X","","X")%>%
                        structure(position = c(15:19)))%>%
  kable_classic() %>%
  kable_styling(font_size = 15)



```


Summarizing the results of both methods, we found significant behavioral responses after the automation when analyzing both face mask and air filter searches. There was a significant increase in searches and likely purchases of both items after the automation. Greenstone et al. (2022) also employed the RD design, which we exemplarily presented in this exercise, on a subgroup suspected of manipulating air pollution data (see e.g., Appendix A4 of this RTutor problem set) and the effect was greater for these cities. The sustained increase in searches suggests that the automation led to immediate and long-term changes in behavior. Going one step further, they conclude that the increase in protective behavior implies that the underreporting prior to the automation imposed meaningful welfare costs.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("DiD graphs for filter and mask data")
```


## Exercise A2 -- Regression Analysis Basics

Given that regression analysis is a fundamental method for nearly all analyses in this RTutor problem set, it would be beneficial to have a rudimentary comprehension of the concept. This appendix provides responses to a few introductory questions and some additional links towards the end, although it is not intended to cover all aspects of this vast subject.

**What is regression analysis?**

Regression analysis is a statistical tool that can be applied using a wide range of methods that depend heavily on the given scenario. In simple linear regression, one tries to model the relationship between two variables. One variable represents an outcome that is potentially dependent on another variable. Using common terminology, this first variable is called the outcome or dependent variable and is generally displayed on the y-axis. Further, the variable which affects the outcome is called an independent or explanatory variable and is displayed on the x-axis. Have a look at the graph below:

```{r "16_1",eval=FALSE}

library(stats)

x <- runif(n=100,min=0, max=10)
y <- x+rnorm(n=100,sd=0.5)

data <- data.frame(y,x)

ggplot(data=data,aes(x=x, y=y))+
  geom_point()+
  geom_smooth(method="lm",se=FALSE, color="red")+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)+
  labs(title = "Linear regression")+
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank())

```

Looking only at the graph above, we might conclude that the variables are positively correlated. If we wanted to be sure, we could simply calculate the correlation coefficient. However, in economics we go one step further since we are generally more interested in causality rather than correlation as it helps us to identify factors that lead to specific outcomes, e.g., changes in consumer behavior, market fluctuations, or policy effects. So, can we also say that an increase in $x$ *caused* an increase in $y$ here? The answer is clearly no, we do not even know what $x$ and $y$ represent or how the data was generated (if we only had access to the graph, we actually know how it was generated because the code is given). As a result, if we do not know the data-generating process, we cannot be sure that variable $x$ actually has an effect on variable $y$. It would also be possible that variable $y$ influenced variable $x$ or that another variable $z$, which we have not accounted for in our model, influenced both of these variables. Further, the correlation might simply be a coincidence. While correlation can be calculated, inferring causality is less trivial as it requires taking the data-generating process into account. For more details regarding this topic, see Panik (2012) or Wooldridge (2013).

Experimental studies are more reliable for causal inference because they enable us to isolate the effect of the treatment variable from other potential confounding factors. By randomly assigning participants to the treatment or control group, we (in theory) ensure that the groups are similar on average, except for the treatment variable. This allows us to estimate the causal effect of the treatment variable on the dependent variable more accurately. However, in economics, it is often not possible to conduct such controlled experiments (cost, ethical or practical reasons). Another option is therefore to conduct observational studies. Their results are in general harder to interpret causally since here we simply observe the results without doing any kind of random assignment. It is therefore harder to isolate any effect of interest that is unbiased (e.g., due to omitted variable bias). See Verbeek (2017), Kennedy (2008), or Wooldridge (2013) for additional information.

**What types of regression models are commonly used?**

Regression analysis is a powerful tool for studying the relationship between variables. There are various methods available for regression analysis, each suited to different types of data and research questions. We will only focus on the most basic methods here. One widely used method is simple linear regression, which models the linear relationship between two variables. This method is particularly useful when exploring how changes in one variable affect another. This is the general equation of the simple linear regression model (note that the first part of this section mainly refers to Wooldridge (2013)):

$$
y = \beta_0 +  \beta_1  x + \varepsilon
$$

The dependent variable is represented by $y$, the independent variable by $x$, and $\varepsilon$ is the disturbance or error term. The coefficient of interest is $\beta_1$, which represents the rate at which $y$ changes relative to $x$. An increase in $x$ by one unit is accompanied by an increase in $\beta_1$ units of $y$. Additionally, we can interpret the $\beta$ coefficients more illustratively by considering the equation’s line of best fit. The $\beta_0$ coefficient represents the y-intercept of the line (where $x$ is 0), which is the point where the line crosses the y-axis (we ignore $\varepsilon$ for the moment). Moreover, using the logarithms of $x$ and/or $y$ changes the interpretation of the coefficient. For example, if we use $x_1$ and $log(y)$, we interpret $\beta_1 \times 100$ as the percentage change in $y$ when $x_1$ changes by one unit. For more information, refer to Wooldridge (2013, 44). When using the term *model* in the context of regression analysis, we are referring to mathematical equations or formulas which describes the relationship between one or more independent variables and a dependent variable.

In multiple linear regression, we use more than one explanatory variable in the model:

$$
y = \beta_0 +  \beta_1  x_1 +  \beta_2 x_2 + … +  \beta_k  x_k   + \varepsilon
$$

We can also write this equation in matrix form, where $X$ is a matrix consisting of $k$ independent variables $x_1, x_2, ..., x_k$. $X$ is an $n \times (k+1)$ matrix, where $n$ is the number of observations (Verbeek, 2017, 11). The $\beta$ coefficients are now represented as a vector containing all the coefficients $\beta_0, \beta_1, ..., \beta_k$. For more information on linear regression in matrix form, refer to chapter E in the appendix of Wooldridge (2013, 807-820).

$$
y = \beta X+\varepsilon
$$

In addition to simple and multiple linear regression, there are other types of linear models, such as GLM (Generalized Linear Models) and polynomial regression models, which can capture more complex relationships between variables. When estimating a linear model, there are various methods to choose from. One common approach is OLS (Ordinary Least Squares), which minimizes the sum of squared residuals between the observed data and the estimated model. This method is used in this RTutor problem set and is explained in more detail in Verbeek (2017), chapter 2.1.

$$
\hat{\beta} = arg\min \sum_{i=1}^n \hat \varepsilon_i^2
$$

Why do we use $\hat{\beta}$ instead of $\beta$ in the equation above? The coefficients $\hat\beta$ are essential because they represent the **estimated** coefficients of the true $\beta$ coefficients that minimize the sum of squared residuals $\hat \varepsilon$. As mentioned earlier, the true error term $\varepsilon$ is unobservable and contains the effects of all unobserved factors that influence the dependent variable apart from the observable factors (independent variables). Therefore, we use the estimated coefficients $\hat \beta$ and the estimated error term $\hat \varepsilon$ in the equation.

The graph above illustrates how the OLS estimator $\hat\beta$ works by minimizing the sum of squared residuals. The line that is fitted to the data represents the equation that produces the smallest possible distance between the observed data points and the regression line. The squared distances from each data point to the line represent the $\hat \varepsilon$ values, which are summed up and minimized until we get the estimated coefficients $\hat \beta$ that minimize the summed-up and squared distance.

To ensure that the OLS estimator $\hat\beta$ has desirable properties, certain assumptions, known as Gauss-Markov assumptions, must hold. Refer to Verbeek (2017, 15-16) for further details.

- `A1:` the error terms follow a normal distribution with an expected value of zero.

–	`A2`: the independent variables are exogenous, meaning they are not correlated with the error term. If they are correlated, then the independent variables are endogenous.

- `A3:` the error terms are homoscedastic, which means they have constant variance.

- `A4:` the error terms are independent and not correlated with previous or following error terms.

When these assumptions are fulfilled, the OLS estimator $\hat\beta$ is: 

–	`unbiased`: the expected value (average of possible values) is equal to the true value of the parameter being estimated. Therefore, the OLS estimator will, on average, produce results that are close to the true value. Only assumptions A1 and A2 need to be satisfied for the estimator to be unbiased (Verbeek 2017, 16).

–	`efficient`: an efficient estimator has the smallest possible variance among all unbiased estimators. This means that it produces more consistent results with less variability than other unbiased estimators when we take multiple samples.

–	`consistent`: the estimated value produced by the OLS estimator will converge to the true value of the parameter being estimated as the sample size increases. As a result, the estimated value will become increasingly accurate the more data is collected.

In other words, the OLS estimator produces results that are on average close to the true value, have less variability than other unbiased estimators, and become more accurate with more data. See chapter 2 in “A guide to econometrics” (Kennedy 2008) to find more criteria that can be used to evaluate an OLS estimator.

**How can we implement linear regressions in R and and what do the results tell us?**

There are numerous packages available for different types of regression analysis. For simple linear regressions using OLS, the `stats` package offers the `lm()` (linear model) function, which can be used for both simple and multiple linear regressions. Another package commonly used for the same purpose is `lfe` (linear *group* fixed effects), which includes the `felm()` (fixed effects linear model) function. Although both functions can be used for simple linear models, the `felm()` function allows us to add clustered standard errors, making it a better choice for the RTutor problem set.

Regression models are typically presented in tables that display several models with different specifications side by side. However, we can also illustrate the output of a single model without a table. In the arbitrary example below, we regress the dependent variable $pm10$ on the independent variable $temp$ using data from the `station_day` data set and assign the resulting regression object to the variable `reg`.

```{r "16_2"}
  
#load package
library(lfe)

#load data
station_day <- readRDS("station_day.RDS")

#linear regression results saved in variable "reg"
reg <- felm(pm10 ~ temp, data= station_day) 

#display output
reg

```

If we want to get more details, we can use the `summary()` function:

```{r "16_3"}
summary(reg)
```

`Call`: after calling the `felm()` function, this line shows the exact function that was used for the regression analysis.

`Residuals`: these are the differences between the actual dependent variable values and the estimated values from the regression model. These residuals $\hat \varepsilon$ indicate how well the model fits the data. The summary statistics shown in this section include the minimum, 1st quartile, median, mean, 3rd quartile, and maximum of the residuals.

`Coefficients`: the first column displays the name of the explanatory variable $temp$ as well as the y-axis intercept. If we included more than one independent variable in our regression, we would have also found them here. In the second column called `Estimate` we find the $\hat\beta_1$ coefficient of the variable $temp$. The estimate of $-1.627$ represents the slope of our function which also describes the relationship between the dependent and independent variable: when the temperature increases by $1^{\circ} C$, $PM_{10}$ decreases by $-1.627\ \mu g/m^3$ (in this model using only these two variables). We also find the $\hat\beta_0$ coefficient which represents the y-axis intercept when $temp = 0$. As the estimate itself is a random variable, it also has a distribution and therefore a standard error which we find in the column `Std. Error`. Next, there is the `t-value` (`t value`), which is the ratio of the coefficient to its standard error indicating how many standard errors the coefficient is away from 0. Finally, the `p-value` (`Pr(>|t|)`) is calculated based on the `t-value` and represents the probability of obtaining the corresponding `t-value` if the null-hypothesis $H_0: \hat \beta = 0$ is true. The `p-value` is therefore used to determine how significant the estimated coefficient $\hat \beta$ is. If we reject the null-hypothesis, e.g., when the `p-value` is less than 0.05, we consider the result to be statistically significant at the 5% level (Wooldridge 2013, 129). However, that also means we could falsely reject the null-hypothesis in 5% of all cases and show a relationship when in fact the result is purely random. For more information on `p-values`, see Wooldridge (2013, 784-787). The `Signif. codes` below are the legend for the stars next to the `p-value` and they indicate at which level we can reject the null-hypothesis. The result of $2e-16$ is essentially 0 so we would reject the null-hypothesis at the 0% level (`***`), implying that temperature has an effect on $PM_{10}$ pollution. However, we must be cautious with the interpretation, as these estimates rely on a simple model that only uses a few variables, and there are likely other factors that have influenced the data generation process.

Further down we find additional statistics for the regression which we only cover briefly: `R-squared` or $R²$ measures how well the regression model fits the observed data (Wooldridge 2013, 38). In other words, it is the percentage indicating how much of the total variance of the dependent variable $y$ is explained by the explanatory variables $X$ (it can be calculated for the `(full model)`, which includes fixed effects, or the `(proj model)`, which does not include fixed effects). Both of them are available in an `adjusted` version which accounts for the number of explanatory variables. In the example above, the $R²$ is 6.5% (we did not include any fixed effects, so the numbers are identical). Additionally, there is the `F-statistic` which is also used to evaluate if the explanatory variables are significantly related to the dependent variable, not on an individual variable level (like the `t-value` before) but for the whole model (see Wooldridge (2013, 146) for details. The `p-value` of the `F-statistic` is the same as the `p-value` of the explanatory variable above since there is only one explanatory variable included in the model.

**Is there another way to display regression results?**

Both the `stargazer()` and the `modelsummary()` functions from their respective packages are suitable options. However, in this RTutor problem set, we only use the latter.

**Further resources:**

- introductory Econometrics - A Modern Approach (Wooldridge 2013)

- accompanying videos of the Market Analysis lecture from Kranz (e.g., https://www.youtube.com/watch?v=T5u8MiCVKug)

- interactive learning by working through another RTutor problem set, e.g., https://brigittepeter.shinyapps.io/RTutorWaterPollutionChina/, exercise 3.


## Exercise A3 -- Inaccurate Automation Date 

As mentioned in *Exercise 3.2*, there might be an issue with the data from *Station 1491*. We will begin with a graph showing the daily average $PM_{10}$ concentration close to the cutoff date (red line).

**Task:** Click on `check`

```{r "17_1"}

#load data
station_day <- readRDS("station_day.RDS")

#create and display graph
ggplot(data = station_day %>%
                  filter(!is.na(pm10) & station_n == 1491 & 
                           date >= auto_date -40 & 
                           date <=auto_date +40))+
  geom_point(aes(x=date, y= pm10), col = "lightgrey")+
  geom_vline(xintercept =  19359, col = "Red")+
  xlab("Date (Stata)")+
  ylab(expression("Raw PM"[10]))+
  labs(title=expression("Station 1491 (Raw PM"[10]*")"))+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)

```

<br/>


Quiz: Have a look at the $PM_{10}$ values right after the cutoff. What potential issues do you notice?

[1]: There appears to be a gap of several days after the fifth day after the cutoff.
[2]: The values right after the cutoff align more closely with the pre-automation trend than with the post-automation trend.
[3]: There is no issue, everything appears to be normal.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("station 1491 correct cutoff")
```

We observe relatively low $PM_{10}$ values in the first days after the cutoff, followed by a few days with no data (during which the updated monitoring system may have been installed) and a sharp increase afterward. It is at least possible that the true automation date differs from the one stated in our data set. This hypothesis is supported by the fact that the graph above looks similar even when using residual $PM_{10}$, which would account for the variation attributed to weather factors.


Quiz: If we assume that the automation date in our data set for *Station 1491* is indeed incorrect, how would the treatment effect change if we (manually) moved the automation date (the red line) 5 days to the *right* of the currently used automation date?

[1]: There would be no change.
[2]: The treatment effect would increase.
[3]: The treatment effect would decrease.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("RD manual change of data station 1491")
```

**Task:** See for yourself. Click on `check`.

```{r "17_2"}
#prepare data 
data_1491 <- station_day %>% 
  filter(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & 
           !is.na(temp) & !is.na(rh) & station_n == 1491)%>%
  mutate(diff_days = date - auto_date,
         resids =resid(felm(pm10 ~ wind_speed + rain + temp + rh | month)))

#run rdrobust() function
rd_raw_1491 <- rdrobust(data_1491$pm10, data_1491$diff_days, c = 0, p = 1,
                        q = 2, kernel = "tri", vce = "hc0", 
                        masspoints = "off")

rd_resid_1491 <- rdrobust(data_1491$resids, data_1491$diff_days, c = 0, 
                          p = 1, q = 2, kernel = "tri", vce = "hc0", 
                          masspoints = "off")

rd_resid_1491_corrected <- rdrobust(data_1491$resids, data_1491$diff_days,
                                    c = 5, p = 1, q = 2, kernel = "tri", 
                                    vce = "hc0", masspoints = "off")

#display and compare RD output
modelsummary(list(rd_raw_1491 , rd_resid_1491, rd_resid_1491_corrected), 
             coef_omit = 2:3, coef_rename = "ATE", stars = TRUE, 
             add_rows = 
               tribble(~A, ~B, ~C, ~D,
                     "Sample","Station 1491","Station 1491","Station 1491",
                     "Month fixed effects","","X","X",
                     "Weather covariates","","X","X",
                     "Manual correction","","","X")%>%
               structure(position = c(3:6)),
             title = "RD Design - Station 1491 Correction")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)

```

The first two models are identical to those in *Exercise 3.2*. In the third column, we find the estimated treatment effect of a model which is based on a manually corrected automation date. The significance and the standard error remain similar to the second model, but the treatment effect increased by 43.0%. In the next step, we want to find out if we find a similar problem for all stations in *Xi’an*.

**Task:**  Click on `check`.

```{r "17_3"}

#create and display graph
ggplot(data = station_day %>%
                  filter(!is.na(pm10) & code_city == 610100 & 
                           date >= auto_date -40 & date <=auto_date +40))+
  geom_point(aes(x=date, y= pm10), col = "lightgrey")+
  geom_vline(xintercept =  19359, col = "Red")+
  xlab("Date (Stata)")+
  ylab(expression("PM"[10]))+
  labs(title= expression("Xi'an (Raw PM"[10]*")"))+
  theme_ipsum(plot_title_size = 12, axis_title_size = 11, 
              axis_text_size = 10)

```

We observe the same pattern as when looking at the graph of *Station 1491*. The exact cause of this discrepancy remains uncertain, but it could range from deliberate deception to technical implementation difficulties or manual errors when scraping the data. One option would be to conduct further research into the true automation date. If we could verify with a degree of certainty that our data is incorrect, we could manually adjust the automation date for these seven stations. When using the models above on the data of *Xi’an*, we find similar treatment effects compared to those of *Station 1491* (analysis not included in this RTutor problem set).

You may be curious whether the potential automation date issue is limited to *Xi’an* or a structural issue. Rest assured that Greenstone et al. likely thoroughly examined their data and would have identified any structural problems. However, we can use some basic calculations and filters on the subsample of 76 cities with better data availability to identify stations that may have similar issues.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Identifying potentially incorrect automation dates")
```

The code in the info box above identifies 29 stations with several missing values around the cutoff. Upon examining the absolute deviation in the residuals, we find that nine stations in *Wuhan* (ID 420100) may have a similar issue to that of *Xi’an* (ID 610100). There is an eight-day gap around the cutoff, and the first value after the cutoff deviates from the subsequent values. However, it is possible that this is simply a random effect, as there is still considerable variation even when accounting for weather variables and fixed effects.

It is worth noting that the overall impact of a single city on the results of the entire sample is only minor. For instance, when we would exclude *Xi’an* from the analysis of the entire data set as conducted in *Exercise 3.3*, the estimated treatment effect is only 0.7% smaller in the model which accounts for covariates and includes fixed effects.

## Exercise A4 -- Deadline vs Pre-Deadline Automation

In *Exercise 3*, we did not differentiate in our analysis between cities that automated their reporting exactly on the official deadline date and those that automated earlier. Out of the 123 cities in the data set, 71 cities (57.7%) automated the reporting and monitoring on their respective deadline date. We will compare the results of these cities to those of the remaining 52 cities which chose a different automation date, using both the entire sample of 123 cities and the respective subsample of 76 cities with better data availability.

**Task:** Click on `check`. 

```{r "18_1",eval=FALSE}

#load data
station_day <- readRDS("station_day.RDS")
city_info_rd <- readRDS("city_info_rd.RDS")


#prepare data: entire sample as reference 
rd_data <- station_day %>%
  left_join(city_info_rd, by = c("code_city", "phase"))%>%
  mutate(diff_days = date - auto_date)%>%
  filter(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & 
           !is.na(temp) & !is.na(rh))%>%
  mutate(resids = resid(felm(pm10 ~ wind_speed + rain + 
                               temp + rh | station_n + month)))

#run rdrobust() 
rd_resid <- rdrobust(rd_data$resids, rd_data$diff_days, c = 0, p = 1, 
                     q = 2, kernel = "tri", vce = "hc0", 
                     cluster = rd_data$code_city,
                     masspoints = "off")

#prepare data: entire sample and deadline cities only
rd_deadline_data <- rd_data %>%
  filter(auto_date == 19724 | auto_date == 19359)

#run rdrobust()
rd_deadline <- rdrobust(rd_deadline_data$resids,rd_deadline_data$diff_days, 
                        c = 0, p = 1, q = 2, kernel = "tri", vce = "hc0", 
                        cluster = rd_deadline_data$code_city, 
                        masspoints = "off")


#prepare data: entire sample and pre-deadline cities only
rd_ndeadline_data <- rd_data %>%
  filter(auto_date != 19724 & auto_date != 19359)

#run rdrobust()
rd_ndeadline = rdrobust(rd_ndeadline_data$resids, 
                        rd_ndeadline_data$diff_days, 
                        c = 0, p = 1, q = 2, kernel = "tri", 
                        vce = "hc0", masspoints = "off",
                        cluster = rd_ndeadline_data$code_city)


#prepare data: 76 cities subsample
rd_76_data <- station_day %>%
  left_join(city_info_rd, by = c("code_city", "phase"))%>%
  filter(list_76 == 1)%>%
  mutate(diff_days = date - auto_date)%>%
  filter(!is.na(pm10) & !is.na(wind_speed) & !is.na(rain) & 
           !is.na(temp) & !is.na(rh))%>%
  mutate(resids = resid(felm(pm10 ~ wind_speed + rain + 
                               temp + rh | station_n + month)))

#prepare data: 76-cities subsample and deadline cities only 
rd_deadline_76_data <- rd_76_data %>%
  filter(auto_date == 19724 | auto_date == 19359)

#run rdrobust()
rd_deadline_76 <- rdrobust(rd_deadline_76_data$resids, 
                           rd_deadline_76_data$diff_days, 
                           c = 0, p = 1, q = 2, kernel = "tri", 
                           vce = "hc0", 
                           cluster = rd_deadline_76_data$code_city, 
                           masspoints = "off")


#prepare data: 76-cities subsample and pre-deadline cities only 
rd_ndeadline_76_data <- rd_76_data %>%
  filter(auto_date != 19724 & auto_date != 19359)

#run rdrobust()
rd_ndeadline_76 <- rdrobust(rd_ndeadline_76_data$resids, 
                            rd_ndeadline_76_data$diff_days, c = 0, 
                            p = 1, q = 2, kernel = "tri", vce = "hc0", 
                            cluster = rd_ndeadline_76_data$code_city, 
                            masspoints = "off")


#display RD output
modelsummary(list(rd_resid, rd_deadline, rd_ndeadline, 
                  rd_deadline_76, rd_ndeadline_76), 
             coef_omit = 2:3, coef_rename = "ATE", 
             stars = TRUE,
             add_rows = 
               tribble(~A, ~B, ~C, ~D, ~E, ~F,
               "Sample", "All", "Deadline","Pre-Deadline",
                                    "76 Deadline","76 Pre-Deadline",
               "Weather covariates","X","X","X","X","X",
               "Station fixed effects","X","X","X","X","X",
               "Month fixed effects","X","X","X","X","X")%>%
                        structure(position = c(3:6)),
             title = "Complete vs Deadline vs Pre-Deadline Sample")%>%
  kable_classic() %>%
  kable_styling(font_size = 15)


#Greenstone et al. (2022): 
#Table 1 - Panel A, column 1 and 5; Table B4, column 5


```



Quiz: Which model is equivalent to the one in *Exercise 3.3*, in which we used residual $PM_{10}$ as the dependent variable and the entire sample of 123 cities?

[1]: model (1)
[2]: model (2)
[3]: model (3)
[4]: model (4)
[5]: model (5)

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Deadline vs pre-deadline")
```



Quiz: Which models use only data of cities which automated before the official deadline? Choose from the following options.

[1]: model (1) and model (3)
[2]: model (2) and model (3)
[3]: model (3) and model (4)
[4]: model (1) and model (5)
[5]: model (3) and model (5)

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("Deadline vs pre-deadline 2")
```

The first RD estimate is the same as in *Exercise 3.3* and serves as a reference. In the adjacent column, the model that only uses data from cities that automated exactly on the deadline date results in an estimated treatment effect of  $57.139\ \mu g/m^3$, which is 63.8% higher than the result for the entire sample. Greenstone et al. (2022, 63) suggest that this increase supports the hypothesis that cities that automated before the deadline chose an earlier automation date to conceal manipulation. However, they do not provide any models that exclusively use data from pre-deadline cities and do not delve into further detail. When we do conduct such a model (see model $(3)$), we find no significant treatment effect. This, combined with the previous result, supports Greenstone et al.’s (2022) interpretation. However, it is also possible that pre-deadline cities were simply less involved in data manipulation, which could have led to the same result. Nevertheless, Greenstone et al. (2022) evaluate the difference between the deadline and pre-deadline cities and their impact on the entire sample result as “at most modest” (Greenstone et al. 2022, 63).

Taking it a step further, we can apply the split between the deadline and pre-deadline cities to the subsample of 76 cities with less missing pre-automation data. In the deadline sample, the RD estimate increases by 12.5%, but the standard error increases by over 41.0%. In the pre-deadline sample, the treatment effect remains insignificant but becomes slightly negative at $-4.634$, while the standard error does not change much. Overall, using this subsample does not provide much additional insight apart from confirming the results of the entire sample.


## Exercise A5 -- ChatGPT Optimization

The text in each section of this RTutor problem set was optimized using the chatbot *ChatGPT* (“Feb 13 Version”, in some occasions “Mar 14 Version”) from OpenAI (https://openai.com/blog/chatgpt).

For this purpose, one of the following commands was used in front of the original text blocks:

- "Optimize language:"
- "Optimize language and content:"
- "Optimize language and content in the following paragraph:"
- "Optimize language and content in the following sentences:"

The objective of optimizing the content was to find potential factual mistakes in the original text. The output from ChatGPT was modified again to correct mistakes created by the chatbot. On many occasions, the original version of the text was kept. A preliminary version of this RTutor problem set, including both the original and the updated text, is available on request.

These are two examples showing the process: 

**Example 1** 

Original text:

Before we start with the practical analysis applying RD designs, we should shortly summarize what we actually want to analyze. As one element of the *War on Pollution*, the Chinese (central) government decided to automate the reporting and monitoring of pollution, starting with the most relevant cities. By the time they announced the *War on Pollution*, they had already started the process of rolling out the new reporting and monitoring technology to upgrade the measuring equipment in many cities. As a result, after the automation event the pollution data was directly uploaded to a central platform. Before this change, local (city) governments received the data from their stations before they reported and uploaded it. Referring to the principal-agent problem, we consider the central government to be the principal and the local city governments to be the agents. The latter’s overall performance is measured by several metrics, including pollution metrics like $PM_{10}$ before the automation. As we have seen, there is a conflict of interest when reporting the real pollution data is reducing the risk of receiving some kind of performance-based reward (e.g., a promotion within the CCP). When actual pollution levels were worse than the target pollution levels set by the central government, the local governments were potentially motivated to manipulate the data in order to still receive the reward. Our goal is therefore to analyze if the exact date when air pollution reporting and monitoring was automated (cutoff) had any effect on the actually published pollution metrics, most importantly $PM_{10}$ as it was the most relevant air pollutant for the evaluation according to Greenstone et al. (2022). Our main hypothesis is based on the described principal-agent problem which implies the possibility of data manipulation on the local level before the automation technology was implemented. A positive treatment effect, which is equivalent to an increase in $PM_{10}$ right after the automation, would not only indicate that the hypothesis is potentially true but also that some form of technology was used to ease the principal-agent problem in this governmental scenario. We will use data before and after the automation date which is naturally splitting all observations in a control and a treatment group. All observations before the automation date are in the control group and all observations after the automation are in the treatment group, yet this split occurred naturally and was not executed by the researchers as a controlled experiment. As a result, we consider this setting to be quasi-experimental or a local experiment at the cutoff (e.g., Sekhon and Titiunik 2017).

ChatGPT output ("optimize language"):

Before we begin our practical analysis using RD designs, let us provide a brief summary of our investigation. In line with the War on Pollution initiative, the Chinese central government implemented the automation of air pollution reporting and monitoring in major cities. The new technology was rolled out to upgrade the monitoring stations in many cities, with local city governments previously receiving the data from their stations before publishing it on a central platform. However, due to the principal-agent problem, local governments may have manipulated the air pollution data for personal gain. We aim to analyze whether the date of automation had any effect on published pollution data, particularly $PM_{10}$, which is considered the most relevant air pollutant for evaluating local government performance during the technology’s implementation period (Greenstone et al. 2022). After automation, local governments are no longer directly involved in the process. Our main hypothesis is based on the possibility of data manipulation before the automation, and a positive treatment effect (an increase in $PM_{10}$ after automation) would support this hypothesis, demonstrating that the technology helped to ease the principal-agent problem. We will use data from before and after the automation date, which naturally splits all observations into a control and treatment group. This setting is considered a quasi-experimental or local experiment at the cutoff (Sekhon and Titiunik, 2017).

Final version:

Before starting the practical analysis using RD designs, we will briefly summarize the topic of our investigation. As part of the *War on Pollution*, the Chinese central government initiated the automation of air pollution reporting and monitoring in major cities, starting with the most relevant ones. By the time the *War on Pollution* was announced, the central government had already started the rollout of new reporting and monitoring technology to upgrade the monitoring stations in many cities. Prior to this, local city governments received the data from their stations before publishing it on a central platform. However, due to the previously described principal-agent problem, local governments may have manipulated their air pollution data for their own benefit, e.g., to receive performance-based rewards or a promotion within the CCP. To investigate this issue, we aim to analyze whether the air pollution reporting and monitoring automation had any effect on the published pollution data, especially $PM_{10}$. Greenstone et al. (2022) consider $PM_{10}$ the most relevant air pollutant in the evaluation process of local governments during the time when the technology was implemented. After the automation, the process no longer required direct involvement from local governments since the data was uploaded directly from the stations. Our main hypothesis is based on the possibility of data manipulation by local governments before the automation. A positive treatment effect, indicating an increase in $PM_{10}$ after the automation, would support the hypothesis and demonstrate that this technology helped to ease the principal-agent problem. We will use data from before and after the automation date which is naturally splitting all observations in a control and a treatment group. All observations from before the automation date are in the control group and all observations from after the automation are in the treatment group, therefore qualifying this setting as quasi-experimental.


**Example 2** 

Original text:

The estimated treatment effect of second model which includes weather covariates as well as fixed effects is almost identical to the first model where we used raw $PM_{10}$ when applying the RD designs to the entire sample of 123 cities. The estimates are both significant, the first one at the 1% level and the second one at the 0.1% level. However, the standard error in the second model, $5.786$, is 45.8% smaller compared to the one from the first model, $10.682$. Further, the calculated optimal bandwidth for the second model is a lot bigger compared to the first model, hence there are also more observations used in the second model.

Referring back to the initial hypothesis and supposing our assumptions hold, this estimated treatment effect for the entire sample indicates that the exact day when the air pollution reporting and monitoring was automated led to an increase in $PM_{10}$ of $34.893\ \mu g/m^3$ (model $(2)$). This increase is equivalent to $77.5\%$ of the WHO guideline for $PM_{10}$ and $41.0\%$ of the yearly average $PM_{10}$ in 2012, which was $85.140\ \mu g/m^3$ These results can be considered plausible support for the initial hypothesis stating that the underlying principal-agent problem resulted in air pollution data manipulation by local governments before the reporting and monitoring was automated. 

ChatGPT output ("optimize language and content"):

The estimated treatment effect of the second model, which includes weather covariates as well as fixed effects, is nearly identical to the first model where raw $PM_{10}$ was used when applying the RD designs to the entire sample of 123 cities. Both estimates are statistically significant, with the first one at the 1% level and the second one at the 0.1% level. However, the standard error in the second model is substantially smaller at $5.786, which is 45.8% compared to the first model at $10.682. Furthermore, the calculated optimal bandwidth for the second model is larger, indicating that more observations are used in the analysis.

Assuming our initial hypotheses and assumptions are valid, the estimated treatment effect for the entire sample suggests that the automation of air pollution reporting and monitoring led to an increase in $PM_{10}$ of $34.893\ \mu g/m^3$ (model $(2)$). This increase corresponds to 77.5% of the WHO guideline for $PM_{10}$ and 41.0% of the yearly average $PM_{10}$ in 2012, which was $85.140\ \mu g/m^3$. These results provide plausible support for the initial hypothesis that the principal-agent problem resulted in air pollution data manipulation by local governments before the reporting and monitoring was automated.

Final version:

When using reported $PM_{10}$ as the dependent variable, we find an estimated treatment effect of $34.717\ \mu g/m^3$, which is significant at the 5% level, for the entire sample of 123 cities. For the second model, which includes weather covariates and fixed effects, the estimate is almost identical at $34.893\ \mu g/m^3$. While the estimate from the first model is significant at the 1% level, the second estimate is significant at the 0.1% level. More importantly, the standard error of the second estimate, which is $5.786$, is 45.8% smaller than the first model’s standard error of $10.682$. Moreover, the second model’s calculated optimal bandwidth is larger than that of the first model. 

What do these results imply concerning the main hypothesis of this RTutor problem set (and the underlying paper)? The estimated treatment effect for the entire sample of around $35\ \mu g/m^3$ is equivalent to around 78% of the yearly average WHO guideline for $PM_{10}$ and corresponds to an increase of roughly 40% relative to the yearly average pre-automation $PM_{10}$ of $86.1\ \mu g/m^3$. If the assumptions from *Exercise 3.1* hold, the results suggest that on the exact day when air pollution reporting and monitoring was automated in each city, $PM_{10}$ concentrations increased significantly. This provides plausible support for our initial hypothesis that air pollution data was manipulated by local governments before the reporting and monitoring were automated. However, before we can make any conclusions, we should analyze if there are any relevant subgroups in our data that contributed overproportionally to the overall result. Further, we have to test the outlined assumptions (*Exercise 5*).





